{:title "Pure ASM in jungles of Panama"
 :layout :post
 :tags  ["jdk10" "jmh" "asm" "panama" "java"]
 :toc false
}

:toc: macro

Hi!

In this article,  Iâ€™ll tell you about some internal features of project Panama (targeting JDK10). You will be known about
how to incredibly increase performance using pure assembler. ... intrinsics


[.text-center]
image:/img/panama/panama.jpg[]
[.text-center]
[quote]
____
We had two builds of jvm, seventy-five native function, five sheets of high powered method handles, a panama repository full of crazy features, and a whole galaxy of native data layouts, headers, compilers, optimizations... and also a quart of heap, a case of wrappers, a pint of raw memory and two dozen AVX2 instructions.

Not that we needed all that for the trip to panama, but once you get locked into a serious jvm crash collection, the tendency is to push it as far as you can.
____

So, let's begin our journey.

== Welcome to Panama

link:http://openjdk.java.net/projects/panama/[Panama] is a new project under OpenJDK that tries to improve the connection between JVM and foreign APIs, including many interfaces commonly used by C programmers.
It is the missing piece in Java ecosystem, a bridge between JAVA and native code.

The primary features that will have brought by the Panama project are:

- Native function calling and native data access with huge JIT support (see link:http://openjdk.java.net/jeps/191[JEP191]) +
  (The similar problem but without huge runtime support can be solved using JNR that I mentioned in my link:/posts/22-06-2015-jnr-fuse/[previous article])
- New data layouts
- Special tooling for wrapping native libraries

The full overview of the problems that Panama tries to solve you can find in the link:https://blogs.oracle.com/jrose/entry/the_isthmus_in_the_vm[blog post] written by John Rose.

But today we will uncover about some special features that haven't been mentioned a lot.

In last December Vladimir Ivanov, one of contributors of Panama project made a commit where He introduce an ability to call a snipped of Java code in runtime...
++++
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Project Panama: Machine code snippets and vector values support in HotSpot JVM <a href="https://t.co/lW9GvKKk5h">https://t.co/lW9GvKKk5h</a> <a href="https://twitter.com/hashtag/java?src=hash">#java</a></p>&mdash; Vladimir Ivanov (@iwan0www) <a href="https://twitter.com/iwan0www/status/672824680227708928">December 4, 2015</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
++++

Really, it is true now, you can made an inline assembler call, crazy stuff... It is like to write your own intrinsic. So let's explore this opportunity.


== The edge of the forest
The first program that every programmer writes in a new language is "Hello, World!". But it is assembler. And it is called from Java, so let's make it simple. +
Say, A+B+C function:
[source, java]
----
static final MethodHandle sum3 = jdk.internal.panama.CodeSnippet.make(
            "sum3", MethodType.methodType(int.class,/*result*/
                                          int.class /*rdi*/,
                                          int.class /*rsi*/,
                                          int.class /*rdx*/),
            true, /* isSupported */
            0x48, 0x89, 0xF0, // mov    rax,rsi
            0x48, 0x01, 0xF8, // add    rax,rdi
            0x48, 0x01, 0xD0  // add    rax,rdx
    );
----

Here we used CodeSnippet class to get MethodHandle to native code. And yes, internal package here isn't for beauty, it is actually means internal API, so you won't be able to use it. +
You can pass primitives and some special classes like Long2 (128 bit register), Long4  (256 bit register) and Long8 (512 bit register) as an arguments.
 ....

But you can say that we were able to use JNI before, what the point of using inline asm? The thing that C2 compiler can easily inline this snipped. So, it gives you an opportunity (if you crazy enought) to write your own JVM intrinsic, but without coding it in JVM.

Let's compare ASM code which we will get after C2 compiled our code.


[.fit-table]
[options="header",grid="cols",width="100%"]
|==================================
|Plain Java |CodeSnippet ASM|JNI
a|
[source,x86asm]
----
[Verified Entry Point]
 sub  rsp,0x18
 mov  QWORD PTR [rsp+0x10],rbp  ;*synch entry

jitresult:
 mov  eax,DWORD PTR [rsi+0x1c]
 add  eax,DWORD PTR [rsi+0x18]
 add  eax,DWORD PTR [rsi+0x20]  ;*iadd


exit:
 add  rsp,0x10
 pop  rbp
 test DWORD PTR [rip+0x15b4ea60],eax
----
a|
[source,x86asm]
----
[Verified Entry Point]
 sub  rsp,0x18
 mov  QWORD PTR [rsp+0x10],rbp;*sync entry
 mov  r10,rsi
 mov  esi,DWORD PTR [rsi+0x1c] ;*field b
 mov  edx,DWORD PTR [r10+0x20] ;*field c
 mov  edi,DWORD PTR [r10+0x18] ;*field a

snippet:
 mov  rax,rsi
 add  rax,rdi
 add  rax,rdx

exit:
 add  rsp,0x10
 pop  rbp
 test DWORD PTR [rip+0x16d21852],eax
----
a|
[source,x86asm]
----
[Verified Entry Point]
 mov  DWORD PTR [rsp-0x14000],eax
 push rbp
 sub  rsp,0x10           ;*sync entry

 mov  edx,DWORD PTR [rsi+0x1c]  ;*field b
 mov  ecx,DWORD PTR [rsi+0x20]  ;*field c
 mov  esi,DWORD PTR [rsi+0x18]  ;*field a

native_call:
 xchg ax,ax
 call 0x00007f7ab5668738

exit:
 add  rsp,0x10
 pop  rbp
 test DWORD PTR [rip+0x166add39],eax
 ret  ;*invokestatic s_nat

runtime_call_rethrow_Java:
 mov    rsi,rax
 add    rsp,0x10
 pop    rbp
 jmp    0x00007f7aadc7b6e0
----

|==================================
As you can see here the only difference between JIT version and our CodeSnippet is argument moving between registers. And C2 perfectly inlined exactly the same piece of code as we wrote down upper. In the same time JNI perform real native call.

But what the point of writing inline asm snippets in Java. Usually you have no reasons, C2 is able to compile your code in something that works much faster. But there are several things that C2 can't do efficiently. The most important is that C2 can't rewrite your algorithm using SIMD operations yet.


== Go deeper to hidden places

Let's imagine, our application has a little function called checksum that summirise bytes and in result gives us hash [0, 256).

[source,java]
----
private static int checksumPlainJava(ByteBuffer buffer, int size) {
    int checksum = 0;
    for (int i = 0; i < size; ++i) {
        checksum += buffer.get(i);
    }
    return (int) (Integer.toUnsignedLong(checksum) % 256);
}
----

We operate big byte buffers and we have to calculate checksums very often. We discovered that this checksum function is our bottleneck. And we need to optimize it. What options do we have?

=== VarHandles
=== JNI
You may see on the last line ugly operation where we are trying to convert our signed int to unsigned to get proper result. Of course, it is bottleneck you might think, isn't it? The cool C++ has unsigned variables, let's make a JNI call!

Ok, here we go:
We will get native address of our buffer using reflection. Now we are able to access memory from C++ code directly.

[source,java]
----
public static long getAddress(ByteBuffer buffy) throws Throwable {
    Field address = Buffer.class.getDeclaredField("address");
    address.setAccessible(true);
    return address.getLong(buffy);
}
----
And C++ code:
[source,cpp]
----
JNIEXPORT jint JNICALL Java_me_serce_panex_ChecksumBenchmark_nativePlainChecksum
    (JNIEnv * env, jclass clz, jlong addr, jint targetLength) {
    char *target = reinterpret_cast<char *>(addr);
    unsigned int checksum = 0;
    for (int i = 0; i < targetLength; ++i) {
        checksum += (unsigned int) target[i];
    }
    return checksum % 256;
}
----

Now we have to check the performance. We may expect incredible results. For performance measurement we will be using link:http://openjdk.java.net/projects/code-tools/jmh/[JMH], de-facto standard in Java benchmarking. You can find a great deal of articles answering a question "why JMH?" on the internet.

//

Slower? But what happened? Why C++ is slower than java? Is it because JNI is slow, right? Let's use CodeSnippets.

But it may be hard to write code in pure assembler, so we can make it in another way. We'll write C++ code, then we'll compile it and put dissabled code into our methods. Sounds creepy, but it is possible.
/* picture */
Several things you should care about:

- You shouldn't have a ret instruction, JVM will care about it
- You should look carefully thorough your assembly code to be sure that it doesn't try to access memory from outside method.
- And, finally you should care about calling convention

// method

Ok, I can't believe my eyes, it is faster, but actually it is still slower than ASM produced by JIT. +
The thing is I used O2 g++ option, which doesn't get us a lot of optimization. So, we should use O3 if wee need more optimizations

There is a simple explanation why is it faster than code emitted by JIT. Here GCC was able to vectorize our loop. So, we used SIMD instructions which gave our processor an ability to "parallize" execution.

But that if we need more performance?

==== SIMD

It is possible to write the same code, but using AVX2 (256 byte registers) instructions. (Thanks for an article where I read how to do it).

This is how our simple checksum function looks like after rewriting:
[source,cpp]
----
----

Isn't so easy to understand, but how fast is it?

// comparison

Awesome it 8x times faster than our original code. Is it possible to make it faster? Yes, but my Haswell doesn't support AVX512, so but you may try.


Let's say, now performance met our requirements, but can we make it more readable than just a blob of ASM code produced by gcc? Of cource, we can save main loop inside Java and use Long4 vectors to pass data.


[source,java]
----
private static int JAVA_avxChecksumAVX2(ByteBuffer buffer, long target, int targetLength) throws Throwable {
        Long4 zeroVec = Long4.ZERO;
        Long4 oneVec = ones;
        Long4 accum = Long4.ZERO;
        int checksum = 0;
        int offset = 0;

        if (targetLength >= 32) {
            for (; offset <= targetLength - 32; offset += 32) {
                Long4 vec = _mm256_loadu_si256(target + offset);
                Long4 vl = _mm256_unpacklo_epi8(vec, zeroVec);
                Long4 vh = _mm256_unpackhi_epi8(vec, zeroVec);

                accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vl, oneVec));
                accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vh, oneVec));
            }
        }

        for (; offset < targetLength; ++offset) {
            checksum += (int) buffer.get(offset);
        }

        accum = _mm256_add_epi32(accum, _mm256_srli_si256_4(accum));
        accum = _mm256_add_epi32(accum, _mm256_srli_si256_8(accum));
        long checksum2 = (_mm256_extract_epi32_0(accum) + _mm256_extract_epi32_4(accum) + checksum);
        return (int) (Integer.toUnsignedLong((int) checksum2) % 256);
    }
----



== Conclusions

- Nothing is impossible
- Useless but interesting
- You need to understand before optimize

=== Thanks

=== Links 