<?xml version='1.0' encoding='UTF-8'?>
<rss version='2.0' xmlns:atom='http://www.w3.org/2005/Atom'>
<channel>
<atom:link href='http://serce.me/' rel='self' type='application/rss+xml'/>
<title>
SerCe's blog
</title>
<link>
http://serce.me/
</link>
<description>
Personal blog
</description>
<lastBuildDate>
Fri, 15 Oct 2021 18:44:48 +1100
</lastBuildDate>
<generator>
clj-rss
</generator>
<item>
<guid>
http://serce.me/posts/14-10-2021-the-five-lies-analysis/
</guid>
<link>
http://serce.me/posts/14-10-2021-the-five-lies-analysis/
</link>
<title>
The Five Lies Analysis
</title>
<description>
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Five Whys analysis is a popular root cause investigation technique with a simple premise that is asking why five times can yield the answer, which is the root cause. While doing so could be a helpful exercise, blindly applying the technique often leads to a suboptimal result. In this article, I’ll explore some of its tradeoffs with a fictional story of an incident in production at &lt;a href=&quot;https://en.wikipedia.org/wiki/Acme_Corporation&quot;&gt;Acme Corp&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/five-lies/crew.png&quot; alt=&quot;crew&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock text-center&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m sorry. The words made sense, but the sarcastic tone did not.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;attribution&quot;&gt;
&amp;#8212; Lana Kane
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_meet_the_crew&quot;&gt;Meet the crew&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s start by setting the scene. A team of engineers is working on a new Acme News website. Suddenly, an incident occurs, which gets resolved quickly. A lengthy investigation then follows the incident to understand what happened and how similar issues can be prevented in the future.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The standard process of achieving this would be to write a postmortem, which can outline the story and propose a set of action items. A postmortem template can simplify the process by including all necessary steps to identify the root cause and might suggest using an analysis, for example, the five whys analysis. The question of who is ultimately responsible for authoring the postmortem is tricky, and the answer depends on many factors. Let’s take a look at what everyone on the team would write if they were the author.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_meet_alice&quot;&gt;Meet Alice&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/five-lies/alice.png&quot; alt=&quot;alice&quot; width=&quot;300&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As we know, Alice is an on-call engineer. Her official title is SRE, and her job involves a deep understanding of the service state based on the available metrics. She has a solid understanding of service health indicators and a good grasp of the monitoring infrastructure. When she started the investigation, she noticed that the application was overloaded due to high CPU usage. As recommended by the postmortem template, she began the five whys analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Five Whys analysis:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Why were the users not able to open the website? Because the application was overloaded, and it started returning errors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why was it overloaded? Because the CPU went to 100%, and the instances didn&amp;#8217;t have enough capacity to process the requests.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why did the CPU usage jump so high? Because we didn&amp;#8217;t notice it earlier when it climbed to 90% and didn&amp;#8217;t fix the issue in time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why did we not notice it? Because there were no alerts for 90% CPU usage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why were there no alerts? Because the alerts were removed two years ago after a migration to a new monitoring system.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As the Five Whys analysis was completed, the root cause was &quot;identified&quot;. The main action item in the postmortem was to re-instantiate high utilisation - 90% CPU alerts to catch similar situations earlier and mitigate the issue before users are effect. Now that all of the questions are answered, and the lessons are learned, Alice is off to another incident.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_meet_bob&quot;&gt;Meet Bob&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/five-lies/bob.png&quot; alt=&quot;bob&quot; width=&quot;300&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Bob is the backend engineer who implemented the feature in the first place and flipped the flag. He has the most context around the change, so it would make sense for Bob to write a postmortem. Bob can use the Five Whys analysis to complete the postmortem and understand how to roll out all future changes smoother.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Five Whys analysis:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Why were the users not able to open the website? Because the application was overloaded.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why was it overloaded? Because CPU jumped to 100%.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why did it jump? Because the application went into an infinite GC loop.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why did GC start consuming all of the CPU? Because the application ran out of heap memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why did it run out of heap memory? Because a new request-level cache was added with unlimited size.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The new feature that Bob implemented enabled users to see article recommendations on the Acme News website. To reduce the load to the recommendation service, Bob added a cache. However, Bob forgot to add a maximum size parameter. Bob then started thinking about how this could be prevented in the future. He decided to change the Cache interface to ensure that max size always has to be provided. This task was added as the main action item to the postmortem.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_meet_charlie&quot;&gt;Meet Charlie&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/five-lies/charlie.png&quot; alt=&quot;charlie&quot; width=&quot;300&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Charlie is a frontend engineer who implemented the feature on the frontend side and is responsible for Acme Corp News overall look. Charlie is also a product owner of the new Acme Corp Recommendations ™, who cares a lot about the product&amp;#8217;s availability, so it might make sense for her to start a postmortem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Five Whys analysis:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Why were the users not able to open the website? Because the application was overloaded.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why was it overloaded? Because there was an influx of requests that caused high CPU usage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why was there an influx? Because the users started sending a lot of requests.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why did they do this? Because each request was retried up to 10 times.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why were there so many retries at the same time? Because there was no exponential backoff.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Charlie’s mental model of the application is primarily based around the request flows. When looking at the application logs, she noticed many retries from the same set of users. When looking at the code that fetches the recommendations, she saw that all of the requests were retried immediately without any exponential backoff or jitter. Hence, she put it as an action item with the plan to implement it by Friday.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_meet_dave&quot;&gt;Meet Dave&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/five-lies/dave.png&quot; alt=&quot;dave&quot; width=&quot;300&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Dave is an engineer on the cloud infrastructure team, and he is responsible for the overall health of the Acme News Corp cluster. Dave cares a lot about reliability, so he might volunteer to perform the investigation and complete the postmortem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Five Whys analysis:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Why were the users not able to open the website? Because the application was overloaded.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why was it overloaded? Because the load on every instance increased sharply.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why was the load per instance so high? Because we didn’t have enough instance capacity to distribute the load.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why not? Because our autoscaling policies didn’t scale fast enough and only added a few more instances.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why? Because the autoscaling cooldown time was too large.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For Dave, each task in the cluster is essentially a black box that can handle a certain number of requests. For each type of task, Dave manages policies that define how the applications can scale up or down. After looking at the graphs, Dave notices that even though a few instances joined the fleet when the load increased sharply, the cooldown prevented autoscaling from bringing up even more tasks to handle the load. Dave writes down a task to update autoscaling policies to allow for very aggressive scale-ups to handle the load as the main action item.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_meet_erin&quot;&gt;Meet Erin&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/five-lies/erin.png&quot; alt=&quot;erin&quot; width=&quot;300&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Erin is an engineer on the core libraries team, and she is working on a shared set of core libraries. When she first heard about the incident and that it was related to the cache library, she volunteered to write a postmortem as the owner who doesn’t shy away from responsibility.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Five Whys analysis:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Why were the users not able to open the website? Because the application was overloaded.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why was it overloaded? Because it was out of memory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why was the application out of memory? Because the cache took too much memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why did the cache take up that much memory? Because our cache libraries are not optimised for memory usage.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why is the cache library not optimised for memory usage? Because we always valued throughput above memory usage.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Erin wrote the original cache library. The library has been a great success as it is incredibly optimised for Acme Corp’s use cases. Nonetheless, so far, the focus was the throughput and not the memory usage. After looking at the library from a different perspective, Erin noticed a few opportunities to share the underlying data structures and wrote down an action item to provide a version of the cache explicitly optimised for memory usage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_the_analysis&quot;&gt;The analysis&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s zoom out and take a look at the results of the exercise. There are five people and five different stories. The key takeaway here is the results of the why the Five Whys analysis are not repeatable. The outcome heavily depends on the angle at which the person performing the analysis looks at the incident. Dave’s point of view on what happened is very different to Charlie’s point of view as they work on very different parts of the system day-to-day, which heavily influences the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But even looking at a single investigation, we can notice that the number “five” itself is interesting. It’s catchy, it’s easy to remember, but it unnecessarily limits the depth of the investigation. Maybe Alice should’ve done some code archeology to figure out why the alerts were removed after the migration. Bob could have dug deeper to collect some of the heap dumps to understand the distribution of data in the cache. Why stop at five if it makes sense to go further?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_the_root_cause&quot;&gt;The root cause&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As we saw, the Five Whys analysis has downsides like any other approach. Yet, it can be used successfully to dive deeper! So are these issues big enough to invalidate the approach?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The real problem reveals itself when the technique becomes a part of a template. Once it&amp;#8217;s in the template, the shape of the analysis becomes solidified. Whether it&amp;#8217;s a part of an engineering &lt;a href=&quot;https://www.atlassian.com/incident-management/postmortem/templates&quot;&gt;postmortem template&lt;/a&gt; or even a &lt;a href=&quot;https://www.justice.act.gov.au/sites/default/files/2019-08/Root_Cause_Analysis_Template.pdf&quot;&gt;government worksheet&lt;/a&gt; - the template restricts the analysis by forcing its limits onto the investigator. The downsides of the analysis become the downsides of the postmortem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Well, if not using five whys, then how would we find the root cause? First, it&amp;#8217;s important to understand that rarely there is such thing as a root cause. Often there would be multiple causes that can be seen when looking at the incident from different angles.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Second, and more importantly, it’s not about finding the root cause at all. It’s about getting an inside out understanding of what happened — and then, based on this understanding, defining what actions can be taken to ensure that the incident doesn’t repeat in the future. Often, these action items can be very distant from the root cause. For instance, maybe Acme News could restrict the blast radius and ensure that the page loads even if recommendations are not available. These mitigations might not always come up when searching for a root cause. This poses a question - if not the Five Whys analysis, then what should be in the template? It can&amp;#8217;t be empty after all!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Before looking for a replacement, it&amp;#8217;s important to understand that the template should encourage building an understanding and a detailed story rather than searching for a root cause. In &lt;a href=&quot;https://www.kitchensoap.com/2014/11/14/the-infinite-hows-or-the-dangers-of-the-five-whys/&quot;&gt;The Infinite Hows&lt;/a&gt;, John Allspaw highlights that &quot;Learning is the goal. Any prevention depends on that learning.&quot; and proposes using &lt;a href=&quot;http://www.kitchensoap.com/wp-content/uploads/2014/09/Velocity2014-PM-Fac-Handout-Debrief.pdf&quot;&gt;Debriefing Facilitation Prompts&lt;/a&gt; from &lt;em&gt;The Field Guide To Understanding Human Error&lt;/em&gt;, by Sidney Dekker.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having the prompts could be a great starting point as it asks you to take a look at the system from multiple angles and understand how each individual part of the system behaved during the incident. Moreover, the approach could further be expanded with the questions from the environment - the questions that can help build the story and facilitate learning. There could even be different prompts for different components that were affected. If Alice or Bob, or any other member of the crew were to use the prompts, they would immediately have to consider how the system behaved from multiple angles giving the investigation the necessary depth.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Five Whys analysis is a useful technique that is easy to remember and a helpful reminder to always look deeper. However, using it directly would only yield a cause, not the root cause, as rarely there is such thing as the root cause. The approach could still be used to deepen the scope of searching for action items. Still, structuring this tool into a template as the primary driver of a root cause analysis can cause more harm than good.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Instead, the template could facilitate learning and help build a story by asking questions that can help understand the whole story deeply. Then, a comprehensive list of action items can stem from the story preventing not only incidents with a similar &quot;root cause&quot; but a whole class of failures.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_thank_you_to&quot;&gt;Thank you to&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/ptuls&quot;&gt;Paul Tune&lt;/a&gt; for reviewing the article.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You for reading the article.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_share_this_article&quot;&gt;Share this article&lt;/h3&gt;
&lt;hr&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Just published some musings on using the five l̶i̶e̶s̶ whys analysis for incident postmortems and other investigations.&lt;a href=&quot;https://t.co/mMHdPB0F6U&quot;&gt;https://t.co/mMHdPB0F6U&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sergey Tselovalnikov (@SerCeMan) &lt;a href=&quot;https://twitter.com/SerCeMan/status/1448641518370103303?ref_src=twsrc%5Etfw&quot;&gt;October 14, 2021&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 14 Oct 2021 00:00:00 +1100
</pubDate>
</item>
<item>
<guid>
http://serce.me/posts/18-11-2020-allocate-direct/
</guid>
<link>
http://serce.me/posts/18-11-2020-allocate-direct/
</link>
<title>
Indirect Effects  of Allocate Direct
</title>
<description>
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hi, folks!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Every single program allocates memory. Byte buffers are at the core of many essential libraries which power the services the modern internet is built upon. If you&amp;#8217;re building such a library, or even just copying data between different files, chances are you&amp;#8217;ll need to allocate a buffer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In Java, &lt;code&gt;ByteBuffer&lt;/code&gt; is the class that allows you to do so. Once you&amp;#8217;ve decided to allocate a buffer, you&amp;#8217;ll be presented with two methods &lt;code&gt;allocate()&lt;/code&gt; and &lt;code&gt;allocateDirect()&lt;/code&gt;. Which one to use? The answer is, as always, it depends. If there were no tradeoffs, there wouldn&amp;#8217;t be two methods. In this article, I&amp;#8217;ll explore some of these tradeoffs, demystify this behaviour, and I hope that the answer will be clear for you by the end of it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/allocatedirect/itsgone.png&quot; alt=&quot;itsgone&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock text-center&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Yeah well, sometimes the things we do don’t matter right now. Sometimes they matter later. We have to care more about later sometimes, you know.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;attribution&quot;&gt;
&amp;#8212; Stan Marsh
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_two_buffers&quot;&gt;Two buffers&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At first glance, the two methods &lt;code&gt;allocate()&lt;/code&gt; and &lt;code&gt;allocateDirect()&lt;/code&gt; are very simple. The &lt;code&gt;allocate()&lt;/code&gt; allocates a buffer in the managed heap of the Java process, a part of this exact space which size is specified with the &lt;code&gt;Xmx&lt;/code&gt; option. The &lt;code&gt;allocateDirect()&lt;/code&gt; method allocates a buffer residing outside of the managed heap.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/allocatedirect/server3.png&quot; alt=&quot;server3&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This difference, however, creates a number of significant runtime implications, which I&amp;#8217;m going to dive into here. But first, let me start by telling a debugging story where direct byte buffers were the murderer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_the_story&quot;&gt;The story&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Every story needs a protagonist. In this case, the protagonist was a Java application built on top of RSocket, a modern application protocol. Here&amp;#8217;s the oversimplified version of the app which you can find on &lt;a href=&quot;https://github.com/SerCeMan/allocatedirect/blob/master/src/main/java/me/serce/allocatedirect/Main.java&quot;&gt;Github&lt;/a&gt;. Let&amp;#8217;s call this app an echo app. The echo code isn&amp;#8217;t trying to do anything complicated, it is a simple echo service built with an awesome &lt;a href=&quot;https://github.com/rsocket/rsocket-java&quot;&gt;rsocket-java&lt;/a&gt; library. All it does is spins up a client and a server, where the client sends messages, and the server echoes them back.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;var server = RSocketServer.create(echo) //...
var client = RSocketConnector.create() //...
while (true) {
  assert Objects.equals(client.send(), client.receive())
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
The supporting code for this article is available &lt;a href=&quot;https://github.com/SerCeMan/allocatedirect&quot;&gt;on Github&lt;/a&gt;. You can, and I highly encourage you to, choose to go through each step yourself by cloning the code and running each example with a simple bash script. All measurements were taken on an EC2 AWS &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/m5/&quot;&gt;m5.large&lt;/a&gt; instance. Unless specified otherwise, &lt;em&gt;Java 13&lt;/em&gt; is used. The point of this article is not to show the numbers but rather demonstrate the techniques that you can use to debug your own application.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The code is useless if it&amp;#8217;s just sitting in the repo and doing nothing, so let&amp;#8217;s clone the repo and start the app.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;git clone https://github.com/SerCeMan/allocatedirect.git
cd allocatedirect &amp;amp;&amp;amp; ./start.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The app starts, and you should see that the logs are flowing. As expected, now it&amp;#8217;s processing a large number of messages. However, if the echo app was exposed to users, they would start noticing significant pauses every now and then. All Java developers know that the first thing to look at in the case of spurious pauses is GC.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
You can find GC logs of the app are stored in &lt;code&gt;/tmp/${gcname}&lt;/code&gt;. The example logs for each run are also available in the &lt;a href=&quot;https://github.com/SerCeMan/allocatedirect/tree/master/logs&quot;&gt;repo&lt;/a&gt;. In this article, gceasy.io was used for visualisation. It&amp;#8217;s a great free online tool which supports the log format of multiple garbage collectors. Even though you can always visualise GC logs using a tool like gceasy, as we&amp;#8217;ll see later, the raw logs often contain a lot more information than most of the tools can display.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Indeed, GC logs show that GC is to blame here. The application is running under G1, which is the default collector since JDK 9. There are multiple young GC pauses on the graph. Young GC is a stop-the-world pause in GC in G1. The application stops completely to perform a cleanup. For the echo server, the graph shows multiple young GC pauses that last for 100-130ms and occur every 10 seconds.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;G1 GC&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/allocatedirect/g1before.png&quot; alt=&quot;g1before&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Luckily for us, in the last few years, there has been an amazing development in the GC space. There are not just one but two new fully concurrent garbage collectors, &lt;a href=&quot;https://wiki.openjdk.java.net/display/zgc/Main&quot;&gt;ZGC&lt;/a&gt; and &lt;a href=&quot;https://wiki.openjdk.java.net/display/shenandoah/Main&quot;&gt;Shenandoah&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While I&amp;#8217;ve had &lt;a href=&quot;https://twitter.com/SerCeMan/status/1246676501925224449&quot;&gt;great success&lt;/a&gt; with ZGC before, Shenandoah has a great advantage of being much friendlier to application memory consumption. Many applications, especially simple JSON in, JSON out stateless services are not memory-constrained. Some application, on the other hand, especially the ones that process a large number of connections might be very sensitive to memory usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Even though the echo app only has a single client and a server in its current state, it could as well handle tens of thousands of connections. It&amp;#8217;s time to enable Shenandoah,  and run the echo app again.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;./start.sh shen # starts with -XX:+UseShenandoahGC&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After enabling Shenandoah, the GC logs start showing an interesting picture. There is definitely a huge improvement in the pause frequency. The pauses now only occur every minute or so. However, the pauses are still around 90ms long, which is far away from the desired sub-millisecond pauses.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Shenandoah GC&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/allocatedirect/shenandoah.png&quot; alt=&quot;shenandoah&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now that the symptoms are clear, and the problem is reproducible, it&amp;#8217;s time to look at the cause. GC graphs don&amp;#8217;t show much more information. Looking at the raw logs directly, on the other hand, reveals the cause which is clearly stated right on the pause line.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-log&quot; data-lang=&quot;log&quot;&gt;...
[info][gc] GC(15) Pause Final Mark (process weakrefs) 86.167ms
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Turns out, weak references are to blame. Put simply, weak references are a way to keep an object in memory until there is a demand for this memory. Large in-memory caches is a common use-case for weak references. If there is enough free heap, a weak reference cache entry can stay there. As soon as GC figures out that there is not enough memory, it&amp;#8217;ll deallocate weak references. In most of the cases, this is a much better outcome than the application failing with an out of memory exception because of a cache.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A frantic search across the repository doesn’t show any usages of weak, soft or phantom references. Not even the search through the third party libraries can show anything. After staring at the metrics for a while, one of the graphs gives a clue! The long GC pauses correlate with a sudden drop in the number of direct byte buffers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;GC vs DirectByteBuffer count&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/allocatedirect/jmc-gc.png&quot; alt=&quot;jmc gc&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
You can get a similar graph by running the echo app and connecting to the JMX port. For this screenshot, I used Java Mission Control (JMC). The &lt;a href=&quot;https://github.com/SerCeMan/allocatedirect/blob/master/start.sh#L53&quot;&gt;start.sh&lt;/a&gt; script contains the options that you can enable to connect to an app with JMX remotely.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At first, the correlation might not make any sense. Byte buffers are not weak references, are they? They are not weak references themselves. However, you might notice, that creating a new direct byte buffer gives you back a plain &lt;code&gt;ByteBuffer&lt;/code&gt; interface which doesn&amp;#8217;t have a &lt;code&gt;close&lt;/code&gt; method or any other way of deallocating the buffer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;ByteBuffer buf = ByteBuffer.allocateDirect(42);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The underlying buffer needs to go away once the last reference to this buffer goes away. The modern API for this in Java is &lt;a href=&quot;https://docs.oracle.com/en/java/javase/13/docs/api/java.base/java/lang/ref/Cleaner.html&quot;&gt;&lt;code&gt;java.lang.ref.Cleaner&lt;/code&gt;&lt;/a&gt;. As we can see, it&amp;#8217;s exactly what &lt;code&gt;DirectByteBuffer&lt;/code&gt; class uses to determine when the underlying buffer should be deallocated.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;https://github.com/openjdk/jdk13/blob/dcd4014cd8a6f49a564cbb95387ad01a80a20bed/src/java.base/share/classes/java/nio/Direct-X-Buffer.java.template#L113-L141&quot;&gt;DirectByteBuffer&lt;/a&gt; constructor&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;DirectByteBuffer(int cap) {
    // ...
    base = UNSAFE.allocateMemory(size); // malloc call
    UNSAFE.setMemory(base, size, (byte) 0);
    // ...
    cleaner = Cleaner.create(this, new Deallocator(base, size, cap));
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Yet, there are no usages of direct buffers in the code of the echo app either, so how could we find them? One way would be to search through the third party libraries using IntelliJ. The approach would work very well for the echo example but would completely fail for any real applications of a decent size. There are just way too many places where byte buffers are used. Looking at the graphs, one can notice that the number of created buffers per minute is huge, literally millions of them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Instead of searching through the code to find all byte buffer references, it is easier to find the place at runtime. One way to find out where the majority of the buffers is created is to fire up the async profiler and profile the &lt;a href=&quot;https://man7.org/linux/man-pages/man3/malloc.3.html&quot;&gt;&lt;code&gt;malloc&lt;/code&gt;&lt;/a&gt; calls which are used by direct buffers to allocate memory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# async profiler can be downloaded from https://github.com/jvm-profiling-tools/async-profiler
./profiler.sh -d 30 -f /tmp/flamegraph.svg $(pgrep -f java) -e malloc&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While running, the profiler managed to sample more than 500000 malloc calls which non-ambiguously show where all of the buffers were created from.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;malloc calls&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;object type=&quot;image/svg+xml&quot; data=&quot;/img/allocatedirect/mallocflame.svg&quot;&gt;&lt;span class=&quot;alt&quot;&gt;mallocflame&lt;/span&gt;&lt;/object&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
The flame graph above visualises the code paths where most of the captured malloc calls occur. The wider the column is, the larger the number of times the code path appeared in the sample. This graph, as well as other flame graphs in this article, is clickable. You can read more on how to read flame graphs &lt;a href=&quot;http://www.brendangregg.com/flamegraphs.html&quot;&gt;here&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As it turned out, there was a place in the code which was using direct buffers. With this rich knowledge of where exactly the direct buffer allocations occur, creating a fix is easy. All that&amp;#8217;s needed is to make a one line change and to replace &lt;code&gt;allocateDirect&lt;/code&gt; with &lt;code&gt;allocate&lt;/code&gt; and send a &lt;a href=&quot;https://github.com/rsocket/rsocket-java/pull/945&quot;&gt;PR upstream&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Running the same app on shenandoah after applying the single line change produces a completely different graph which pleases the eyes with sub-millisecond GC pauses.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Shenandoah GC&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/allocatedirect/shenandoah-heap.png&quot; alt=&quot;shenandoah heap&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_the_costs&quot;&gt;The costs&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The story revealed a dark side of direct byte buffers. If there is a dark side, there must be a bright side as well! There is. But before we look at the bright side, we need to explore a few more sides which also appeared to be grey.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_allocations&quot;&gt;Allocations&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Previously, we’ve observed implicit deallocations costs, so now it’s time to take a look at allocations. Could direct buffers be much cheaper to create? After all, going off-heap has been a performance trend for a while. A small benchmark can help to estimate the costs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;https://github.com/SerCeMan/allocatedirect/blob/master/bench/src/main/java/me/serce/AllocateBuffer1.java&quot;&gt;AllocationBenchmark.java&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Param({&quot;128&quot;, &quot;1024&quot;, &quot;16384&quot;})
int size;

@Benchmark
public ByteBuffer heap() {
  return ByteBuffer.allocate(size);
}

@Benchmark
public ByteBuffer direct() {
  return ByteBuffer.allocateDirect(size);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After cloning the repo, you can run the benchmark yourself with the command below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# Don't just read! Clone the repo and try yourself! 🤓
./bench.sh alloc1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The absolute numbers are not that interesting. Even the slowest operation only takes a few microseconds. But the difference between the heap buffers and direct buffers is fascinating.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Benchmark               (size)  Mode  Cnt     Score     Error  Units
AllocateBuffer1.direct     128  avgt    5  1022.137 ± 148.510  ns/op
AllocateBuffer1.heap       128  avgt    5    23.969 ±   0.051  ns/op

AllocateBuffer1.direct    1024  avgt    5  1228.785 ± 127.090  ns/op
AllocateBuffer1.heap      1024  avgt    5   179.350 ±   2.989  ns/op

AllocateBuffer1.direct   16384  avgt    5  3039.485 ± 111.714  ns/op
AllocateBuffer1.heap     16384  avgt    5  2620.722 ±   5.395  ns/op&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Even though direct buffers lose in all of the runs, the difference is much more noticeable on small buffers while on large buffers, the overhead is almost negligible. Due to the 50x difference on a small buffer, it’s a much more compelling example to look into. Let’s start a benchmark again, make it run for much longer, and use async profiler to see what where the time is spent.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;ByteBuffer.allocateDirect()&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;object type=&quot;image/svg+xml&quot; data=&quot;/img/allocatedirect/alloc_direct_perf.svg&quot;&gt;&lt;span class=&quot;alt&quot;&gt;alloc direct perf&lt;/span&gt;&lt;/object&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The flame graph already hints towards some of the overhead. Not only the direct buffers need to allocate memory, but it also needs to reserve it to check the maximum native memory limit. On top of this, the buffer needs to be zeroed as &lt;code&gt;malloc&lt;/code&gt; can&amp;#8217;t guarantee that it doesn&amp;#8217;t return you some garbage while the buffer needs to be ready to use. And finally, it needs to register itself for deallocation as a soft reference. All of this seems like a lot of work, but the actual allocation still takes a half of the time! So, even if the heap buffer doesn&amp;#8217;t need to do any work other than calling &lt;code&gt;malloc&lt;/code&gt;, it should only be as twice as slow, not 50 times! Profiling heap buffer allocations can hopefully reveal where such a vast difference is coming from.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;ByteBuffer.allocate()&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;object type=&quot;image/svg+xml&quot; data=&quot;/img/allocatedirect/alloc_heap_perf.svg&quot;&gt;&lt;span class=&quot;alt&quot;&gt;alloc heap perf&lt;/span&gt;&lt;/object&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The heap buffer flame graph is surprisingly blank. There isn&amp;#8217;t much happening on the graph. Yet, there are still some allocations in the yellow flame tower on the right. However, the whole allocation path only takes 2% of the time, and the rest is nothing? Exploring the yellow tower gives a further clue. Most of its time is taken by a function that&amp;#8217;s called &lt;code&gt;MemAllocator::allocate_inside_tlab_slow&lt;/code&gt;. The meaning of the &lt;code&gt;allocate_slow&lt;/code&gt; part is self-explanatory, but it&amp;#8217;s &lt;code&gt;inside_tlab&lt;/code&gt; that is the answer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;TLAB stands for Thread Local Allocation Buffer. TLAB is a space in the Eden, the space where all new objects are born, dedicated for each thread to allocate objects. When different threads allocate memory, they don’t have to contend on the global memory. Every thread allocates objects locally, and because the buffer is not shared with other threads, there is no need to use call &lt;code&gt;malloc&lt;/code&gt;. All that’s needed is to move the pointer by a few bytes. The fact that most of the allocations happen in TLAB could explain why heap buffers are so much faster when their size is small. When the size is large, the allocations won’t occur in TLAB due to the limits on its size, which will result in buffer allocation times being almost on par.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now that we&amp;#8217;ve assumed that we know why it&amp;#8217;s so much faster, can we jump to the next section? Not so fast!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So far, TLAB is just a theory, and we need to conduct an experiment to validate it. One of the easiest ways is to simply disable TLAB with the &lt;code&gt;-XX:-UseTLAB&lt;/code&gt; options.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;// run with ./bench.sh alloc4
@Fork(jvmArgsAppend = { &quot;-XX:-UseTLAB&quot; })
@Benchmark
public ByteBuffer heap() {
  return ByteBuffer.allocate(size);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Benchmark             (size)  Mode  Cnt    Score   Error  Units
AllocateBuffer2.heap     128  avgt    5  151.999 ± 8.477  ns/op&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Were we right? Yes and no. The performance results with disabled TLAB are not as impressive anymore. Though, the pure allocation time is still about three times faster even considering that the benchmark needs to not only allocate memory for the buffer itself but also for the &lt;code&gt;ByteBuffer&lt;/code&gt; class. The still significant difference shows the cost of going back to the operating system with a syscall every time to ask for more memory with occasional page faults.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;ByteBuffer.allocate(), -XX:-UseTLAB&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;object type=&quot;image/svg+xml&quot; data=&quot;/img/allocatedirect/alloc_heap_no_tlab.svg&quot;&gt;&lt;span class=&quot;alt&quot;&gt;alloc heap no tlab&lt;/span&gt;&lt;/object&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As a rule of thumb, if your buffers are mostly short-lived and small, using heap byte buffers will likely be a more performant choice for you. Conveniently, it&amp;#8217;s exactly what the javadoc of the ByteBuffer class is warning us about.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock&quot;&gt;
&lt;blockquote&gt;
It is therefore recommended that direct buffers be allocated primarily for large, long-lived buffers that are subject to the underlying system&amp;#8217;s native I/O operations.
&lt;/blockquote&gt;
&lt;div class=&quot;attribution&quot;&gt;
&amp;#8212; ByteBuffer.java
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect4&quot;&gt;
&lt;h5 id=&quot;_memory_costs&quot;&gt;Memory costs&lt;/h5&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So far, we&amp;#8217;ve only been measuring allocations. Still, looking at the flame graphs, we can also see the de-allocation path which is frequently invoked by JMH that runs the benchmarks by explicitly invoking &lt;a href=&quot;https://github.com/openjdk/jmh/blob/4264de9486c32b48da8161e3ac076a0187b4176f/jmh-core/src/main/java/org/openjdk/jmh/runner/BaseRunner.java#L273&quot;&gt;&lt;code&gt;System.gc()&lt;/code&gt;&lt;/a&gt; and finalization before each iteration. That way, the previously allocated buffers will be deallocated.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;However, in the real applications, as we saw in the debugging story, we&amp;#8217;re at a mercy of the GC to deallocate those buffers. In this case, the amount of memory consumed by the app might be hard to predict as it depends on the GC and how the GC behaves on this workload. For how long would the following code run?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class HeapMemoryChaser {
  public static void main(String[] args) {
    while (true) {
      ByteBuffer.allocate(1024);
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The code above contains an infinite loop, so you can say forever, and you will be completely right. There are no conditions, so there is nothing that can prevent the loop from running. Will this snippet run forever too?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class DirectMemoryChaser {
  public static void main(String[] args) {
    while (true) {
      ByteBuffer.allocateDirect(1024);
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Will the above code run forever as well? It depends on how lucky we are. There are no guarantees on when a GC would clean up the allocated direct buffers. Various JVM options can vary the result from run forever with no issues to a crash after a few seconds. Running the above code with &lt;code&gt;-Xmx6G&lt;/code&gt; on a VM with 8GB RAM runs for about 20 seconds until it gets killed by the operating system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ time java -Xmx6G DirectMemoryChaser
Killed

real	0m24.211s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;code&gt;dmesg&lt;/code&gt; shows an insightful message explaining that the process was killed due to lack of memory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[...] Out of memory: Killed process 4560 (java) total-vm:10119088kB, anon-rss:7624780kB, file-rss:1336kB, shmem-rss:0kB, UID:1000 pgtables:15216kB oom_score_adj:0
[...] oom_reaper: reaped process 4560 (java), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Allocating direct buffers without care can cause the app to go far beyond the expected memory usage as there are no guarantees on when the soft references are going to be cleaned. Crashing right after the start is counterintuitively a good result. At least, you can observe the failure, reproduce it, understand it and fix it. A crash after a few hours of running is much worse, and without a clear feedback loop, it&amp;#8217;s much harder to resolve the issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When debugging such issues, or as a preventative measure, consider using &lt;code&gt;-XX:+AlwaysPreTouch&lt;/code&gt; to at least exclude the heap growth out of the equation. One way to prevent this is to run the infinite growth of direct buffers is to use &lt;code&gt;-XX:MaxDirectMemorySize=${MAX_DIRECT_MEM}&lt;/code&gt; to ensure that the usage of direct memory doesn&amp;#8217;t grow uncontrollably.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_the_benefits&quot;&gt;The benefits&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So far, the direct byte buffers have only caused troubles. In the story, switching to heap-based byte buffers was a clear win, though there are a lot of hidden dangers in using them. Would it be reasonable to use heap byte buffers only and never use direct buffers? The choice exists for a reason, and there reasons to use direct buffers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_off_heap_graal&quot;&gt;Off Heap Graal&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We have observed problems with allocations and deallocations, so what if a buffer is only allocated once and never deallocated? One buffer is not enough most of the time, so you can create a pool of buffers, borrow them for some time and then return back. It is what Netty does with their &lt;a href=&quot;https://netty.io/wiki/using-as-a-generic-library.html&quot;&gt;&lt;code&gt;ByteBuf&lt;/code&gt;&lt;/a&gt; classes which are built to fix some of the downsides of the &lt;code&gt;ByteBuffer&lt;/code&gt; class. Nevertheless, it&amp;#8217;s still not clear why one should prefer direct buffers over heap buffers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Avoiding GC altogether could be one of the reason. You could be managing terabytes of memory without any GC overhead. While you could manage large amounts of memory with direct byte buffers, there is a limit of 2^31 on the indices that you can use with a single buffer. A solution is coming in the form of a &lt;a href=&quot;https://openjdk.java.net/jeps/383&quot;&gt;Foreign-Memory Access API&lt;/a&gt; which is available for the second preview in JDK 15. But avoiding GC is not the main reason.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_io&quot;&gt;IO&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The IO is where direct byte buffers shine! Let&amp;#8217;s say we need to copy some memory between two files. Heap byte buffers are obviously backed by memory in a heap, so the contents of the files would have to be copied to be sent back seconds later. This can be avoided completely with direct byte buffers. Direct byte buffers excel when you don&amp;#8217;t need a buffer per se but rather a pointer to a piece of memory somewhere outside of the heap. Again, at this point, this is only a hypothesis of a random person on the internet. Let&amp;#8217;s prove or disprove it with the following benchmark.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;https://github.com/SerCeMan/allocatedirect/blob/master/bench/src/main/java/me/serce/CopyFileBenchmark.java&quot;&gt;CopyFileBenchmark.java&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Benchmark // ./bench.sh reverse
public void reverseBytesInFiles() throws Exception {
  ByteBuffer buf = this.buffer;
  buf.clear();
  try (FileChannel channel1 = FileChannel.open(Paths.get(DIR + &quot;file1&quot;), READ);
       FileChannel channel2 = FileChannel.open(Paths.get(DIR + &quot;file2&quot;), WRITE)) {
    while (buf.hasRemaining()) {
      channel1.read(buf);
    }
    buf.put(0, buf.get(SIZE - 1));
    buf.flip();
    while (buf.hasRemaining()) {
      channel2.write(buf);
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The code above reads  64 MB of random data from the first files, reverses the byte order of each long in the array and then puts it back. Reversing the first and the last bytes here is a tiny operation which the only goal is to modify the contents of the file in some way as copying could simply be done by calling &lt;code&gt;channel.transferTo&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The results show that the direct buffer is the clear winner, almost twice as fast!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Benchmark                              (bufferType)  Mode  Cnt   Score   Error  Units
CopyFileBenchmark.reverseBytesInFiles        direct  avgt    5  36.383 ± 0.683  ms/op
CopyFileBenchmark.reverseBytesInFiles          heap  avgt    5  59.816 ± 0.834  ms/op&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The next step is to understand where the time was spent to validate our hypothesis. Taking a flamegraph for the direct buffer shows what we expected — all of the time spent in kernel reading and writing files.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Directy Buffer&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;object type=&quot;image/svg+xml&quot; data=&quot;/img/allocatedirect/reverse_offheap.svg&quot;&gt;&lt;span class=&quot;alt&quot;&gt;reverse offheap&lt;/span&gt;&lt;/object&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For the heap buffer, for both operations, reading and writing, the memory has to be copied first between the buffers which we can clearly see from the flamegraph. A solid chunk of it is taken by the &lt;code&gt;copyMemory&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Heap Buffer&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;object type=&quot;image/svg+xml&quot; data=&quot;/img/allocatedirect/reverse_heap.svg&quot;&gt;&lt;span class=&quot;alt&quot;&gt;reverse heap&lt;/span&gt;&lt;/object&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The IO here does not only refer to writing to disk, but it can also be writing to a socket which is what a considerable portion of the java applications is performing all day non-stop. As you can see, carefully choosing your buffers can significantly affect performance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_endianness&quot;&gt;Endianness&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Can direct byte buffers be even faster? While reading the javadoc for create methods, note an important remark:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock&quot;&gt;
&lt;blockquote&gt;
The new buffer&amp;#8217;s position will be zero, its limit will be its capacity, its mark will be undefined, each of its elements will be initialized to zero, and its byte order will be ByteOrder#BIG_ENDIAN.
&lt;/blockquote&gt;
&lt;div class=&quot;attribution&quot;&gt;
&amp;#8212; ByteBuffer.java
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The byte order of byte buffers created in java is always big endian by default. While having an always predictable default is great, it also means that sometimes, it might not match the endianness of the underlying platform. In the case of an &lt;code&gt;m5.large&lt;/code&gt; AWS instance, this is indeed the case.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;jshell&amp;gt; java.nio.ByteOrder.nativeOrder()
$1 ==&amp;gt; LITTLE_ENDIAN&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This fact immediately raises the question if, or rather when changing endianness can yield any significant performance wins. The only way to find out is to measure it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;https://github.com/SerCeMan/allocatedirect/blob/master/bench/src/main/java/me/serce/OrderBenchmark.java&quot;&gt;OrderBenchmark.java&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;static final int SIZE = 1024 * 1024 * 1024;

@Param({&quot;direct-native-order&quot;, &quot;direct&quot;})
String bufferType;

@Setup
public void setUp() throws Exception {
  switch (bufferType) {
    case &quot;direct&quot;:
      buffer = ByteBuffer.allocateDirect(SIZE); break;
    case &quot;direct-native-order&quot;:
      buffer = ByteBuffer.allocateDirect(SIZE).order(ByteOrder.nativeOrder()); break;
  }
  channel = FileChannel.open(Paths.get(&quot;/dev/urandom&quot;), READ);
  while (buffer.hasRemaining()) { channel.read(buffer); }
  buffer.flip();
  this.buffer = buffer.asLongBuffer();
}

@Benchmark // run with ./bench.sh order
public long sumBytes() {
  long sum = 0;
  for (int i = 0; i &amp;lt; SIZE / 8; i++) {
    sum += buffer.get(i);
  }
  return sum;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The above benchmark measures a specific use-case. We load a gigabyte worth of random longs into memory. Then, we simply read them one by one. It’s interesting that depending on the endianness, the result will be different as it affects the order of the bytes. We don’t care about the byte order for this use-case, however, as a random value with reversed byte order is still a random value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Benchmark                       (bufferType)  Mode  Cnt    Score   Error  Units
OrderBenchmark.sumBytes  direct-native-order  avgt    5  136.025 ± 2.262  ms/op
OrderBenchmark.sumBytes               direct  avgt    5  195.980 ± 8.360  ms/op&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first impression is that iterating through a gigabyte worth of random memory is pretty darn fast. The second is that the native order byte buffer is performing 1.5 times faster! As before, running async profiler helps to reveal the reason why the native order is more performant.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Non-native (Big Endian)&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;object type=&quot;image/svg+xml&quot; data=&quot;/img/allocatedirect/sum_big_endian.svg&quot;&gt;&lt;span class=&quot;alt&quot;&gt;sum big endian&lt;/span&gt;&lt;/object&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Native (Little Endian)&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;object type=&quot;image/svg+xml&quot; data=&quot;/img/allocatedirect/sum_native_endian.svg&quot;&gt;&lt;span class=&quot;alt&quot;&gt;sum native endian&lt;/span&gt;&lt;/object&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Comparing the graphs above, the first difference that stands out is that byte buffer classes are actually different depending on the byte order. The native buffer is &lt;code&gt;DirectLongBufferU&lt;/code&gt; while the non-native one is &lt;code&gt;DirectLongBufferS&lt;/code&gt;. The main difference between them is the presence of the &lt;code&gt;Bits.swap&lt;/code&gt; method.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Looking further into the method, we can see that it delegates directly to &lt;code&gt;Long.reverseBytes&lt;/code&gt;. While its implementation in Java is quite complex, one can notice the &lt;code&gt;@HotSpotIntrinsicCandidate&lt;/code&gt; annotation. The annotation is a signal that at runtime, JIT could replace the method with pre-prepared assembly code. Adding a set of JVM options, &lt;code&gt;-XX:CompileCommand=print,*OrderBenchmark.sumBytes*&lt;/code&gt;, to the benchmark allows us to peek at the resulting assembly code to understand how exactly the &lt;code&gt;reverseBytes&lt;/code&gt; affects the resulting code.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-cols stretch&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Non-native (Big Endian)&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Native (Little Endian)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;....
loop:
 mov    r10d,DWORD PTR [rdx+0x14]
 mov    ecx,DWORD PTR [r10+0x8]
 mov    r8,r10                  ; &amp;lt;- buffer
 cmp    ecx,0x16577b
 jne    0x00007f9d5c277070
 mov    ebx,DWORD PTR [r8+0x1c] ; &amp;lt;- limit
 cmp    r11d,ebx
 jge    0x00007f9d5c277078  ; checkIndex(i)
 mov    r10,QWORD PTR [r8+0x10]
 movsxd r8,r11d
 shl    r8,0x3
 add    r8,r10
 mov    r10,r8
 mov    r10,QWORD PTR [r10] ; r10 = get(i)
 bswap  r10                 ; reverseBytes(r10)
 add    rax,r10             ; sum += r10
 inc    r11d                ; i+=1
 cmp    r11d,0x8000000      ; i &amp;lt; SIZE/8
 jl     loop
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;...
loop:
 mov    r11d,DWORD PTR [r8+0x14] ; &amp;lt;- buffer
 mov    r9d,DWORD PTR [r11+0x1c] ; &amp;lt;- limit
 cmp    ecx,r9d
 jge    0x00007f268c274f57 ; checkIndex(i)
 mov    r10,QWORD PTR [r11+0x10]
 movsxd r11,ecx
 shl    r11,0x3
 add    r11,r10
 mov    r10,r11
 add    rbx,QWORD PTR [r10] ; sum += get(i)
 inc    ecx                 ; i+=1
 cmp    ecx,0x8000000       ; i &amp;lt; SIZE/8
 jl     loop
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Comparing the compilations listings of these two implementations, we can notice that the biggest difference between them is the &lt;code&gt;bswap&lt;/code&gt; instruction which is the essence of the &lt;code&gt;Bytes.swap&lt;/code&gt; method. As expected, it reverses the byte order every time a long is read from the buffer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Reading a gigabyte of memory into longs is an interesting workload, but it’s not necessarily the one that you’re likely to encounter in production. Endianness can be a useful thing to remember about, but unless working with native libraries or working with massive files, it’s unlikely to be a concern.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Every non-trivial Java application directly or indirectly uses byte buffers. On the surface, ByteBuffer is a simple class. It’s just a pointer to a chunk of memory. Nevertheless, even by looking at such a simple class, you can discover a deep rabbit hole. Even though we’ve only looked at the tip of the iceberg, I hope that you have a clear idea now of when you could use a heap buffer, and when you would choose a direct buffer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Modern JVM runtimes are complicated environments. While they provide sane defaults, they also present multiple options. The choice is always there, it&amp;#8217;s up to you to make that choice, but it&amp;#8217;s crucial to be aware of the consequences. Fortunatelly, JVM runtimes also come with a whole lot of various observability tools, JMX metrics, GC logs, profilers, and if you really want it&amp;#8217;s not even that hard to look at the generated assembly code. Using techniques shown in this article, you can make a choice not for the workload of a guy from the internet, but for &lt;em&gt;your&lt;/em&gt; workload, which can result in amazing results in production later. We have to care more about later sometimes, you know.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_thank_you_to&quot;&gt;Thank you to&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Uri Baghin and &lt;a href=&quot;https://twitter.com/ptuls&quot;&gt;Paul Tune&lt;/a&gt; for reviewing the article.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You for reading the article.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_share_this_article&quot;&gt;Share this article&lt;/h3&gt;
&lt;hr&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Just published a new blog post &amp;quot;Indirect Effects of Allocate Direct&amp;quot;. JVM, byte buffers, native memory, debugging GC side effects, profiling in prod, and even some Intel x86 inside. &lt;a href=&quot;https://t.co/oE6s88CMeu&quot;&gt;https://t.co/oE6s88CMeu&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sergey Tselovalnikov (@SerCeMan) &lt;a href=&quot;https://twitter.com/SerCeMan/status/1328999241541328897?ref_src=twsrc%5Etfw&quot;&gt;November 18, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_references&quot;&gt;References&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/jvm-profiling-tools/async-profiler/&quot;&gt;Async Profiler&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=iwSCtxMbBLI&quot;&gt;Beyond ByteBuffers by Brian Goetz&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://gceasy.io/&quot;&gt;GC Easy GC Analyser&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://shipilev.net/jvm/anatomy-quarks/4-tlab-allocation/&quot;&gt;JVM Anatomy Quark #4: TLAB allocation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=DKJ0w30M0vg&quot;&gt;Netty - One Framework to rule them all by Norman Maurer&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://netty.io/wiki/using-as-a-generic-library.html&quot;&gt;Netty&amp;#8217;s ByteBuf API&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 18 Nov 2020 00:00:00 +1100
</pubDate>
</item>
<item>
<guid>
http://serce.me/posts/23-07-2020-you-dont-need-no-service-mesh/
</guid>
<link>
http://serce.me/posts/23-07-2020-you-dont-need-no-service-mesh/
</link>
<title>
You don't need no Service Mesh
</title>
<description>
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hi!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Service meshes have attracted an enormous amount of hype around them. With at least a few talks about service meshes during each tech conference, one can easily be convinced that having a service mesh in their infrastructure is a must. However, hype isn’t a good indicator of whether the new shiny tech is the right solution for your problems. So below, I’ll try to express an anti-hype opinion on service meshes to hopefully make it less confusing when you want to decide whether you may or may not need one.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/servicemesh/rick.png&quot; alt=&quot;rick&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock text-center&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There&amp;#8217;s a lesson here, and I&amp;#8217;m not going to be the one to figure it out.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;attribution&quot;&gt;
&amp;#8212; Rick Sanchez
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_the_invention&quot;&gt;The invention&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s take a step back in history and take a look at one of the &lt;a href=&quot;https://eng.lyft.com/envoy-7-months-later-41986c2fd443&quot;&gt;early articles&lt;/a&gt; about introducing Envoy at Lyft.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As it turns out, almost every company with a moderately-sized service oriented architecture is having the same problems that Lyft did prior to the development and deployment of Envoy:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;An architecture composed of a variety of languages, each containing a half-baked RPC library, including partial (or zero) implementations of rate limiting, circuit breaking, timeouts, retries, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Differing or partial implementations of stats, logging, and ….&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While Envoy is not a service mesh by itself, the outlined problems describe the exact reason why service meshes were invented. They add “rate limiting, circuit breaking, …” and other reliability, observability, and security features to the services by enforcing the communication to go through the service mesh proxies, a data plane. Additionally, they require a separate component, a control plane, to control the configuration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;However, at this point, a lot of people miss the context in which service meshes were introduced. Service meshes are able to solve the problem not because it’s impossible to solve them in any other way. There are many battle-proof RPC libraries that take on the challenges of a separate data plane layer, &lt;a href=&quot;https://github.com/twitter/finagle&quot;&gt;Finagle&lt;/a&gt;, &lt;a href=&quot;https://github.com/grpc&quot;&gt;gRPC&lt;/a&gt;, &lt;a href=&quot;https://github.com/line/armeria&quot;&gt;Armeria&lt;/a&gt;, &lt;a href=&quot;https://github.com/apple/servicetalk&quot;&gt;Servicetalk&lt;/a&gt;, to name a few. After all, the very first service mesh - Linkerd 1.0 &lt;a href=&quot;https://github.com/linkerd/linkerd&quot;&gt;is powered by Finagle&lt;/a&gt;. The RPC libraries will need a component which provides service discovery and configuration management to make it a true mesh. For instance, Zookeeper, or Consul, a component that service meshes call a control plane.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Why introduce a new concept to solve the problems that have been solved before? The service mesh concept wasn’t introduced to address problems that hadn’t been addressed before but rather address them in a way that doesn’t require any modifications to the application code, which is incredibly convenient when it’s hard to introduce an RPC layer into an existing heterogeneous microservice environment.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When you hear service mesh, Istio with Envoy might be the first thing that comes to mind, but it wasn’t the first service mesh to enter the market. Linkerd authors who pioneered the space, described exactly this situation in the &lt;a href=&quot;https://linkerd.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/#why-is-the-service-mesh-necessary&quot;&gt;&quot;why is the service mesh necessary&quot;&lt;/a&gt;. Interestingly, in many hype-y articles on the Internet this context is often forgotten, or omitted.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Solving a problem well, even if it’s a problem that a lot of people have, doesn’t magically provide the tech with a lot of hype. There is always a sponsor behind it. I don’t know who the sponsor was here, and I’m going to speculate, but it’s hard to sell an RPC library in the world where open source is a fundamental requirement. There is no clear business model there, that’s why most of the mature RPC libraries were open-sourced by large tech companies for which it’s not a part of the core business model. A library is just code, not a piece of infrastructure. Service meshes are a different story. It’s an isolated non-trivial piece of infrastructure. As a vendor, not only can you provide consultancy around the configuration and deployment, but you can also sell complete hosted solutions around it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_disillusionments&quot;&gt;Disillusionments&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now that we’ve established the problems, the solution, and most importantly, the context in which the solution was made, let’s take a look at the alternatives. The most obvious one, in the spirit of KISS, is to use an RPC library for your preferred language. Here is where the context is crucial: if you have a large fleet of services, each written in its own language/ecosystem, and the only language that they share is HTTP then having a single shared RPC library is going to be hard. Perhaps, you’ve got a fabric of deployed and running services, but everyone is afraid of touching them, no one knows how they work, and each redeploy is an adventure. A service mesh is here to help you, because at least you’ll be able to roll out new infrastructure features to the mesh regularly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;On the other hand, if you have a fleet of healthy services written in a single application stack, then it’s a good idea to think twice before introducing a service mesh. By simply introducing or evolving a shared RPC library, you’ll get the exact same benefits and avoid dealing with the downsides of maintaining service meshes. By studying the service mesh limitations thoroughly, you can avoid finding yourself in the trough of disillusionment.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/servicemesh/curve.png&quot; alt=&quot;Hype Cycle&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_different_ecosystem&quot;&gt;Different ecosystem&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The ecosystem of the service mesh of your choice will likely be different from the ecosystem of your services. Beautiful websites always make you believe that the solution is plug&amp;#8217;n&amp;#8217;play, always works and never goes down. In reality, sooner or later problems, bugs, quirks in behaviour will reveal themselves, as they always do. At that point, you’ll need to have engineers who work on the service-mesh’s ecosystem which when it’s different from the main app, effectively limits the set of people who can introduce changes or fix problems. This is likely to reintroduce silos, which is against the whole DevOps spirit. Yes, having a DevOps team of engineers who are doing DevOps-y things &lt;a href=&quot;https://continuousdelivery.com/2012/10/theres-no-such-thing-as-a-devops-team/&quot;&gt;is against DevOps&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_unnecessary_overhead&quot;&gt;Unnecessary overhead&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Not only having a proxy in front of each service adds overhead (often significant, talking about &lt;a href=&quot;https://istio.io/latest/docs/ops/deployment/performance-and-scalability/&quot;&gt;90pt&lt;/a&gt; rather than 99pt in the performance summary &lt;a href=&quot;https://www.infoq.com/presentations/latency-response-time/&quot;&gt;doesn’t make software run faster&lt;/a&gt;) and consumes resources, but you also requires time (or rather a team of people) to manage them. Yes, it can help to make some of the tasks potentially easier - yay, you can now add canary deployments with a few lines of YAML to simple applications now. However, you still need to manage canary deployments of the proxies themselves which don’t have a proxy in front of them. The problems just get pushed up the stack.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_limiting_your_architecture_to_what_the_proxy_supports&quot;&gt;Limiting your architecture to what The Proxy supports.&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you’re reading this paragraph, HTTP/3 is slowly being rolled out to the Internet. It uses UDP as transport. Why use UDP rather than create a completely new protocol you ask? That’s because anything but TCP and UDP is simply “blocked” by the boxes, various proxies on the internet - routers, gateways, etc. This phenomenon got named &lt;a href=&quot;https://http3-explained.haxx.se/en/why-quic/why-ossification&quot;&gt;ossification&lt;/a&gt;. So, only TCP or UDP are left is the practical chose, and even UDP is partially blocked by various corporate proxies which slows down the adoption.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Even though your microservice environment is probably much smaller compared to the Internet, you can draw parallels with service meshes. Proxies can ossify your application architecture by limiting how your services talk to each other, and there is not much benefit in having proxies if you can bypass them. Suppose you want to build a reactive application which is using RSocket over pure tcp? Or perhaps a message-driven application using an actor model? Or maybe push the performance boundaries with Aeron? Not going to happen until the box in the middle becomes aware of the protocol.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_do_i_need_one&quot;&gt;Do I need one?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What does it all mean for you as an engineer? The answer to whether you need to adopt the service mesh approach comes down to the state of the microservice environment you’re trying to improve. As we have established, compared to an RPC framework, service meshes allow you to:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Deploy the infra changes more often than deploying your services.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Introduce infra changes without touching the service code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The point 1. is important when for whatever reason you can’t redeploy your services very often, e.g. maybe no one remembers how it’s done anymore, or maybe there are other restrictions. The point 2. is important when your stack is heterogeneous, e.g. some services are built in Go, some in Java, some in Haskell, etc. Where are you on the interval from a huge set of heterogeneous services with unknown deployment schedules to a set of homogenous regularly deployed services defines whether a service mesh is the best solution for you.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Service meshes have a lot of hype around them, and way too much in my opinion. However, before committing to a piece of technology, it’s crucial to understand the problems it solves, and the context in which the solution was made. A service mesh is not an ultimate “good practice” but simply one of the patterns to solve a set of issues, and it&amp;#8217;s quite a heavy one.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Rather than jumping on board, look carefully - the last thing you want is to find out that you have invested in a solution for a problem that you don’t have. Service meshes are an amazing piece of tech solving a whole lot of problems. Not in every case, it is the best solution.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_thank_you_to&quot;&gt;Thank you to&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You for reading this article.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/ptuls&quot;&gt;Paul Tune&lt;/a&gt; for reviewing the article.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_share_this_article&quot;&gt;Share this article&lt;/h3&gt;
&lt;hr&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;&amp;quot;You don&amp;#39;t need no Service Mesh&amp;quot;. Just published a new blog post with an anti-hype opinion on the over-hyped topic. &lt;a href=&quot;https://t.co/SVXS3nWKzj&quot;&gt;https://t.co/SVXS3nWKzj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sergey Tselovalnikov (@SerCeMan) &lt;a href=&quot;https://twitter.com/SerCeMan/status/1286242507664191488?ref_src=twsrc%5Etfw&quot;&gt;July 23, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_references&quot;&gt;References&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://eng.lyft.com/envoy-7-months-later-41986c2fd443&quot; class=&quot;bare&quot;&gt;https://eng.lyft.com/envoy-7-months-later-41986c2fd443&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/linkerd/linkerd/&quot; class=&quot;bare&quot;&gt;https://github.com/linkerd/linkerd/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://servicemesh.io/&quot; class=&quot;bare&quot;&gt;https://servicemesh.io/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://continuousdelivery.com/2012/10/theres-no-such-thing-as-a-devops-team/&quot; class=&quot;bare&quot;&gt;https://continuousdelivery.com/2012/10/theres-no-such-thing-as-a-devops-team/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://http3-explained.haxx.se/en/why-quic/why-ossification&quot; class=&quot;bare&quot;&gt;https://http3-explained.haxx.se/en/why-quic/why-ossification&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://linkerd.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/#why-is-the-service-mesh-necessary&quot; class=&quot;bare&quot;&gt;https://linkerd.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/#why-is-the-service-mesh-necessary&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 23 Jul 2020 00:00:00 +1000
</pubDate>
</item>
<item>
<guid>
http://serce.me/posts/16-05-2019-the-matter-of-time/
</guid>
<link>
http://serce.me/posts/16-05-2019-the-matter-of-time/
</link>
<title>
The matter of time()
</title>
<description>
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hi!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As software engineers, we all rely on the notion of time: a crucial concept in ensuring that events in our programs follow a chronological order. Yet, invoking a simple call to “get the current time” can potentially yield unexpected results and lead to unforeseen consequences if not used correctly. Moreover, the invariants about time we observe on our local development machine may not necessarily hold in the cloud, or in any distributed system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this article, I’ll go through the different ways we can obtain the current time in our programs, and present cases where our intuitions and expectations of time from these clocks may mislead us at best or cause catastrophic failures at worst.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/time/time_1.png&quot; alt=&quot;time 1&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock text-center&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What would be the biological reality of planet earth rotating once every eighteen hours instead of twenty-four? You have less time, but you have more days in the year. So there’s a sense of losing something, and also gaining something. With an 18-hour clock there’s a lot more yesterdays.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;attribution&quot;&gt;
&amp;#8212; Untitled (Clock)&lt;br&gt;
&lt;cite&gt;2014&lt;/cite&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock warning&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Warning&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
This article is illustrated with examples of code in Java. However, most of the content of this article is applicable to any language or runtime.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_your_local_clocks&quot;&gt;Your local clocks&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let me start by asking you six questions. Here is a set of code snippets. Is it possible that the expression passed to &lt;code&gt;isThisPossible&lt;/code&gt; is true? Take a guess.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a id=&quot;aq1&quot;&gt;&lt;/a&gt;1. Is this possible?&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;long t1 = System.currentTimeMillis();
long t2 = System.currentTimeMillis();

isThisPossible(t2 - t1 == 0);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;btn-group btn-group-toggle btn-group-justified&quot; data-toggle=&quot;buttons&quot;&gt;
  &lt;label class=&quot;btn btn-success&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q1&quot; id=&quot;yes&quot; autocomplete=&quot;off&quot; value=&quot;yes&quot;&gt;Yes
  &lt;/label&gt;
  &lt;label class=&quot;btn btn-danger&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q1&quot; id=&quot;no&quot; autocomplete=&quot;off&quot; value=&quot;no&quot;&gt;No
  &lt;/label&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a id=&quot;aq2&quot;&gt;&lt;/a&gt;2. Is this possible?&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;long t1 = System.nanoTime();
long t2 = System.nanoTime();

isThisPossible(t2 - t1 == 0);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;btn-group btn-group-toggle btn-group-justified&quot; data-toggle=&quot;buttons&quot;&gt;
  &lt;label class=&quot;btn btn-success&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q2&quot; id=&quot;yes&quot; autocomplete=&quot;off&quot; value=&quot;yes&quot;&gt;Yes
  &lt;/label&gt;
  &lt;label class=&quot;btn btn-danger&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q2&quot; id=&quot;no&quot; autocomplete=&quot;off&quot; value=&quot;no&quot;&gt;No
  &lt;/label&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a id=&quot;aq3&quot;&gt;&lt;/a&gt;3. Is this possible?&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;long t1 = System.currentTimeMillis();
long t2 = System.currentTimeMillis();

isThisPossible(t2 &amp;lt; t1);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;btn-group btn-group-toggle btn-group-justified&quot; data-toggle=&quot;buttons&quot;&gt;
  &lt;label class=&quot;btn btn-success&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q3&quot; id=&quot;yes&quot; autocomplete=&quot;off&quot; value=&quot;yes&quot;&gt;Yes
  &lt;/label&gt;
  &lt;label class=&quot;btn btn-danger&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q3&quot; id=&quot;no&quot; autocomplete=&quot;off&quot; value=&quot;no&quot;&gt;No
  &lt;/label&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a id=&quot;aq4&quot;&gt;&lt;/a&gt;4. Is this possible?&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;long t1 = System.nanoTime();
long t2 = System.nanoTime();

isThisPossible(t2 &amp;lt; t1);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;btn-group btn-group-toggle btn-group-justified&quot; data-toggle=&quot;buttons&quot;&gt;
  &lt;label class=&quot;btn btn-success&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q4&quot; id=&quot;yes&quot; autocomplete=&quot;off&quot; value=&quot;yes&quot;&gt;Yes
  &lt;/label&gt;
  &lt;label class=&quot;btn btn-danger&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q4&quot; id=&quot;no&quot; autocomplete=&quot;off&quot; value=&quot;no&quot;&gt;No
  &lt;/label&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a id=&quot;aq5&quot;&gt;&lt;/a&gt;5. Is this possible?&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;long t1 = System.currentTimeMillis();

isThisPossible(t1 &amp;lt; 0);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;btn-group btn-group-toggle btn-group-justified&quot; data-toggle=&quot;buttons&quot;&gt;
  &lt;label class=&quot;btn btn-success&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q5&quot; id=&quot;yes&quot; autocomplete=&quot;off&quot; value=&quot;yes&quot;&gt;Yes
  &lt;/label&gt;
  &lt;label class=&quot;btn btn-danger&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q5&quot; id=&quot;no&quot; autocomplete=&quot;off&quot; value=&quot;no&quot;&gt;No
  &lt;/label&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a id=&quot;aq6&quot;&gt;&lt;/a&gt;6. Is this possible?&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;long t1 = System.nanoTime();

isThisPossible(t1 &amp;lt; 0);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;btn-group btn-group-toggle btn-group-justified&quot; data-toggle=&quot;buttons&quot;&gt;
  &lt;label class=&quot;btn btn-success&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q6&quot; id=&quot;yes&quot; autocomplete=&quot;off&quot; value=&quot;yes&quot;&gt;Yes
  &lt;/label&gt;
  &lt;label class=&quot;btn btn-danger&quot;&gt;
    &lt;input type=&quot;radio&quot; name=&quot;q6&quot; id=&quot;no&quot; autocomplete=&quot;off&quot; value=&quot;no&quot;&gt;No
  &lt;/label&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;button id=&quot;checkquizres&quot; class=&quot;btn btn-default btn-group-justified&quot; type=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#collapseExample&quot; aria-expanded=&quot;false&quot; aria-controls=&quot;collapseExample&quot;&gt;
  Check results
&lt;/button&gt;
&lt;div class=&quot;collapse&quot; id=&quot;collapseExample&quot;&gt;
  &lt;script&gt;
  var answer = function(i, m1, m2) {
      var m = {
        'yes': m1,
        'no': m2,
        undefined: &quot;Wasn't answered&quot;
      }[$('input[name=q' + i + ']:checked').val()]
      $('#sqr' + i).text(i + ') ' + m)
  }
  document.addEventListener('DOMContentLoaded', function(){
    $('#checkquizres').on('click', function () {
      answer(1,
        &quot;Yes, it's definitely possible&quot;,
        &quot;Actually, It is possible&quot;
      )
      answer(2,
        &quot;Yes, it's possible, but it depends on underlying the system&quot;,
        &quot;Actually, it is possible but it depends on the underlying system&quot;
      )
      answer(3,
        &quot;Yes, it's possible&quot;,
        &quot;No, it is possible&quot;
      )
      answer(4,
        &quot;Yes, it's possible but almost impossible to reproduce and it depends on the underlying system&quot;,
        &quot;It is possible but almost impossible to reproduce and it depends on the underlying system&quot;
      )
      answer(5,
        &quot;No, it's not possible, at least something is not possible&quot;,
        &quot;Spot on, it's not possible!&quot;
      )
      answer(6,
        &quot;It is possible according to the documentation&quot;,
        &quot;No, it is possible according to the documentation&quot;
      )
    })
  })
  &lt;/script&gt;
  &lt;div id=&quot;showquizres&quot; class=&quot;well&quot;&gt;
    &lt;p id=&quot;sqr1&quot;&gt;&lt;/p&gt;
    &lt;p id=&quot;sqr2&quot;&gt;&lt;/p&gt;
    &lt;p id=&quot;sqr3&quot;&gt;&lt;/p&gt;
    &lt;p id=&quot;sqr4&quot;&gt;&lt;/p&gt;
    &lt;p id=&quot;sqr5&quot;&gt;&lt;/p&gt;
    &lt;p id=&quot;sqr6&quot;&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Was the result surprising? My sincere kudos if it was not. The next section of the article will explain why certain behaviour can or can not be observed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But why do we even care? Very often we don’t need to, but the snippets of code where the business logic relies on the observed timestamps are typically critical pieces of infrastructure code where correctness is a must. False assumptions in these parts of the code can lead to huge incidents. This, for instance, happened to &lt;a href=&quot;https://blog.cloudflare.com/how-and-why-the-leap-second-affected-cloudflare-dns/&quot;&gt;Cloudflare in 2017&lt;/a&gt;, where the root cause &quot;was the belief that time cannot go backwards&quot;. Cloudflare is one of the few companies that openly publishes incident reports, but it&amp;#8217;s not uncommon to suffer from such false assumptions, as a few Google searches can confirm, and we can all learn from these mistakes.
To understand why certain clocks behave in a certain way, we first need to understand what properties different clocks can give us.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_monotonicity&quot;&gt;Monotonicity&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first property is monotonicity. A monotonically increasing function means that for every subsequent invocation of such a function the produced value is never smaller than any of the previous values. So, a monotonic clock is a clock that never goes backwards. Sadly, and surprisingly, this property is not a feature of many clocks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_resolution&quot;&gt;Resolution&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Resolution is the second property. It is the smallest observable difference between two clock ticks. The resolution of a simple mechanical watch with a second hand is one second. When you’re staring at the watch, the meaningful watch hand position can be at 12 seconds or 13 seconds, but never 12 and a half.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_latency&quot;&gt;Latency&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Very often latency is overlooked when we’re talking about clocks, but it’s quite important when we’re considering other properties like resolution. For instance, it doesn’t matter if you have the most precise atomic watch on your hand with picosecond resolution ‒ if I ask you what time it is and it takes you roughly a second, sometimes less, sometimes more, to take a look and respond, all of this precision fades away.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, what properties do Java clocks have, and how do they apply to the questions that we looked at the beginning?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_clocks_on_the_wall&quot;&gt;Clocks on the wall&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s start with &lt;code&gt;System.currentTimeMillis()&lt;/code&gt;. Usually, the best place to start the exploration is the documentation written in the Javadoc, and there is a lot there to take in. Here is an excerpt of what is important to us right now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/lang/System.java#l375&quot;&gt;Javadoc&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;/**
 * Returns the current time in milliseconds. Note that
 * while the unit of time of the return value is a millisecond,
 * the granularity of the value depends on the underlying
 * operating system and may be larger.  For example, many
 * operating systems measure time in units of tens of
 * milliseconds.
 *
 * ...
 *
 * @return  the difference, measured in milliseconds, between
 *          the current time and midnight, January 1, 1970 UTC.
 */
public static native long currentTimeMillis();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As we can see, the clock provides us with a millisecond precision value but the actual resolution depends on the operating system. Moreover, if we measure the latency by measuring the execution time, it will be way below 1 millisecond, so it&amp;#8217;s maybe not a surprise that the answer to the &lt;a href=&quot;#aq1&quot;&gt;first question&lt;/a&gt; was yes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But can it go backwards? The Javadoc doesn’t mention anything about monotonicity, so we need to dig deeper, and take a look at the implementation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock warning&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Warning&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
This article only explores the native implementation for Linux and MacOS. However, similar techniques can be applied to other operating systems as well.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The method is native, so the implementation depends on the underlying OS. The native implementation for Linux and MacOS look almost identical.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/linux/os_linux.cpp#l1204&quot;&gt;Linux&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;jlong os::javaTimeMillis() {
  timeval time;
  int status = gettimeofday(&amp;amp;time, NULL);
  assert(status != -1, &quot;linux error&quot;);
  return jlong(time.tv_sec) * 1000  +  jlong(time.tv_usec / 1000);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/bsd/os_bsd.cpp#l893&quot;&gt;MacOS&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;jlong os::javaTimeMillis() {
  timeval time;
  int status = gettimeofday(&amp;amp;time, NULL);
  assert(status != -1, &quot;bsd error&quot;);
  return jlong(time.tv_sec) * 1000  +  jlong(time.tv_usec / 1000);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The functions invoke exactly the same syscall, &lt;code&gt;gettimeofday&lt;/code&gt;. The man page can provide us with more info, but more importantly with some valuable notes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://man7.org/linux/man-pages/man2/gettimeofday.2.html&quot;&gt;man page&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-man&quot; data-lang=&quot;man&quot;&gt;NAME
       gettimeofday, settimeofday - get / set time

NOTES
       The time returned by gettimeofday() is affected by discontinuous
       jumps in the system time (e.g., if the system administrator manually
       changes the system time).  If you need a monotonically increasing
       clock, see clock_gettime(2).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As noted above, the time is affected by discontinuous jumps in the system time, which could be backwards, hence the clock is not monotonic. The answer to the &lt;a href=&quot;#aq3&quot;&gt;third question&lt;/a&gt; was yes which does make sense: if we change the current time to one hour ago, we still want &lt;code&gt;currentTimeMillis&lt;/code&gt; to return current time, even though the definition of the current time has changed. That’s why it&amp;#8217;s often called wall-clock time, the clock on the wall can also jump back in time if we adjust it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_the_nanos_of_the_current_time&quot;&gt;The nanos of the current time&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The same exploration path can be taken for &lt;code&gt;System.nanoTime()&lt;/code&gt;. Let’s start from the Javadoc which has even more intriguing details than the previous one; here is an excerpt.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/lang/System.java#l394&quot;&gt;Javadoc&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;/**
 * Returns the current value of the running Java Virtual Machine's
 * high-resolution time source, in nanoseconds.
 *
 * This method can only be used to measure elapsed time and is
 * not related to any other notion of system or wall-clock time.
 * The value returned represents nanoseconds since some fixed but
 * arbitrary &amp;lt;i&amp;gt;origin&amp;lt;/i&amp;gt; time (perhaps in the future, so values
 * may be negative) ...
 *
 * &amp;lt;p&amp;gt;This method provides nanosecond precision, but not necessarily
 * nanosecond resolution ...
 *
 * &amp;lt;p&amp;gt;The values returned by this method become meaningful only when
 * the difference between two such values, obtained within the same
 * instance of a Java virtual machine, is computed.
 *
 * ...
 */
public static native long nanoTime();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Apparently, the time returned by this clock isn’t related to any real-world time; it can only be used to compare the timestamps within the same JVM instance, and it’s relative to an arbitrary “origin” which can be in the future, and therefore it might be negative – which answers the &lt;a href=&quot;#aq3&quot;&gt;sixth question&lt;/a&gt;. Similar to &lt;code&gt;currentTimeMillis&lt;/code&gt;, this method provides nanosecond precision, but not necessarily nanosecond resolution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Nano time can only be used to measure time intervals, so it ought to be monotonic, right? Unfortunately, the Javadoc doesn’t say anything about monotonicity, so the next step is the implementation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/linux/os_linux.cpp#l1301&quot;&gt;Linux&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;jlong os::javaTimeNanos() {
  if (os::supports_monotonic_clock()) {
    struct timespec tp;
    int status = Linux::clock_gettime(CLOCK_MONOTONIC, &amp;amp;tp);
    assert(status == 0, &quot;gettime error&quot;);
    jlong result = jlong(tp.tv_sec) * (1000 * 1000 * 1000) + jlong(tp.tv_nsec);
    return result;
  } else {
    timeval time;
    int status = gettimeofday(&amp;amp;time, NULL);
    assert(status != -1, &quot;linux error&quot;);
    jlong usecs = jlong(time.tv_sec) * (1000 * 1000) + jlong(time.tv_usec);
    return 1000 * usecs;
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here comes the first surprise: nano time is indeed monotonic but &lt;strong&gt;only&lt;/strong&gt; if the underlying operating system supports it. To be fair, any modern Linux server supports &lt;code&gt;CLOCK_MONOTONIC&lt;/code&gt;; there are, however, some &lt;a href=&quot;https://stackoverflow.com/a/51345008/1542319&quot;&gt;rare situations&lt;/a&gt; in which it might not hold true.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/bsd/os_bsd.cpp#l893&quot;&gt;MacOS&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;jlong os::javaTimeNanos() {
  const uint64_t tm = mach_absolute_time();
  const uint64_t now = (tm * Bsd::_timebase_info.numer) / Bsd::_timebase_info.denom;
  const uint64_t prev = Bsd::_max_abstime;
  if (now &amp;lt;= prev) {
    return prev;   // same or retrograde time;
  }
  const uint64_t obsv = Atomic::cmpxchg(now, &amp;amp;Bsd::_max_abstime, prev);
  assert(obsv &amp;gt;= prev, &quot;invariant&quot;);   // Monotonicity
  // If the CAS succeeded then we're done and return &quot;now&quot;.
  // If the CAS failed and the observed value &quot;obsv&quot; is &amp;gt;= now then
  // we should return &quot;obsv&quot;.  If the CAS failed and now &amp;gt; obsv &amp;gt; prv then
  // some other thread raced this thread and installed a new value, in which case
  // we could either (a) retry the entire operation, (b) retry trying to install now
  // or (c) just return obsv.  We use (c).   No loop is required although in some cases
  // we might discard a higher &quot;now&quot; value in deference to a slightly lower but freshly
  // installed obsv value.   That's entirely benign -- it admits no new orderings compared
  // to (a) or (b) -- and greatly reduces coherence traffic.
  // We might also condition (c) on the magnitude of the delta between obsv and now.
  // Avoiding excessive CAS operations to hot RW locations is critical.
  // See https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate
  return (prev == obsv) ? now : obsv;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first thing that stands out is the giant wall of comments. As software engineers, we know that if there is a long comment then something dodgy must be going on. Indeed, the comment is quite interesting. The call to &lt;a href=&quot;https://opensource.apple.com/source/Libc/Libc-320.1.3/i386/mach/mach_absolute_time.c.auto.html&quot;&gt;&lt;code&gt;mach_absolute_time&lt;/code&gt;&lt;/a&gt; uses the &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_Stamp_Counter&quot;&gt;RDTSC&lt;/a&gt; instruction underneath which can &lt;strong&gt;potentially&lt;/strong&gt; lead to non-monotonic behaviour on machines with multiple CPU sockets, which recently span up another thought-provoking discussion on the &lt;a href=&quot;https://groups.google.com/forum/#!topic/mechanical-sympathy/7WnH37dA6Yc&quot;&gt;mechanical sympathy&lt;/a&gt; mailing list.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, at least, we can be confident that nano time is always monotonic on MacOS, right? Actually, it depends on the JVM version. The code listed above was introduced in JDK9 in &lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8040140&quot;&gt;JDK-8040140&lt;/a&gt; and backported to JDK8. Before, all you could hope for was non-monotonic time which provided at best microsecond resolution because &lt;code&gt;gettimeofday&lt;/code&gt; was used. If we run some &lt;a href=&quot;https://shipilev.net/blog/2014/nanotrusting-nanotime/#_latency&quot;&gt;benchmarks&lt;/a&gt;, we&amp;#8217;ll see that the latency for these calls can be as small as 30ns, so suddenly the answer to the &lt;a href=&quot;#aq2&quot;&gt;second&lt;/a&gt; and the &lt;a href=&quot;#aq4&quot;&gt;fourth&lt;/a&gt; questions is true, or rather &quot;it depends&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_when_milliseconds_are_not_enough&quot;&gt;When milliseconds are not enough&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The microsecond precision in the case of &lt;code&gt;gettimeofday&lt;/code&gt; is much more than &lt;code&gt;System.currentTimeMillis()&lt;/code&gt; can give us, but in the process of conversion precision is lost.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;jlong os::javaTimeMillis() {
  timeval time;
  int status = gettimeofday(&amp;amp;time, NULL);
  assert(status != -1, &quot;linux error&quot;);
  return jlong(time.tv_sec) * 1000  +  jlong(time.tv_usec / 1000);
                                                      // ^^ precision loss
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The OS can give us additional information which we violently discard in order to fit it into a single long. What if we really want to know these micros? In JDK 8, the new JSR 310 arrived which made it possible to obtain an instance of &lt;code&gt;Instant&lt;/code&gt; class which contains the number of seconds since the epoch and the number of nanoseconds since the last second started.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;https://jcp.org/en/jsr/detail?id=310&quot;&gt;JSR 310: Date and Time API&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;Instant instant = Clock.systemUTC().instant();
long epochSecond = instant.getEpochSecond();
int nanoSinceSecond = instant.getNano();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, all Java developers got access to wall-clock time with high precision, right? Not so fast, if we take a look at the implementation in JDK8, we’ll find out that it simply delegates straight to &lt;code&gt;System.currentTimeMillis()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/time/Clock.java#l469&quot;&gt;JDK8 Clock&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Override
public long millis() {
    return System.currentTimeMillis();
}
@Override
public Instant instant() {
    return Instant.ofEpochMilli(millis());
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Evidently, this is not optimal and there is a corresponding issue &lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8068730&quot;&gt;JDK-8068730&lt;/a&gt; which has already been resolved and as a result, the precision was increased. It requires an update to JDK9+ where the method delegates to a native call with the following implementation on Linux. Assuming that your OS can provide microsecond resolution, this clock is a great example of a clock with nanosecond precision, but only microsecond resolution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&lt;a href=&quot;http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/linux/os_linux.cpp#l1211&quot;&gt;JDK9+ Clock&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;void os::javaTimeSystemUTC(jlong &amp;amp;seconds, jlong &amp;amp;nanos) {
  timeval time;
  int status = gettimeofday(&amp;amp;time, NULL);
  assert(status != -1, &quot;linux error&quot;);
  seconds = jlong(time.tv_sec);
  nanos = jlong(time.tv_usec) * 1000;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_time_exchange&quot;&gt;Time exchange&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The possibility to get the current wall-clock time with microsecond resolution is great, but is needed often? One of the reasons to use wall-clock time is to be able to relate an event that happened on one machine to another event that happened on a different machine, or more precisely, to decide on the order of these events. The events can be very different in nature. Some of them might not be very critical, like the timestamp on a log line, but some of them must be correct, like when there is a conflict in a database due to two values being written concurrently and timestamps are used to determine which event was last. This strategy is called Last Write Wins, or simply LWW.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;canva-embed&quot; data-design-id=&quot;DADZmyL_mRw&quot; data-height-ratio=&quot;0.5625&quot; style=&quot;padding:56.2500% 5px 5px 5px;background:rgba(0,0,0,0.03);border-radius:8px;&quot;&gt;&lt;/div&gt;&lt;script async src=&quot;https:&amp;#x2F;&amp;#x2F;sdk.canva.com&amp;#x2F;v1&amp;#x2F;embed.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;On the slides above, two clients Alice and Bob are trying to write simultaneously into an eventually consistent webscale database with two nodes. While the first value written by Alice was successfully synchronized,  Alice’s second write happened to be at approximately the same time as Bob’s. In this situation, the database must resolve the conflict so that the data is consistent between all of the nodes. In the case of LWW, the latest write will be chosen by comparing the timestamps of each write. LWW works perfectly if the clocks are perfectly synchronised, however, if the clocks are poorly synchronised and the clock of the first node has drifted ahead of the second node, LWW becomes Lucky Write Wins – the client connected to the lucky node always wins the conflict.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_ntp&quot;&gt;NTP&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The standard approach to make sure that the clocks on different nodes in the cluster are synchronized is to use Network Time Protocol (NTP). Not only does NTP help synchronise clocks, it also helps propagate a leap second flag. Leap second is an occasional event where an additional second is introduced in between 23:59:59 of a chosen day and 00:00:00 of the following day. It’s often implemented as playing the same second twice which from the observer’s point of view might look like a jump 1 second back in time. The last leap second was introduced on the 31st of December 2016 which resulted in the above-mentioned DNS incident.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/time/time_2.png&quot; alt=&quot;time 2&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The conventional way of dealing with leap seconds is &quot;leap smearing&quot;. The NTP server which is responsible for leap smearing can distribute the additional second amongst 12 hours before and 12 hours after the second is introduced. The wall-clock time during these 24 hours is ticking slower and every second is 1/86400 longer which might be surprising, however less surprising than a jump back in time. The catch is that not many NTP servers support leap smearing, the public NTP servers most definitely don’t.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The major cloud providers, &lt;a href=&quot;https://developers.google.com/time/smear&quot;&gt;Google&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/blogs/aws/look-before-you-leap-the-coming-leap-second-and-aws/&quot;&gt;AWS&lt;/a&gt; both provide NTP services with leap smearing support. If your application is hosted on a platform that provides an NTP service and you care about clock synchronisation it’s worthwhile checking that NTP synchronisation is set up with the provider’s NTP service. Not only can it help avoid the nasty consequences of applying leap seconds naïvely, but it also dramatically decreases the synchronisation error since the network latency is typically much lower within a single datacenter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;AWS NTP with chrony&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;sergey:~$ chronyc sources -v
210 Number of sources = 9

  .-- Source mode  '^' = server, '=' = peer, '#' = local clock.
 / .- Source state '*' = current synced, '+' = combined , '-' = not combined,
| /   '?' = unreachable, 'x' = time may be in error, '~' = time too variable.
||                                                 .- xxxx [ yyyy ] +/- zzzz
||      Reachability register (octal) -.           |  xxxx = adjusted offset,
||      Log2(Polling interval) --.      |          |  yyyy = measured offset,
||                                \     |          |  zzzz = estimated error.
||                                 |    |           \
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^* 169.254.169.123               3  10   377   433    -25us[  -36us] +/-  356us&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Using a local NTP server can reduce the clock drift down to milliseconds or even microseconds in the best case, but what is the worst case? There is not much research on this topic, however some notable results were mentioned in the Google Spanner paper.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Between synchronizations, a daemon advertises a slowly increasing time uncertainty. ε is derived from conservatively applied worst-case local clock drift. ε also depends on time-master uncertainty and communication delay to the time masters. In our production environment, ε is typically a sawtooth function of time, varying from about 1 to 7 ms over each poll interval. ε̅ is therefore 4 ms most of the time. The daemon’s poll interval is currently 30 seconds, and the current applied drift rate is set at 200 microseconds/second, which together accounts for the sawtooth bounds from 0 to 6 ms.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;attribution&quot;&gt;
&amp;#8212; Spanner: Google’s Globally-Distributed Database
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_logical_conclusion&quot;&gt;Logical conclusion&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Even if the monitoring in our cluster shows that the clocks are synchronised with microsecond precision, we need to be cautious and shouldn’t rely on this in our software if a failure of this assumption is unacceptable. So, if a failure is unacceptable and we need to know the order of the events in a distributed system, is there anything we can do? As always, there is a number of solutions suggested by academia.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_lamport_clocks&quot;&gt;Lamport clocks&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What we need is a reliable replacement for our system clocks, so that for every two events &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; we can say that either &lt;em&gt;A&lt;/em&gt; happened before &lt;em&gt;B&lt;/em&gt;, or &lt;em&gt;B&lt;/em&gt; happened before &lt;em&gt;A&lt;/em&gt;. Such order between events is called total order. In the &lt;a href=&quot;https://lamport.azurewebsites.net/pubs/time-clocks.pdf&quot;&gt;&quot;Time, Clocks, and the Ordering of Events in a Distributed System&quot;&lt;/a&gt; paper Leslie Lamport described the &quot;happens before&quot; relation and logical clocks that can be used to define total order for a set of events using the following algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-all stretch&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Sending a message&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Receiving a message&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-pseudocode&quot; data-lang=&quot;pseudocode&quot;&gt;time = time + 1;
send(message, time);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-pseudocode&quot; data-lang=&quot;pseudocode&quot;&gt;(message, ts) = receive();
time = max(ts, time) + 1;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Every actor, in this case, Alice and Bob, will maintain a shared view of the current time by maintaining a &lt;code&gt;time&lt;/code&gt; counter which increases every time a message is sent, and when a message is received, the &lt;code&gt;time&lt;/code&gt; is always bigger than the last observed counter. That way if Alice updates the Database as shown below with the value 2 and tells Bob about the last known state, Bob&amp;#8217;s final write carries with it the knowledge of seeing Alice’s counter, so it’s chosen as the final state of the database.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Note&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
On the slides below Alice tells Bob the value which she wrote to the first node. Alternatively, Bob could have read the same value from the first node and it would lead to the same result – Alice and Bob don&amp;#8217;t have to communicate directly.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;canva-embed&quot; data-design-id=&quot;DADZnGWHlAU&quot; data-height-ratio=&quot;0.5625&quot; style=&quot;padding:56.2500% 5px 5px 5px;background:rgba(0,0,0,0.03);border-radius:8px;&quot;&gt;&lt;/div&gt;&lt;script async src=&quot;https:&amp;#x2F;&amp;#x2F;sdk.canva.com&amp;#x2F;v1&amp;#x2F;embed.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This works perfectly as long as we need to define some total order of the events in the system which captures the causality. It&amp;#8217;s important to note that having total order means that concurrent events will be ordered in some way, not necessarily the most logical way. On the slides below, Alice never talked to Bob, but her counter is bigger which leads to her write being chosen in the case of a conflict.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;canva-embed&quot; data-design-id=&quot;DADZnC8PNnM&quot; data-height-ratio=&quot;0.5625&quot; style=&quot;padding:56.2500% 5px 5px 5px;background:rgba(0,0,0,0.03);border-radius:8px;&quot;&gt;&lt;/div&gt;&lt;script async src=&quot;https:&amp;#x2F;&amp;#x2F;sdk.canva.com&amp;#x2F;v1&amp;#x2F;embed.js&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_vector_clocks&quot;&gt;Vector clocks&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To deal with truly concurrent events, we need a new definition or order which is able to express the situation in which events can happen concurrently. Such order is called partial order. Basically, this means that for any two events &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt;, it&amp;#8217;s possible to say whether &lt;em&gt;A&lt;/em&gt; happened before &lt;em&gt;B&lt;/em&gt;, &lt;em&gt;B&lt;/em&gt; happened before &lt;em&gt;A&lt;/em&gt; or &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt; happened concurrently. To determine partial order the following algorithm can be used this, where every actor has a separate time counter, and keeps track of the latest timestamp of any other actor in the system.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-all stretch&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Sending a message&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Receiving a message&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-pseudocode&quot; data-lang=&quot;pseudocode&quot;&gt;V[myId] = V[myId] + 1
send(message, V);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-pseudocode&quot; data-lang=&quot;pseudocode&quot;&gt;(message, Vr) = receive();
for (i, v) in Vr {
    V[i] = max(V[i], v);
}
V[myId] = V[myId] + 1;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The algorithm was described in 1988, and later using vector clocks for conflict resolution in a database was described in the Dynamo paper. On the following slides, Alice keeps track of her own time counter as well as Bob&amp;#8217;s last known time counter. That way when Alice sends a message to Bob, he updates his counters and the next message sent to the database is chosen during the conflict resolution because each component of Bob’s time vector is larger than the respective component of the previous vector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;canva-embed&quot; data-design-id=&quot;DADZnKp0nOE&quot; data-height-ratio=&quot;0.5625&quot; style=&quot;padding:56.2500% 5px 5px 5px;background:rgba(0,0,0,0.03);border-radius:8px;&quot;&gt;&lt;/div&gt;&lt;script async src=&quot;https:&amp;#x2F;&amp;#x2F;sdk.canva.com&amp;#x2F;v1&amp;#x2F;embed.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When there is a real conflict, vector clocks can help to determine whether the events were truly concurrent. In the scenario below, two nodes end up with the events, &lt;code&gt;[0, 1]&lt;/code&gt; and &lt;code&gt;[0, 1]&lt;/code&gt; which cannot be ordered. In this situation, the database can keep both values, and return them the next time it is read, to let either Alice or Bob decide which one to keep so that the data is not lost.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;canva-embed&quot; data-design-id=&quot;DADZnBQkwxE&quot; data-height-ratio=&quot;0.5625&quot; style=&quot;padding:56.2500% 5px 5px 5px;background:rgba(0,0,0,0.03);border-radius:8px;&quot;&gt;&lt;/div&gt;&lt;script async src=&quot;https:&amp;#x2F;&amp;#x2F;sdk.canva.com&amp;#x2F;v1&amp;#x2F;embed.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;These properties, however, do not come for free. The metadata needs to be exchanged with every message, and multiple versions need to be stored. After all, some databases, like Cassandra don&amp;#8217;t use vector clocks &lt;a href=&quot;https://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks&quot;&gt;for a reason&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;code&gt;System.nanoTime()&lt;/code&gt; for measuring time intervals&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;code&gt;System.currentTimeMillis()&lt;/code&gt; for obtaining wall-clock time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;code&gt;Clock.systemUTC().instant()&lt;/code&gt; for getting wall-clock time with ns &lt;em&gt;precision&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not every clock can give you the resolution you want even if its precision is high&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The wall-clock time can be off by dozens of milliseconds (or more, or less)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use NTP from your cloud provider if time synchronisation matters&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Logical clocks might be more appropriate than the real clocks but they have associated costs&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_thanks&quot;&gt;Thanks&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You for reading this article&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Uri Baghin for reviewing the article&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_share_this_article&quot;&gt;Share this article&lt;/h3&gt;
&lt;hr&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Just published a new blog post &amp;quot;The matter of time()&amp;quot;. JVM, clocks, and mysterious time in distributed systems. It&amp;#39;s just about time! &lt;a href=&quot;https://t.co/r11FtwZh02&quot;&gt;https://t.co/r11FtwZh02&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sergey Tselovalnikov (@SerCeMan) &lt;a href=&quot;https://twitter.com/SerCeMan/status/1128963307753287680?ref_src=twsrc%5Etfw&quot;&gt;May 16, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_references&quot;&gt;References&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://shipilev.net/blog/2014/nanotrusting-nanotime/&quot;&gt;Nanotrusting the Nanotime&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.usenix.org/system/files/conference/osdi12/osdi12-final-16.pdf&quot;&gt;Spanner: Google’s Globally-Distributed Database&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html&quot;&gt;AWS NTP&lt;/a&gt; / &lt;a href=&quot;https://developers.google.com/time/&quot;&gt;Google NTP&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=un1AHZBgFfk&quot;&gt;Video PWLSF - Bryan Fink on &quot;A Brief History of NTP Time: Memoirs of an Internet Timekeeper&quot;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/103043/&quot;&gt;Internet time synchronization: the network time protocol&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.cloudflare.com/how-and-why-theleap-second-affected-cloudflare-dns/&quot;&gt;How and why the leap second affected Cloudflare DNS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://aphyr.com/posts/299-the-trouble-with-timestamps&quot;&gt;The trouble with timestamps&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://lamport.azurewebsites.net/pubs/time-clocks.pdf&quot;&gt;Time, Clocks, and the Ordering of Events in a Distributed System&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://zoo.cs.yale.edu/classes/cs426/2012/lab/bib/fidge88timestamps.pdf&quot;&gt;Timestamps in Message-Passing Systems That Preserve the Partial Ordering&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.rapid7.com/2014/03/14/synchronizing-clocksin-a-cassandra-cluster-pt-1-the-problem/&quot;&gt;Synchronizing Clocks In a Cassandra Cluster&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks&quot;&gt;Why Cassandra doesn’t need vector clocks&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.allthingsdistributed.com/files/amazondynamo-sosp2007.pdf&quot;&gt;Dynamo: Amazon’s Highly Available Key-value Store&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://basho.com/posts/technical/why-vector-clocks-are-easy/&quot;&gt;Why Vector Clocks Are Easy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://basho.com/posts/technical/why-vector-clocks-are-hard/&quot;&gt;Why Vector Clocks Are Hard&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;script src=&quot;//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.0/jquery.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;//cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 16 May 2019 00:00:00 +1000
</pubDate>
</item>
<item>
<guid>
http://serce.me/posts/29-06-2017-fantastic-dsls/
</guid>
<link>
http://serce.me/posts/29-06-2017-fantastic-dsls/
</link>
<title>
Fantastic DSLs and where to find them
</title>
<description>
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hi!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Kotlin is a very rich language. Unlike many other languages, it allows developers to create another language inside it. For example,
to mimic HTML syntax or to build a completely typed SQL query. But Kotlin’s power isn&amp;#8217;t limited to simple DSLs. With some Kotlin-fu,
it’s possible to write a DSL that allows manipulating untyped data structures in a typed manner.
In this article, we’ll go through different ways to define DSL in Kotlin, from very simple to fantastically powerful.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/fantastic/kotlin_island.png&quot; alt=&quot;kotlin island&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Peter the Great at one time even considered moving the capital of Russia to Kotlin Island, proof of the sovereign’s great
affinity with water. This utopian idea failed, but many of the fantasies of this baroque autocrat still managed to become implemented.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock warning&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Warning&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
Some parts of this article might be hard to understand without knowledge of Kotlin syntax. I tried to explain
every feature I showed, but the general ability to speak Kotlin is strongly suggested.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, let&amp;#8217;s begin the journey.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_what_is_dsl&quot;&gt;What is DSL&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;quoteblock&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Domain-specific language (noun): a computer programming language of limited expressiveness focused on a particular domain&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;attribution&quot;&gt;
&amp;#8212; Martin Fowler&lt;br&gt;
&lt;cite&gt;Domain-Specific Languages&lt;/cite&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here though, I prefer to give DSLs a slightly different definition which reflects what is written in the article&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;a language (or a set of abstractions) that&amp;#8217;s built to deal with a specific domain&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The main difference is that a DSL might not only be a separate language but also a subset of a language which is used
to work on a specific domain. This kind of DSL can even be built in Java with some fluent API, but very often it’s
indistinguishable from a plain good code. To contrast in Kotlin, many remarkable features might make an internal
DSL look different.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_calling_convention&quot;&gt;Calling convention&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first feature actively used by DSLs in Kotlin is a special calling convention.
If the last parameter of a method is a function, and you&amp;#8217;re passing a lambda expression there, you can specify
it outside of parentheses.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For example, if one wants to create a function &lt;code&gt;dotimes&lt;/code&gt; that takes a number &lt;code&gt;n&lt;/code&gt;, a function &lt;code&gt;f&lt;/code&gt; and applies
it, the easiest way to do that is&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;good old dotimes&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;fun dotimes(n: Int, f: () -&amp;gt; Unit) {
    for (i in 0..n-1) {
        f()
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;dotimes&lt;/code&gt; can be called in this way&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;dotimes(5, {
    println(&quot;Hello, Kotlin!&quot;)
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Or, using the lambda parameter convention and placing lambda function outside parentheses.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;dotimes(5) {
    println(&quot;Hello, Kotlin!&quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Moreover, the parentheses can be omitted completely if a lambda is the only parameter of a function. E.g. &lt;code&gt;do5times&lt;/code&gt; function
that only takes a lambda as a parameter can be defined and called as&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;fun do5times(f: () -&amp;gt; Unit) = dotimes(5, f)

do5times {
    println(&quot;Hello, Kotlin!&quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But despite being important, that calling convention is just a tiny contribution to DSLs when compared to extension functions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_extension_functions&quot;&gt;Extension functions&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Extension functions simply allow you to extend the functionality of a class from the outside.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Simple extension function&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;fun String.removeSpaces(): String {
    return this.filter({ c -&amp;gt; c != ' ' })
}

print(&quot;Hi ! , ext&quot;.removeSpaces()) // &quot;Hi!,ext&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here, the &lt;code&gt;removeSpace&lt;/code&gt; function is defined on the class String which enables an ability to call &lt;code&gt;removeSpaces&lt;/code&gt; on any &lt;code&gt;String&lt;/code&gt; instance. Unsurprisingly, it removes all the spaces from it. Inside the functions, &lt;code&gt;this&lt;/code&gt; refers to the instance of a receiver class and can be omitted like you do when you&amp;#8217;re writing a member function. That might sound complicated if you have never heard about extension functions before, but looking at the result of the compilation might make it much easier to understand.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Decompiled java code&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public static String removeSpaces(String $receiver) {
  StringBuilder sb = new StringBuilder();
  for (int i = 0; i &amp;lt; $receiver.length(); i++) {
    char c = $receiver.charAt(i);
    if (c != ' ') {
      sb.append(c);
    }
  }
  return sb.toString();
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Extension functions are not some kind of magic. It’s not a Groovy-like monkey patching, they get compiled to simple static functions. But that example shows us a very important caveat - extension functions are resolved statically because there is no dispatch mechanism for static methods&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Even though this snippet is very simple, it can raise another question - &quot;where did the &lt;code&gt;StringBuilder&lt;/code&gt; came from?&quot;.
An close look at the first snippet through &lt;code&gt;Java&lt;/code&gt; glasses gives the answer - there is no function called &lt;code&gt;filter&lt;/code&gt; defined in
the class String. &lt;code&gt;filter&lt;/code&gt; is also an extension function defined in the Kotlin standard library.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;filter function from kotlin stdlib&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;public inline fun String.filter(predicate: (Char) -&amp;gt; Boolean): String {
  val destination = StringBuilder()
  for (index in 0..length - 1) {
    val element = get(index)
    if (predicate(element))
      destination.append(element)
  }
  return destination.toString()
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Kotlin defines a lot of extension functions for Java classes in the standard library. That&amp;#8217;s why Kotlin is so convenient
to use. One might notice that the function has an &lt;code&gt;inline&lt;/code&gt; modifier on it which explains why decompiled &lt;code&gt;removeSpaces&lt;/code&gt; has
a &lt;code&gt;StringBuilder&lt;/code&gt; inside and not a call to &lt;code&gt;filter&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many newcomers to Kotlin use the &lt;code&gt;inline&lt;/code&gt; modifier everywhere under the impression that inlining can improve performance. It can,
but in many cases, inline functions don&amp;#8217;t improve performance at all, they even can make it worse. There is an inspection for that in IntelliJ IDEA.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/fantastic/inspection.png&quot; alt=&quot;inspection&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are also some other uses for &lt;code&gt;inline&lt;/code&gt; which can be found in &lt;a href=&quot;https://kotlinlang.org/docs/reference/inline-functions.html&quot;&gt;docs&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_extension_function_on_generic_type&quot;&gt;Extension function on generic type&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Kotlin compiler is smart enough to allow for the definition of extension functions on a certain generic type.
In this example, &lt;code&gt;toIntArray&lt;/code&gt; function can be called only on a collection that contains integers.
This makes extension functions truly unique, there is no way (without subclassing) to define a method for &lt;code&gt;Collection&lt;/code&gt;
class that can be called only on an &lt;code&gt;Int&lt;/code&gt; collection.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;fun Collection&amp;lt;Int&amp;gt;.toIntArray(): IntArray {
  val result = IntArray(size)
  var index = 0
  for (element in this)
    result[index++] = element
  return result
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;listOf(1, 2, 3).toIntArray()       // works
listOf(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;).toIntArray() // type error&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If Kotlin has become your native language, you might be wondering now, why I&amp;#8217;m talking about these simple features in an article about DSLs. The thing is, the majority of Kotlin DSLs are based on the expressiveness of the two features mentioned above.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_first_simple_dsl&quot;&gt;First simple DSL&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Given the aforementioned features, it&amp;#8217;s very easy to write a first very simple DSL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s say we need to write an event-based droid fighting platform so that users can provide their own strategies and register them on the platform. For each event the user is interested in, they must provide a callback with the droid&amp;#8217;s behaviour. A droid has an interface with a few methods for defeating other droids. Or, humans if you will.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;the droid&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;interface Droid {
  val peopleAround: Boolean
  val gun: Gun

  fun fire(gun: Gun)
  fun moveLeft()
  fun moveRight()
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This sounds like an ideal case for DSL and now we need to define a public API which the clients will be happy to use.
To provide the droid’s behaviour we’ll write a public function.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;API&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;private val droid: Droid = getDroid() // inaccessible from the public API

public fun on(cmd: String, f: Droid.() -&amp;gt; Unit) {
// ...
  droid.f()
// ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The type of the argument &lt;code&gt;f&lt;/code&gt; might look weird, but it&amp;#8217;s just the type of 0-arity extension function on the type Droid. And
finally, the APIs consumers can register events in the platform.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;strategy example&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;on(&quot;back&quot;) {
  moveLeft()
  if (peopleAround) {
    fire(gun)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here, the anonymous extension function is a second parameter and therefore can be written outside parentheses.
&lt;code&gt;this&lt;/code&gt; in the function has a type &lt;code&gt;Droid&lt;/code&gt; and therefore &lt;code&gt;moveLeft()&lt;/code&gt; as well as other functions and properties can be called by themselves without providing an explicit receiver type..&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The strategy looks very natural, it clearly says that if our droid receives a &lt;code&gt;back&lt;/code&gt; command, it should move left and
try to shoot some folks around him. The next snippet shows to what it can be compiled to in order to make it even clearer for those who don&amp;#8217;t speak
kotlin well yet.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;decompiled java call&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;on(&quot;back&quot;, new Function1&amp;lt;Droid, Unit&amp;gt;() {
  public Unit invoke(Droid droid) {
    droid.moveLeft();
    if (droid.getPeopleAround()) {
      droid.fire(droid.getGun());
    }
    return Unit.INSTANCE;
  }
});&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_html_builders&quot;&gt;HTML builders&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Building DSLs using extension functions isn&amp;#8217;t limited to simple droid fighting strategies. For example, it allows us to build a completely typed HTML syntax; HTML builders are even mentioned in the
&lt;a href=&quot;https://kotlinlang.org/docs/reference/type-safe-builders.html&quot;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;html builders&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val list = listOf(&quot;Kotlin&quot;, &quot;is&quot;, &quot;awesome&quot;)
val result: HTML =
  html {
    head {
      title { +&quot;HTML DSL in Kotlin&quot; }
    }
    body {
      p {
        +&quot;a line about Kotlin&quot;
      }
      a(href = &quot;jetbrains.com/kotlin&quot;) {
        +&quot;Kotlin&quot;
      }
      p {
        +&quot;Kotlin is:&quot;
        ul {
          for (arg in list)
            li { +arg }
        }
      }
    }
  }
println(result)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And these type-safe builders aren&amp;#8217;t a Kotlin invention, on the JVM land they were originated in Groovy. But Groovy is a dynamic language,
builders there are not type-safe.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock important&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Important&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
It wouldn&amp;#8217;t be completely fair to say that even though it&amp;#8217;s what Kotlin&amp;#8217;s documentation
           says, Groovy supports static compilation optionally and there are some ways to compile
           builders statically as well. (&lt;a href=&quot;http://melix.github.io/blog/2013/02/13/static_builders_inception.html&quot; class=&quot;bare&quot;&gt;http://melix.github.io/blog/2013/02/13/static_builders_inception.html&lt;/a&gt;)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The implementation of a DSL in dynamically typed languages is often very different to statically typed languages. In Kotlin, in order to build a DSL, you need to describe the whole schema of the future language. And given that the result is a deeply nested data structure, the easiest way to convert it to string is to traverse the whole data-structure recursively.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;base interface&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;interface Element {
  fun render(builder: StringBuilder, indent: String)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The simplest line of text can be represented as&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;class TextElement(val text: String) : Element {
  override fun render(builder: StringBuilder, indent: String) {
    builder.append(&quot;$indent$text\n&quot;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The real tag representation is a bit more complex&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;abstract class Tag(val name: String) : Element {
  val children = arrayListOf&amp;lt;Element&amp;gt;()
  val attributes = hashMapOf&amp;lt;String, String&amp;gt;()

  // open tag
  // render attributes
  // render children recursively
  // close tag
  override fun render(builder: StringBuilder, indent: String) {
    builder.append(&quot;$indent&amp;lt;$name${renderAttributes()}&amp;gt;\n&quot;)
    for (c in children) {
      c.render(builder, indent + &quot;  &quot;)
    }
    builder.append(&quot;$indent&amp;lt;/$name&amp;gt;\n&quot;)
  }

  private fun renderAttributes() = attributes.map { (k, v) -&amp;gt; &quot; $k=\&quot;$v\&quot;&quot; }.joinToString(&quot;&quot;)

  protected fun &amp;lt;T : Element&amp;gt; initTag(tag: T, init: T.() -&amp;gt; Unit) {
    tag.init()
    children.add(tag)
  }

  operator fun String.unaryPlus() {
    children.add(TextElement(this))
  }

  override fun toString(): String {
    val builder = StringBuilder()
    render(builder, &quot;&quot;)
    return builder.toString()
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It contains a representation of attributes and a set of children. But the main part that requires attention is the &lt;code&gt;initTag&lt;/code&gt;
function which looks very similar to the function &lt;code&gt;on&lt;/code&gt; from the &quot;robot fighting&quot; DSL definition.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another interesting part is an extension function &lt;code&gt;unaryPlus&lt;/code&gt; defined as an operator for class String inside the &lt;code&gt;Tag&lt;/code&gt;.
It allows us to use a convenient (but let&amp;#8217;s be honest not obvious at all) way to insert a line of text inside code like:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;¯\_(ツ)_/¯ unary plus to append a line of text&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;body {
  +&quot;just a random line&quot;
  +&quot;another line&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And the rest of the DSL is an enumeration of all possible tags.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;&amp;lt;head&amp;gt;,&amp;lt;title&amp;gt;,&amp;lt;body&amp;gt;,&amp;lt;a&amp;gt;,&amp;lt;ul&amp;gt;,&amp;lt;li&amp;gt;,&amp;lt;p&amp;gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;class HTML : Tag(&quot;html&quot;) {
  fun head(init: Head.() -&amp;gt; Unit) = initTag(Head(), init)

  fun body(init: Body.() -&amp;gt; Unit) = initTag(Body(), init)
}

class Head : Tag(&quot;head&quot;) {
  fun title(init: Title.() -&amp;gt; Unit) = initTag(Title(), init)
}

class Title : Tag(&quot;title&quot;)

abstract class BodyTag(name: String) : Tag(name) {
  fun p(init: P.() -&amp;gt; Unit) = initTag(P(), init)
  fun ul(init: UL.() -&amp;gt; Unit) = initTag(UL(), init)
  fun a(href: String, init: A.() -&amp;gt; Unit) {
    val a = A()
    initTag(a, init)
    a.href = href
  }
}

class Body : BodyTag(&quot;body&quot;)
class UL : BodyTag(&quot;ul&quot;) {
  fun li(init: LI.() -&amp;gt; Unit) = initTag(LI(), init)
}

class LI : BodyTag(&quot;li&quot;)
class P : BodyTag(&quot;p&quot;)

class A : BodyTag(&quot;a&quot;) {
  var href: String
    get() = attributes[&quot;href&quot;] ?: &quot;&quot;
    set(value) {
      attributes[&quot;href&quot;] = value
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you can see, all these classes define a possible hierarchy of calls. This DSL is just a toy DSL, and therefore it
covers a very small and limited subset of HTML. It is extremely tedious to write the whole
HTML DSL manually. The actual &lt;a href=&quot;https://github.com/Kotlin/kotlinx.html&quot;&gt;HTML DSL implementation&lt;/a&gt; uses a real
&lt;a href=&quot;https://github.com/Kotlin/kotlinx.html/blob/master/generate/src/main/resources/html_5.xsd&quot;&gt;XSD schema&lt;/a&gt; to generate
all possible classes for the DSL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_there_is_always_a_problem&quot;&gt;There is always a problem&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This could already be awesome, but the example demonstrates a very weird behaviour — nobody stops you from defining
tags inside each other multiple times.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;the problem&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;head {
  head {
    head {
      // stil possible to write head because implicit receiver html is available
    }
  }
  title { +&quot;XML encoding with Kotlin&quot; }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Prior to Kotlin 1.1, the only solution was to redefine function with deprecation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;class Head : Tag(&quot;head&quot;) {
  @Deprecated(message = &quot;wrong scope&quot;, level = DeprecationLevel.ERROR)
  fun head(init: Head.() -&amp;gt; Unit) = initTag(Head(), init)

  fun title(init: Title.() -&amp;gt; Unit) = initTag(Title(), init)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/fantastic/err1.png&quot; alt=&quot;err1&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The problem with this approach is that it requires an incredible amount of boilerplate and a full understanding of all
possible combinations. In 1.1, &lt;a href=&quot;https://github.com/Kotlin/KEEP/blob/master/proposals/scope-control-for-implicit-receivers.md&quot;&gt;KEEP-57&lt;/a&gt; introduced an alternative to that approach: the &lt;code&gt;@DslMarker&lt;/code&gt; annotation was introduced which
allows us to define a &lt;code&gt;DSL marker&lt;/code&gt; and introduces a set of rules for classes annotated with that marker:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;an implicit receiver may belong to a DSL if marked with a corresponding DSL marker annotation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;two implicit receivers of the same DSL are not accessible in the same scope&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the closest one wins&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;other available receivers are resolved as usual, but if the resulting resolved call binds to such a receiver, it&amp;#8217;s a compilation error&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, the HTML DSL can be fixed by introducing a &lt;code&gt;@HtmlTagMarker&lt;/code&gt; DSL marker and annotating &lt;code&gt;Tag&lt;/code&gt; with it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;@HtmlTagMarker
abstract class Tag(val name: String) : Element {
 // ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/fantastic/err2.png&quot; alt=&quot;err2&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;DSLs that give us an ability to construct nested data structures such as HTML builders, different configurations, UI builders, etc. is
where Kotlin really shines. Kotlin took an awesome idea from Groovy and made it safe and easy to use.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are a few more examples of DSLs of that kind:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://blog.jetbrains.com/teamcity/2016/11/kotlin-configuration-scripts-an-introduction/&quot;&gt;TeamCity DSL&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://github.com/gradle/gradle-script-kotlin&quot;&gt;Gradle with Kotlin&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://github.com/gradle/gradle-script-kotlin&quot;&gt;Anko&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://spekframework.org&quot;&gt;Spek framework&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But unsurprisingly, it&amp;#8217;s not the only type of DSL that can be implemented in Kotlin&amp;#8230;&amp;#8203;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_fantastic_dsl&quot;&gt;Fantastic DSL&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Not all domains are born the same. Let&amp;#8217;s consider a completely different domain. A system which handles transactions
containing a payment in some currency and two people - a sender and a receiver.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/fantastic/domain.svg&quot; alt=&quot;domain&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The transaction structure has to be immutable to make it safer. But sometimes, we might need to create a new transaction with an updated field. For example, the name of the receiver (from) person might need to be changed to let&amp;#8217;s say &quot;John&quot;. There are a few ways to implement that in Kotlin&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_data_classes&quot;&gt;Data classes&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s start with an idiomatic Kotlin way. The class hierarchy can be concisely represented as&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;data&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;data class Transaction(val payment: Payment, val parts: Parts)
data class Payment(val currency: String, val amount: Int)
data class Parts(val from: Person, val to: Person)
data class Person(val id: Int, val name: String)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An instance of the &lt;code&gt;Transaction&lt;/code&gt; can easily be created as well&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;create&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val trs = Transaction(
  Payment(&quot;AUD&quot;, 15),
  Parts(
    Person(0, &quot;Alex&quot;),
    Person(1, &quot;Ben&quot;)
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But problems start when we need to update this nested data structure. Generally, there two ways to do that. The first option is to completely recreate the transaction which doesn&amp;#8217;t look good.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;update [1]&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val trans = Transaction(trs.payment, Parts(
  Person(trs.parts.from.id, &quot;John&quot;),
  trs.parts.to)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another is to use &lt;a href=&quot;https://kotlinlang.org/docs/reference/data-classes.html#copying&quot;&gt;copy&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;update [2]&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val stansTrs2 = trs.copy(
  parts = trs.parts.copy(
    from = trs.parts.from.copy(
      name = &quot;John&quot;
    )
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And the copy version doesn’t look good either. Even though it’s tolerable now, the bigger the data structure,
the uglier the code look like. On a deeply nested immutable data structure, it looks like a triangle instead of a
simple call chain from the mutable world.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;ohhhh&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val stansTrs2 = trs.copy(
  parts = trs.parts.copy(
    from = trs.parts.from.copy(
      person = trs.parts.from.person.copy(
        parts = trs.parts.from.person.parts.copy(
          from = trs.parts.from.person.parts.from.copy(
            person = trs.parts.from.person.parts.from.person.copy(
              parts = trs.parts.from.person.parts.from.person.parts.copy(
                from = trs.parts.from.person.parts.from.person.parts.from.copy(
                  person = trs.parts.from.person.parts.from.person.parts.from.person.copy(
                    parts = trs.parts.from.person.parts.from.person.parts.from.person.parts.copy(
                      from = trs.parts.from.person.parts.from.person.parts.from.person.parts.from.copy(
                        name = &quot;jonh&quot;
                      ))))))))))))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Don&amp;#8217;t get me wrong, I like parentheses. It feels like a lisp (which I like a lot), but what no one likes is the wall of boilerplate above.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_persistent_data_structures&quot;&gt;Persistent Data Structures&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But talking about lisps, there is another awesome language called Clojure. It&amp;#8217;s a lisp running on JVM where every data structure is persistent (don&amp;#8217;t confuse with &lt;a href=&quot;https://stackoverflow.com/questions/10034537/persistent-vs-immutable-data-structure&quot;&gt;immutable&lt;/a&gt;). In Clojure, the same problem can be solved by defining the transaction structure as a persistent map.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;create&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;(def ts {:payment {:currency &quot;AUD&quot;
                   :amount   15}
         :parts   {:from {:id   0
                          :name &quot;Alex&quot;}
                   :to   {:id   1
                          :name &quot;Ben&quot;}}})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Not as concise as Kotlin&amp;#8217;s version, but still pretty good. What is completely different to Kotlin, is the update function&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;update&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-clojure&quot; data-lang=&quot;clojure&quot;&gt;(def ts2 (assoc-in ts [:parts :from :name] &quot;John&quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It&amp;#8217;s only one line! And it&amp;#8217;s exactly what we aimed for. The next picture might be essential for understanding how it works.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/fantastic/domain_clj.svg&quot; alt=&quot;domain clj&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Given that each node has a known type - &lt;code&gt;clojure.lang.APersistentMap&lt;/code&gt; - and the universal way of traversing is &lt;code&gt;map.get(&quot;key&quot;)&lt;/code&gt;,
it&amp;#8217;s possible to write a function &lt;code&gt;assoc-in&lt;/code&gt; which can change a value under a given &quot;path&quot; and to recreate
the data structure &lt;a href=&quot;http://cjohansen.no/clojure-to-die-for/&quot;&gt;node by node&lt;/a&gt;. But Clojure&amp;#8217;s internals are plain java classes that
can be used from Kotlin easily just with a few &quot;convenience&quot; adapters to keep familiar syntax.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;create&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val tran = pArrayMap(
  &quot;payment&quot; to pArrayMap(
    &quot;currency&quot; to &quot;AUD&quot;,
    &quot;amount&quot; to 15
  ),
  &quot;parts&quot; to pArrayMap(
    &quot;from&quot; to pArrayMap(
      &quot;id&quot; to 0,
      &quot;name&quot; to &quot;Alex&quot;
    ),
    &quot;to&quot; to pArrayMap(
      &quot;id&quot; to 1,
      &quot;name&quot; to &quot;Ben&quot;
    )
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Yes, the creation looks rather ugly. It&amp;#8217;s untyped, all the key names are represented as strings, but let&amp;#8217;s look at the update function.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;update&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val trans2 = trans.pUpdate(listOf(&quot;parts&quot;, &quot;from&quot;, &quot;name&quot;), &quot;John&quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It&amp;#8217;s still as concise and beautiful as Clojure&amp;#8217;s one.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But is it possible to build a DSL which keeps types from Kotlin types and provides the conciseness of Clojure?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_cursor_dsl&quot;&gt;Cursor DSL&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is possible! Using a special DSL, you can define the structure of the &quot;transactional&quot; domain in a following way.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;interface Transaction
val &amp;lt;F&amp;gt; Cursor&amp;lt;Transaction, F&amp;gt;.payment by Node&amp;lt;Payment&amp;gt;()
val &amp;lt;F&amp;gt; Cursor&amp;lt;Transaction, F&amp;gt;.parts by Node&amp;lt;Parts&amp;gt;()

interface Payment
val &amp;lt;F&amp;gt; Cursor&amp;lt;Payment, F&amp;gt;.currency by Leaf&amp;lt;String&amp;gt;()
val &amp;lt;F&amp;gt; Cursor&amp;lt;Payment, F&amp;gt;.amount by Leaf&amp;lt;Int&amp;gt;()

interface Parts
val &amp;lt;F&amp;gt; Cursor&amp;lt;Parts, F&amp;gt;.to by Node&amp;lt;Person&amp;gt;()
val &amp;lt;F&amp;gt; Cursor&amp;lt;Parts, F&amp;gt;.from by Node&amp;lt;Person&amp;gt;()

interface Person
val &amp;lt;F&amp;gt; Cursor&amp;lt;Person, F&amp;gt;.id by Leaf&amp;lt;Int&amp;gt;()
val &amp;lt;F&amp;gt; Cursor&amp;lt;Person, F&amp;gt;.name by Leaf&amp;lt;String&amp;gt;()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This looks scary, but it&amp;#8217;s just a bit of necessary boilerplate. This code should be read like&lt;/p&gt;
&lt;/div&gt;
&lt;style&gt;
    .prh-keyword {
        color: #000080;
        font-weight: bold;
    }
    .prh-boilerplate {
        opacity: 0.2;
    }
&lt;/style&gt;
&lt;pre style=&quot;margin: 0; background: white; line-height: 125%&quot;&gt;
&lt;span class=&quot;prh-keyword&quot;&gt;interface&lt;/span&gt; Transaction
&lt;span class=&quot;prh-keyword&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;prh-boilerplate&quot;&gt; &amp;lt;F&amp;gt; Cursor&amp;lt;&lt;/span&gt;Transaction&lt;span class=&quot;prh-boilerplate&quot;&gt;, F&amp;gt;&lt;/span&gt;.payment &lt;span class=&quot;prh-keyword&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;prh-boilerplate&quot;&gt;Node&amp;lt;&lt;/span&gt;Payment&lt;span class=&quot;prh-boilerplate&quot;&gt;&amp;gt;()&lt;/span&gt;
&lt;span class=&quot;prh-keyword&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;prh-boilerplate&quot;&gt; &amp;lt;F&amp;gt; Cursor&amp;lt;&lt;/span&gt;Transaction&lt;span class=&quot;prh-boilerplate&quot;&gt;, F&amp;gt;&lt;/span&gt;.parts &lt;span class=&quot;prh-keyword&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;prh-boilerplate&quot;&gt;Node&amp;lt;&lt;/span&gt;Parts&lt;span class=&quot;prh-boilerplate&quot;&gt;&amp;gt;()&lt;/span&gt;

&lt;span class=&quot;prh-keyword&quot;&gt;interface&lt;/span&gt; Payment
&lt;span class=&quot;prh-keyword&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;prh-boilerplate&quot;&gt; &amp;lt;F&amp;gt; Cursor&amp;lt;&lt;/span&gt;Payment&lt;span class=&quot;prh-boilerplate&quot;&gt;, F&amp;gt;&lt;/span&gt;.currency &lt;span class=&quot;prh-keyword&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;prh-boilerplate&quot;&gt;Leaf&amp;lt;&lt;/span&gt;String&lt;span class=&quot;prh-boilerplate&quot;&gt;&amp;gt;()&lt;/span&gt;
&lt;span class=&quot;prh-keyword&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;prh-boilerplate&quot;&gt; &amp;lt;F&amp;gt; Cursor&amp;lt;&lt;/span&gt;Payment&lt;span class=&quot;prh-boilerplate&quot;&gt;, F&amp;gt;&lt;/span&gt;.amount &lt;span class=&quot;prh-keyword&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;prh-boilerplate&quot;&gt;Leaf&amp;lt;&lt;/span&gt;Int&lt;span class=&quot;prh-boilerplate&quot;&gt;&amp;gt;()&lt;/span&gt;

&lt;span class=&quot;prh-keyword&quot;&gt;interface&lt;/span&gt; Parts
&lt;span class=&quot;prh-keyword&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;prh-boilerplate&quot;&gt; &amp;lt;F&amp;gt; Cursor&amp;lt;&lt;/span&gt;Parts&lt;span class=&quot;prh-boilerplate&quot;&gt;, F&amp;gt;&lt;/span&gt;.to &lt;span class=&quot;prh-keyword&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;prh-boilerplate&quot;&gt;Node&amp;lt;&lt;/span&gt;Person&lt;span class=&quot;prh-boilerplate&quot;&gt;&amp;gt;()&lt;/span&gt;
&lt;span class=&quot;prh-keyword&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;prh-boilerplate&quot;&gt; &amp;lt;F&amp;gt; Cursor&amp;lt;&lt;/span&gt;Parts&lt;span class=&quot;prh-boilerplate&quot;&gt;, F&amp;gt;&lt;/span&gt;.from &lt;span class=&quot;prh-keyword&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;prh-boilerplate&quot;&gt;Node&amp;lt;&lt;/span&gt;Person&lt;span class=&quot;prh-boilerplate&quot;&gt;&amp;gt;()&lt;/span&gt;

&lt;span class=&quot;prh-keyword&quot;&gt;interface&lt;/span&gt; Person
&lt;span class=&quot;prh-keyword&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;prh-boilerplate&quot;&gt; &amp;lt;F&amp;gt; Cursor&amp;lt;&lt;/span&gt;Person&lt;span class=&quot;prh-boilerplate&quot;&gt;, F&amp;gt;&lt;/span&gt;.id &lt;span class=&quot;prh-keyword&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;prh-boilerplate&quot;&gt;Leaf&amp;lt;&lt;/span&gt;Int&lt;span class=&quot;prh-boilerplate&quot;&gt;&amp;gt;()&lt;/span&gt;
&lt;span class=&quot;prh-keyword&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;prh-boilerplate&quot;&gt; &amp;lt;F&amp;gt; Cursor&amp;lt;&lt;/span&gt;Person&lt;span class=&quot;prh-boilerplate&quot;&gt;, F&amp;gt;&lt;/span&gt;.name &lt;span class=&quot;prh-keyword&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;prh-boilerplate&quot;&gt;Leaf&amp;lt;&lt;/span&gt;String&lt;span class=&quot;prh-boilerplate&quot;&gt;&amp;gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;br/&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The creation looks very similar to the untyped version, but it&amp;#8217;s completely typed. It references properties defined above.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val trans = domain&amp;lt;Transaction&amp;gt; {
  (payment) {
    currency.set(&quot;AUD&quot;)
    amount.set(15)
  }
  (parts) {
    (from) {
      id.set(0)
      name.set(&quot;Alex&quot;)
    }
    (to) {
      id.set(1)
      name.set(&quot;Ben&quot;)
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It&amp;#8217;s possible to update the transaction easily. And not just one field, in fact, the code above creates an empty
data structure and applies an update function to it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val trans2 = trans.cursor.parts.from.update {
  name.set(&quot;John&quot;)
}
println(trans.cursor.parts.from.name.value) // &quot;Alex&quot;
println(trans2.cursor.parts.from.name.value) // &quot;John&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val trans3 = trans2.cursor.update {
  (payment) {
    currency.set(&quot;USD&quot;)
    amount.set(12)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What is really awesome is that the &lt;code&gt;set&lt;/code&gt; function can only be called inside the &lt;code&gt;update&lt;/code&gt; block. It&amp;#8217;s possible to think about the &lt;code&gt;update&lt;/code&gt;
block as an open transaction where a few updates are applied.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_implementation&quot;&gt;Implementation&lt;/h3&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_read&quot;&gt;Read&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The easiest way to start implementing it is to imagine that the data structure is already created and everything we need to do is to read
a value from it. The obvious untyped solution will be to call &lt;code&gt;trans.get(&quot;parts&quot;).get(&quot;from&quot;).get(&quot;name&quot;)&lt;/code&gt;. And this
approach works fine until we need to update it. After the first &lt;code&gt;get&lt;/code&gt; call, the reference to the root transaction is lost
and there&amp;#8217;ll be no way to run the update operation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Instead, it&amp;#8217;s possible to focus on the way of traversing the data structure without loosing the reference to the root. To accomplish this, it&amp;#8217;s possible to implement &lt;code&gt;Focus&lt;/code&gt; interface which holds the reference to the root and accumulates a path inside.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;interface Focus&amp;lt;out Op&amp;gt; {
  fun narrow(k: String): Focus&amp;lt;Op&amp;gt;
  val op: Op
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The interesting thing that &lt;code&gt;Focus&lt;/code&gt; is parametrised over an operation. That operation can be &lt;code&gt;Read&lt;/code&gt; or &lt;code&gt;Write&lt;/code&gt; depending on the context.
When a leaf is reached, the typed version will finally perform an action using that operation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;narrow down the usage&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;val f = Focus(trans)       // {&quot;root&quot; -&amp;gt; Transaction, path -&amp;gt; []}
val f2 = f.narrow(&quot;parts&quot;) // {&quot;root&quot; -&amp;gt; Transaction, path -&amp;gt; [&quot;parts&quot;]}
val f3 = f2.narrow(&quot;from&quot;) // {&quot;root&quot; -&amp;gt; Transaction, path -&amp;gt; [&quot;parts&quot;, &quot;from&quot;]}
// ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/fantastic/domain_focus.svg&quot; alt=&quot;domain focus&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But even though the focus does its job very well, it&amp;#8217;s completely untyped, and strings have to be used to navigate through.
The type must be stored somewhere. As everyone knows that any problem can be solved with an additional
layer of abstraction! Let&amp;#8217;s define a wrapper parametrised over the type of an underlying node.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;the missed layer&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;class Cursor&amp;lt;out T, out Op&amp;gt;(val f: Focus&amp;lt;Op&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;code&gt;Cursor&lt;/code&gt; is parametrised over a node type and the &lt;code&gt;operation&lt;/code&gt; is derived from the focus. And now, the &lt;code&gt;Transaction&lt;/code&gt; definition starts making sense. The narrowing can be delegated to the &lt;code&gt;Node&lt;/code&gt; object that knows the type and uses the name of a property to create a new &lt;code&gt;Cursor&lt;/code&gt; with a new &lt;code&gt;Focus&lt;/code&gt; inside.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;interface Transaction
val &amp;lt;F&amp;gt; Cursor&amp;lt;Transaction, F&amp;gt;.payment by Node&amp;lt;Payment&amp;gt;()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here, the &lt;code&gt;payment&lt;/code&gt; is an extension property on the &lt;code&gt;Transaction&lt;/code&gt; type which is just a marker interface. It will never be
instantiated, instead by delegating property to &lt;code&gt;Node&amp;lt;Payment&amp;gt;&lt;/code&gt;, the conversion
&lt;code&gt;Cursor&amp;lt;Transacton, F&amp;gt; &amp;#8658; Cursor&amp;lt;Payment, F&amp;gt;&lt;/code&gt; will be made.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;how Node is defined&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;open class Node&amp;lt;out T&amp;gt; {
  open operator fun &amp;lt;Op&amp;gt; getValue(ref: Cursor&amp;lt;*, Op&amp;gt;, property: KProperty&amp;lt;*&amp;gt;): Cursor&amp;lt;T, Op&amp;gt; {
    return Cursor(ref.f.narrow(property.name))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Inside &lt;code&gt;Node&lt;/code&gt;, a new Cursor is created with the focus narrowing down using a property name. Using this technique,
by just calling extension properties a focus can narrow down to the last node where the last node is delegated to &lt;code&gt;Leaf&lt;/code&gt;
instead of &lt;code&gt;Node&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;interface Person
val &amp;lt;F&amp;gt; Cursor&amp;lt;Person, F&amp;gt;.name by Leaf&amp;lt;String&amp;gt;()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;code&gt;Leaf&amp;lt;V&amp;gt;&lt;/code&gt; is defined in the same way as Node except for the return value of &lt;code&gt;getValue&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;open class Leaf&amp;lt;out V&amp;gt; {
  open operator fun &amp;lt;Op&amp;gt; getValue(ref: Cursor&amp;lt;*, Op&amp;gt;, property: KProperty&amp;lt;*&amp;gt;): Cursor&amp;lt;Leaf&amp;lt;V&amp;gt;, Op&amp;gt; {
    return Cursor(ref.f.narrow(property.name))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Leaf is needed to define an extension property that allows reading a
 value from that node. The property has the following signature &lt;code&gt;val &amp;lt;V, T&amp;gt; Cursor&amp;lt;Leaf&amp;lt;V&amp;gt;, Read&amp;lt;T&amp;gt;&amp;gt;.value: V&lt;/code&gt; which
 says: given the cursor focused on a leaf and parametrised over a read operation, provide a value contained by the leaf.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/fantastic/domain_red.png&quot; alt=&quot;domain red&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The remaining logic is described below&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;// the main data structure where T type - is the root type
// in our case, T is Transaction.
// root is just an empty persisntent map
class Domain&amp;lt;out T&amp;gt;(val root: PMap = PHashMap.EMPTY)

// The read operation that focus owns (Op)
interface Read&amp;lt;out M&amp;gt; {
  val path: Path         // path to the current node (ex. [&quot;payment&quot;, &quot;currency&quot;])
  val domain: Domain&amp;lt;M&amp;gt;  // the reference to the root
}

// the implementation of the focus
class Reader&amp;lt;out T&amp;gt;(val p: Path, val dm: Domain&amp;lt;T&amp;gt;) : Focus&amp;lt;Read&amp;lt;T&amp;gt;&amp;gt; {
  // this is how narrowing happens, just extend the path and keep the refernce to the root
  override fun narrow(k: String): Focus&amp;lt;Read&amp;lt;T&amp;gt;&amp;gt; = Reader(p.append(k), dm)

  override val op: Read&amp;lt;T&amp;gt; = object : Read&amp;lt;T&amp;gt; {
    override val domain: Domain&amp;lt;T&amp;gt; = dm
    override val path: Path = p
  }
}

// take a focus, take a read operation from it and ask for value
// by traversing the root using path
val &amp;lt;V, T&amp;gt; Cursor&amp;lt;Leaf&amp;lt;V&amp;gt;, Read&amp;lt;T&amp;gt;&amp;gt;.value: V
  get() = f.op.path.getIn(f.op.domain.root) as V

// this is how cursor get's created, emtpy path and reference to the root
val &amp;lt;T&amp;gt; Domain&amp;lt;T&amp;gt;.cursor: Cursor&amp;lt;T, Read&amp;lt;T&amp;gt;&amp;gt;
  get() = Cursor(Reader(Path.EMPTY, this))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_update&quot;&gt;Update&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So far we can traverse the data structure and read values from it. The next step is to learn how to update it. Problems start when we realise that the underlying data structure is persistent and there is no way to mutate it. To emulate mutation, a special wrapper has to be defined. It reassigns the reference after each mutation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;immutable &amp;#8658; mutable&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;class Mutable(var m: PMap) {
  fun write(p: Path, a: Any?) {
    m = p.assocIn(m, a)
  }

  fun read(p: Path) = p.getIn(m)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Then, we&amp;#8217;ll need to implement the &lt;code&gt;Write&lt;/code&gt; operation which supports reading and writing under a specific path. At first glance,
&lt;code&gt;read&lt;/code&gt; operation is unnecessary, but it&amp;#8217;s needed to read the final result after all modification were applied using an empty path. Another application of the &lt;code&gt;read()&lt;/code&gt; operation is node initialisation. E.g. if you create an empty domain and decide to write a value to leaf using a cursor, all the parent nodes need to be initialised first.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Op for &lt;code&gt;Cursor&amp;lt;T, Write&amp;gt;&lt;/code&gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;interface Write {
  fun read(): Any?
  fun write(a: Any?)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And the corresponding cursor&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;class WriterCursor(val m: Mutable, val path: Path) : Focus&amp;lt;Write&amp;gt; {
  // exactly the same narrowing pattern
  override fun narrow(k: String): Focus&amp;lt;Write&amp;gt; = WriterCursor(m, path.append(k))

  override val op: Write = object : Write {
    override fun write(a: Any?) = m.write(path, a)
    override fun read(): Any? = m.read(path)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And at some point in time, we might want to switch from the &lt;code&gt;Read&lt;/code&gt; cursor to the &lt;code&gt;Write&lt;/code&gt; cursor. For that, a special function exists.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Cursor&amp;lt;T, Read&amp;gt; &amp;#8658; Cursor&amp;lt;T, Write&amp;gt;&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;fun &amp;lt;T, M&amp;gt; Cursor&amp;lt;M, Read&amp;lt;T&amp;gt;&amp;gt;.update(update: Cursor&amp;lt;M, Write&amp;gt;.() -&amp;gt; Unit): Domain&amp;lt;T&amp;gt; {
  // take a root, make a mutable from it
  val m = Mutable(f.op.domain.root)
  // create a writer from mutable and apply `update` supplied from outside
  // exactly the same pattern as any other DSL has
  Cursor&amp;lt;M, Write&amp;gt;(WriterCursor(m, f.op.path)).update()
  // read the final value from the root and return a new instance of Domain
  return Domain(m.read(Path.EMPTY) as PMap)
}

// to simplify the initialisation
fun &amp;lt;M&amp;gt; domain(f: Cursor&amp;lt;M, Write&amp;gt;.() -&amp;gt; Unit) = Domain&amp;lt;M&amp;gt;().cursor.update(f)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And finally, a set of public typed operation that API consumers use&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;// for each leaf initial value is null
// for each node initial value is empty persistent map
fun Write.init(k: KClass&amp;lt;*&amp;gt;) {
  if (read() == null) {
    write(when (k) {
      Leaf::class -&amp;gt; null
      else -&amp;gt; PArrayMap.EMPTY
    })
  }
}

operator inline fun &amp;lt;reified T&amp;gt; Cursor&amp;lt;T, Write&amp;gt;.invoke(
    updateFn: Cursor&amp;lt;T, Write&amp;gt;.() -&amp;gt; Unit): Unit {
  // init the current node (it might be null if we haven't visited it before)
  f.op.init(T::class)
  updateFn()
}

fun &amp;lt;T&amp;gt; Cursor&amp;lt;Leaf&amp;lt;T&amp;gt;, Write&amp;gt;.set(t: T): Unit {
  // just delegate to write
  f.op.write(t)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;invoke&lt;/code&gt; function is responsible for Node initialisation whereas &lt;code&gt;set&lt;/code&gt; sets the Leaf&amp;#8217;s value&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;domain&amp;lt;Transaction&amp;gt; {
  (payment) {  // &amp;lt;- here invoke is called
    currency.set(&quot;AUD&quot;)
    amount.set(15)
  }
}

// ↑ is equal to the desugarised version ↓
domain&amp;lt;Transaction&amp;gt; {
  payment.invoke({
    currency.set(&quot;AUD&quot;)
    amount.set(15)
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And at the end, a Path that does all the work, but in fact, it does nothing except for delegating functionality to functions from
Clojure that do all the work on untyped persistent data structures.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-kotlin&quot; data-lang=&quot;kotlin&quot;&gt;import clojure.`core$assoc_in` as assocIn
import clojure.`core$get_in` as getIn
import clojure.lang.*

data class Path(private val v: APersistentVector) {
  companion object {
    val EMPTY = Path(PersistentVector.EMPTY)
  }

  fun append(a: String): Path = Path(v.cons(a) as APersistentVector)
  fun getIn(model: Any?): Any? = getIn.invokeStatic(model, v)
  fun assocIn(m: Any?, a: Any?): Any? = assocIn.invokeStatic(m, v, a)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Using these primitives, we built a really powerful type safe DSL to work on immutable data structures. Yes, it has a few downsides.
E.g. data classes solution has better performance. And most of the time it&amp;#8217;s concise enough, unless you have a really
deeply nested tree.
In that case, you might also try to use &lt;a href=&quot;https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/basic-lensing&quot;&gt;the lenses pattern&lt;/a&gt;
which comes from the functional world and solves the same problem. But if you already have untyped data structures in your project
and have to work with them, Kotlin provides a truly unique set of features that allows you to build a powerful DSL to make your life
safer and easier.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It&amp;#8217;s very probable that some parts of the solution shown above might still be unclear, in that case, I encourage you to clone &lt;a href=&quot;https://github.com/SerCeMan/talk-fantastic-dsls-example&quot;&gt;the code example&lt;/a&gt; in your IDE, run it and try to play with types. It will help a lot and can give you some interesting ideas on how advanced Kotlin features can be used.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Kotlin provides many unique features to build DSLs easily&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DSLs in Kotlin work best as configuration APIs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;They can be a powerful abstraction over untyped data structures&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_warnings&quot;&gt;Warnings&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Most of the time plain code is better than DSL&lt;br&gt;
There is no point in building DSL &quot;just because I can&quot;, plain Kotlin code is often much easier to read and understand.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a way to extend and bypass your DSL&lt;br&gt;
If you publish DSL as a part of your API, it&amp;#8217;s always a good idea to give a way to bypass or extend it. Of course, if it&amp;#8217;s a Gradle-like DSL then you can cover everything. But in the case of a html DSL, a user might want to introduce some tags that your DSL doesn&amp;#8217;t support. Or, he can have an already rendered string which needs to be inserted somewhere.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_links&quot;&gt;Links&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/SerCeMan/talk-fantastic-dsls-example&quot;&gt;Cursor DSL source code&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why you should use DSLs: &lt;a href=&quot;http://jonnyzzz.com/blog/2016/09/02/dsl-building/&quot;&gt;Building DSL Instead of an IDE Plugin&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why you shouldn&amp;#8217;t: &lt;a href=&quot;https://victor.kropp.name/blog/kotlin-dsls-good-bad-and-ugly/&quot;&gt;DSLs in Kotlin: The Good, the Bad and the Ugly&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_thanks&quot;&gt;Thanks&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Kotlin team for creating an awesome language! &lt;br&gt;
Please, press a ★ button on the &lt;a href=&quot;https://github.com/JetBrains/kotlin&quot;&gt;Kotlin&amp;#8217;s GitHub repo&lt;/a&gt; if you haven&amp;#8217;t done it yet.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/jetzajac&quot;&gt;@JetZajac&lt;/a&gt; who initially came up with the idea of persistent data structure based DSLs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You for reading it&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_share_this_article&quot;&gt;Share this article&lt;/h3&gt;
&lt;hr&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Just wrote a new &lt;a href=&quot;https://twitter.com/hashtag/kotlin?src=hash&quot;&gt;#kotlin&lt;/a&gt; blog post! &amp;quot;Fantastic DSLs and where to find them&amp;quot; &lt;a href=&quot;https://t.co/T9z5lFE45K&quot;&gt;https://t.co/T9z5lFE45K&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sergey Tselovalnikov (@SerCeMan) &lt;a href=&quot;https://twitter.com/SerCeMan/status/880365305314254848&quot;&gt;June 29, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Thu, 29 Jun 2017 00:00:00 +1000
</pubDate>
</item>
<item>
<guid>
http://serce.me/posts/01-06-2016-wild-panama/
</guid>
<link>
http://serce.me/posts/01-06-2016-wild-panama/
</link>
<title>
Pure assembly in the forest of Panama
</title>
<description>
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hi!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this article,  I’ll tell you about some internal features of Project Panama. You’ll find out
how to increase the performance of your Java program using a pure inline assembler.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/wild-panama/panama.jpg&quot; alt=&quot;panama&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph text-center&quot;&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;quoteblock&quot;&gt;
&lt;blockquote&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We had two builds of jvm, seventy-five native functions, five sheets of high powered method handles, a Panama repository full of crazy features, and a whole galaxy of native data layouts, headers, compilers, optimizations&amp;#8230;&amp;#8203; and also a quart of heap, a case of wrappers, a pint of raw memory and two dozen AVX2 instructions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Not that we needed all that for the trip to Panama, but once you get locked into a serious jvm crash collection, the tendency is to push it as far as you can.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock important&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Important&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
This article is written mostly about something that may never be released&lt;br&gt;
About API that may never be seen&lt;br&gt;
About code you shouldn&amp;#8217;t use in production&lt;br&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock warning&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Warning&lt;/div&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
A lot of information in this article is based on my personal experiments with the internal state of Panama forest in June 2016, so it may be deprecated when you are reading it
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, let&amp;#8217;s begin our journey.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_welcome_to_panama&quot;&gt;Welcome to Panama&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://openjdk.java.net/projects/panama/&quot;&gt;Panama&lt;/a&gt; is a new project under OpenJDK that tries to improve the connection between JVM and foreign APIs, including many interfaces commonly used by C programmers.
It is the missing piece in the Java ecosystem, a bridge between JAVA and native code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The primary features that will be introduced in Project Panama are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Native function calling and data access, respectfully, with huge JIT support (see &lt;a href=&quot;http://openjdk.java.net/jeps/191&quot;&gt;JEP191&lt;/a&gt;)&lt;br&gt;
(Similar problems but without huge runtime support can be solved using JNR as explained here &lt;a href=&quot;/posts/22-06-2015-jnr-fuse/&quot;&gt;previous article&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New data layouts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Special tools for wrapping native libraries&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The full overview of the problems that Panama tries to solve can be found here: &lt;a href=&quot;https://blogs.oracle.com/jrose/entry/the_isthmus_in_the_vm&quot;&gt;blog post&lt;/a&gt; (written by John Rose).
But some features in the mercurial forest of Project Panama don&amp;#8217;t really belong to JEP 191. These features are Vector API and Machine Code Snippets.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Last December, Vladimir Ivanov, one of the core contributors of Panama project made a commit where he introduced an ability to call a snippet of machine code in runtime&amp;#8230;&amp;#8203;
&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Project Panama: Machine code snippets and vector values support in HotSpot JVM &lt;a href=&quot;https://t.co/lW9GvKKk5h&quot;&gt;https://t.co/lW9GvKKk5h&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/java?src=hash&quot;&gt;#java&lt;/a&gt;&lt;/p&gt;&amp;mdash; Vladimir Ivanov (@iwan0www) &lt;a href=&quot;https://twitter.com/iwan0www/status/672824680227708928&quot;&gt;December 4, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;br&gt;
This is an amazing feature, you can make an inline assembler call, crazy stuff&amp;#8230;&amp;#8203; It’s like the new Unsafe, but even cooler!
It’s like writing your own intrinsic, but in runtime. In this post I&amp;#8217;ll be primarily focused on Machine Code Snippets. So let&amp;#8217;s explore this opportunity.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_the_edge_of_the_forest&quot;&gt;The edge of the forest&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first program that every programmer writes in a new language is &quot;Hello, World!&quot;. But it’s assembler, and it is called from Java. So let&amp;#8217;s make it simple.&lt;br&gt;
For example, an A+B+C function looks like this in each:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Plain Java&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public static int sum(int a, int b, int c) {
    return a + b + c;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;X86 assembly&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;...
mov rax, rsi ; res = arg1
add rax, rdi ; res += arg2
add rax, rdx ; res += arg3
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;CodeSnippet&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;static final MethodHandle sum3 = jdk.internal.panama.CodeSnippet.make(
            &quot;sum3&quot;, MethodType.methodType(int.class,/*result*/
                                          int.class /*rdi*/,
                                          int.class /*rsi*/,
                                          int.class /*rdx*/),
            true, /* isSupported */
            0x48, 0x89, 0xF0, // mov    rax,rsi
            0x48, 0x01, 0xF8, // add    rax,rdi
            0x48, 0x01, 0xD0  // add    rax,rdx
    );&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here we used &lt;code&gt;jdk.internal.panama.CodeSnippet&lt;/code&gt; class to get MethodHandle to native code. And yes, this package is functionally important, it actually means internal API, so you very probably won&amp;#8217;t be able to use it.&lt;br&gt;
As an &lt;a href=&quot;http://hg.openjdk.java.net/panama/panama/hotspot/file/6818b4b2e922/src/cpu/x86/vm/sharedRuntime_x86_64.cpp#l1141&quot;&gt;arguments&lt;/a&gt; of &lt;code&gt;MethodType#methodType&lt;/code&gt; you can pass primitives and some special classes like &lt;code&gt;Long2&lt;/code&gt; (128 bit register), &lt;code&gt;Long4&lt;/code&gt;  (256 bit register) and &lt;code&gt;Long8&lt;/code&gt; (512 bit register).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Based on what you’ve seen above, you could say that we were able to use JNI before, so what’s the point of using inline ASM? This is true, but the thing is the C2 compiler can easily inline the code snippet. So, it gives you an opportunity (if you’re crazy enough) to write your own JVM intrinsic without coding it in the JVM.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s compare assembly produced by the JVM after compiling and inlining for every method.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-cols stretch fit-table&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 33.3333%;&quot;&gt;
&lt;col style=&quot;width: 33.3333%;&quot;&gt;
&lt;col style=&quot;width: 33.3334%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Plain Java&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;CodeSnippet ASM&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;JNI&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;[Verified Entry Point]
 sub  rsp,0x18
 mov  QWORD PTR [rsp+0x10],rbp  ;*synch entry

jitresult:
 mov  eax,DWORD PTR [rsi+0x1c]
 add  eax,DWORD PTR [rsi+0x18]
 add  eax,DWORD PTR [rsi+0x20]  ;*iadd


exit:
 add  rsp,0x10
 pop  rbp
 test DWORD PTR [rip+0x15b4ea60],eax&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;[Verified Entry Point]
 sub  rsp,0x18
 mov  QWORD PTR [rsp+0x10],rbp;*sync entry
 mov  r10,rsi
 mov  esi,DWORD PTR [rsi+0x1c] ;*field b
 mov  edx,DWORD PTR [r10+0x20] ;*field c
 mov  edi,DWORD PTR [r10+0x18] ;*field a

snippet:
 mov  rax,rsi
 add  rax,rdi
 add  rax,rdx

exit:
 add  rsp,0x10
 pop  rbp
 test DWORD PTR [rip+0x16d21852],eax&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;[Verified Entry Point]
 mov  DWORD PTR [rsp-0x14000],eax
 push rbp
 sub  rsp,0x10           ;*sync entry

 mov  edx,DWORD PTR [rsi+0x1c]  ;*field b
 mov  ecx,DWORD PTR [rsi+0x20]  ;*field c
 mov  esi,DWORD PTR [rsi+0x18]  ;*field a

native_call:
 xchg ax,ax
 call 0x00007f7ab5668738

exit:
 add  rsp,0x10
 pop  rbp
 test DWORD PTR [rip+0x166add39],eax
 ret  ;*invokestatic s_nat

runtime_call_rethrow_Java:
 mov    rsi,rax
 add    rsp,0x10
 pop    rbp
 jmp    0x00007f7aadc7b6e0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you can see here the only difference between the C2 JIT version and our CodeSnippet is the movement of arguments between registers to satisfy calling convention. And the C2 perfectly inlined exactly the same piece of code as shown above. At the same time, JNI performs a real native call.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But what’s the point of writing inline asm snippets in Java? Usually there is no reason to do so, the C2 is able to compile your code into something that works much faster. But there are several things that the C2 can’t do efficiently. The most important is that the C2 can’t rewrite your algorithm using SIMD operations yet.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_go_deeper_to_hidden_places&quot;&gt;Go deeper to hidden places&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Usually our applications are not about A+B+C functions, but about some real code. And our applications can contain, say, the function that calculates checksums of buffers. A perfectly real task, that you can encounter in different kinds of software.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s imagine our application has a little function called checksum that makes a sum of bytes in the buffer and gives us hash [0, 256) as a result.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here’s the code:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;private static int checksumPlainJava(ByteBuffer buffer, int size) {
    int checksum = 0;
    for (int i = 0; i &amp;lt; size; ++i) {
        checksum += buffer.get(i);
    }
    // make it unsigned first to avoid negative result
    return (int) (Integer.toUnsignedLong(checksum) % 256);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In our application we operate big byte buffers and we have to calculate checksums very often. We discovered that this checksum function is our bottleneck. And we need to optimize it. What options do we have?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_jni&quot;&gt;JNI&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You may see on the last line the ugly operation where we are trying to convert our signed int to unsigned to get the proper result. Of course, it’s the bottleneck you might think, isn&amp;#8217;t it? The cool C++ has unsigned variables - let&amp;#8217;s make a JNI call!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Ok, here we go, C++ code:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;JNI checksum&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;JNIEXPORT jint JNICALL Java_me_serce_panex_ChecksumBenchmark_nativePlainChecksum
    (JNIEnv * env, jclass clz, jlong addr, jint targetLength) {
    char *target = reinterpret_cast&amp;lt;char *&amp;gt;(addr);
    unsigned int checksum = 0;
    for (int i = 0; i &amp;lt; targetLength; ++i) {
        checksum += (unsigned int) target[i];
    }
    return checksum % 256;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now we have to check the performance. We may expect incredible results. For performance measurement we will be using &lt;a href=&quot;http://openjdk.java.net/projects/code-tools/jmh/&quot;&gt;JMH&lt;/a&gt;, the de-facto standard in Java benchmarking. You can find a great deal of articles answering the question &quot;why JMH?&quot; on the internet.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There is no way to get a native memory address for DirectByteBuffer, so we are using reflection trick here to get the field that contains this address. Now we’re able to access memory from C++ code directly. We’re checking how fast the function is in case of &lt;em&gt;4&lt;/em&gt;/&lt;em&gt;8096&lt;/em&gt;/&lt;em&gt;129536&lt;/em&gt; size buffers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Benchmark setup&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;private ByteBuffer buffer;
private long address = 0;

@Param({&quot;4&quot;, &quot;8096&quot;, &quot;129536&quot;})
private int size = 4;

public static long getAddress(ByteBuffer buffy) throws Throwable {
    Field address = Buffer.class.getDeclaredField(&quot;address&quot;);
    address.setAccessible(true);
    return address.getLong(buffy);
}

@Setup
public void setup() throws Throwable {
    buffer = ByteBuffer.allocateDirect(size).order(ByteOrder.nativeOrder());
    ThreadLocalRandom random = ThreadLocalRandom.current();
    for (int i = 0; i &amp;lt; size / 4; i++) {
        buffer.putInt(random.nextInt());
    }
    address = getAddress(buffer);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;literalblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;And the results&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;Benchmark                       (size)  Mode  Cnt   Score    Error  Units
ChecksumBenchmark.JNI_Checksum       4  avgt    3   0.009 ±  0.001  us/op
ChecksumBenchmark.JNI_Checksum    8096  avgt    3   3.085 ±  0.039  us/op
ChecksumBenchmark.JNI_Checksum  129536  avgt    3  48.879 ±  5.655  us/op
ChecksumBenchmark.plainJava          4  avgt    3   0.006 ±  0.001  us/op
ChecksumBenchmark.plainJava       8096  avgt    3   2.190 ±  0.834  us/op
ChecksumBenchmark.plainJava     129536  avgt    3  34.452 ±  3.341  us/op&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you can see, the JNI loop is slower. But what happened? Could it mean that JNI is really slow? As we saw earlier CodeSnippet is faster. So we can try the same with code, but written using CodeSnippet!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;However, it may be hard to write code in pure machine codes, so we can make it another way. We can write C++ code; then compile it, open it in a hex editor and put the machine code into our method. Sounds creepy, but it’s possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Several things you should be careful about:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You shouldn&amp;#8217;t have a ret instruction, JVM will take care of it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You should look carefully through your assembly code to be sure that it doesn&amp;#8217;t try to access outside memory using an outside method.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And, finally, you should be careful about calling convention&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Typical &lt;code&gt;ls&lt;/code&gt; picture that you can see get after several experiments&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/wild-panama/crashes.png&quot; alt=&quot;crashes&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here’s the code and we’re ready to run benchmark again&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;static final MethodHandle codeSnippetChecksum = jdk.internal.panama.CodeSnippet.make(
        &quot;checksum&quot;, MethodType.methodType(int.class, long.class, int.class),
        isX64(),
        0x48, 0x85, 0xF6, 0x74, 0x1E, 0x48, 0x01, 0xFE, 0x31, 0xC0, 0x66, 0x0F, 0x1F, 0x44,
        0x00, 0x00, 0x0F, 0xBE, 0x17, 0x48, 0x83, 0xC7, 0x01, 0x01, 0xD0, 0x48, 0x39, 0xF7,
        0x75, 0xF2, 0x0F, 0xB6, 0xC0, 0xEB, 0x02, 0x31, 0xC0);

@Benchmark
public int codeSnippetChecksum() throws Throwable {
    return (int) plainC_O2.invoke(address, size);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;literalblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Result&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;Benchmark                              (size)  Mode  Cnt   Score    Error  Units
ChecksumBenchmark.JNI_Checksum              4  avgt    4   0.008 ±  0.001  us/op
ChecksumBenchmark.JNI_Checksum           8096  avgt    4   3.060 ±  0.056  us/op
ChecksumBenchmark.JNI_Checksum         129536  avgt    4  49.865 ±  2.135  us/op
ChecksumBenchmark.codeSnippetChecksum       4  avgt    4   0.005 ±  0.001  us/op
ChecksumBenchmark.codeSnippetChecksum    8096  avgt    4   2.806 ±  0.243  us/op
ChecksumBenchmark.codeSnippetChecksum  129536  avgt    4  48.911 ±  0.448  us/op
ChecksumBenchmark.plainJava                 4  avgt    4   0.006 ±  0.001  us/op
ChecksumBenchmark.plainJava              8096  avgt    4   2.163 ±  0.035  us/op
ChecksumBenchmark.plainJava            129536  avgt    4  34.414 ±  0.984  us/op&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And finally, you can observe pretty much the same results. The only noticeable difference is for buffers that have a very small size. And even the CodeSnippet version is slower than the code produced by JIT.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The key is I used -O2 GCC option, which doesn&amp;#8217;t perform a lot of interesting optimizations.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;literalblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;g++ -shared -fpic  -Wall -O2   -I/usr/include ... checksum.c -o libchecksum.so&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And as a result, GCC didn&amp;#8217;t perform well, and we&amp;#8217;ve got an almost literal translation of that we wrote in C++ to assembly. At the same time, JIT gave us a good unrolled loop.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-cols stretch&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;JIT&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;GCC O2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;....
loop:
 movsx  r10d,BYTE PTR [rbp+0x7]
 movsx  r8d,BYTE PTR [rbp+0x6]
 movsx  r11d,BYTE PTR [rbp+0x5]
 movsx  ebx,BYTE PTR [rbp+0x4]
 movsx  ecx,BYTE PTR [rbp+0x3]
 movsx  edx,BYTE PTR [rbp+0x2]
 movsx  edi,BYTE PTR [rbp+0x1]
 movsx  ebp,BYTE PTR [rbp+0x0]
 add    eax,ebp
 add    eax,edi
 add    eax,edx
 add    eax,ecx
 add    eax,ebx
 add    eax,r11d
 add    eax,r8d
 add    eax,r10d
 add    r9d,0x8 ; i+= 8
 cmp    r9d,r13d
 jl     loop  ;*if_icmpge
....&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;...
loop:
 movsx  edi,BYTE PTR [rsi+rdx*1]
 add    rsi,0x1 ; i+= 1
 add    eax,edi
 cmp    ecx,esi ; if return
 jg     loop
 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, we can use -O3 if we need more optimizations.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;literalblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;With -03&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;Benchmark                                (size)  Mode  Cnt   Score    Error  Units
ChecksumBenchmark.JNI_Checksum                4  avgt    4   0.009 ±  0.001  us/op
ChecksumBenchmark.JNI_Checksum             8096  avgt    4   3.089 ±  0.066  us/op
ChecksumBenchmark.JNI_Checksum           129536  avgt    4  49.481 ±  2.071  us/op
ChecksumBenchmark.codeSnippetChecksum         4  avgt    4   0.005 ±  0.001  us/op
ChecksumBenchmark.codeSnippetChecksum      8096  avgt    4   2.784 ±  0.153  us/op
ChecksumBenchmark.codeSnippetChecksum    129536  avgt    4  49.350 ±  2.208  us/op
ChecksumBenchmark.codeSnippetChecksumO3       4  avgt    4   0.006 ±  0.001  us/op
ChecksumBenchmark.codeSnippetChecksumO3    8096  avgt    4   0.621 ±  0.022  us/op
ChecksumBenchmark.codeSnippetChecksumO3  129536  avgt    4   9.672 ±  0.201  us/op
ChecksumBenchmark.plainJava                   4  avgt    4   0.006 ±  0.001  us/op
ChecksumBenchmark.plainJava                8096  avgt    4   2.161 ±  0.089  us/op
ChecksumBenchmark.plainJava              129536  avgt    4  34.825 ±  1.178  us/op&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There is a simple explanation why GCC -03 version is faster than code emitted by JIT. Here GCC was able to vectorize our loop. So, it used SIMD instructions which gave our processor an ability to &quot;parallelize&quot; execution.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-cols stretch&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;col style=&quot;width: 50%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;JIT&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;GCC O3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;....
loop:
 movsx  r10d,BYTE PTR [rbp+0x7]
 movsx  r8d,BYTE PTR [rbp+0x6]
 movsx  r11d,BYTE PTR [rbp+0x5]
 movsx  ebx,BYTE PTR [rbp+0x4]
 movsx  ecx,BYTE PTR [rbp+0x3]
 movsx  edx,BYTE PTR [rbp+0x2]
 movsx  edi,BYTE PTR [rbp+0x1]
 movsx  ebp,BYTE PTR [rbp+0x0]
 add    eax,ebp
 add    eax,edi
 add    eax,edx
 add    eax,ecx
 add    eax,ebx
 add    eax,r11d
 add    eax,r8d
 add    eax,r10d
 add    r9d,0x8 ; i+= 8
 cmp    r9d,r13d
 jl     loop  ;*if_icmpge
....&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt;....
loop:
 add          r11, 0x1
 add          r8, 0x20
 cmp          r10, r11
 vpmovsxbw    ymm2, xmm1
 vextracti128 xmm1, ymm1, 0x1
 vpmovsxwd    ymm3, xmm2
 vextracti128 xmm2, ymm2, 0x1
 vpmovsxbw    ymm1, xmm1
 vpaddd       ymm0, ymm3, ymm0
 vpmovsxwd    ymm2, xmm2
 vpaddd       ymm0, ymm2, ymm0
 vpmovsxwd    ymm2, xmm1
 vextracti128 xmm1, ymm1, 0x1
 vpaddd       ymm0, ymm2, ymm0
 vpmovsxwd    ymm1, xmm1
 vpaddd       ymm0, ymm1, ymm0
 ja           loop
....&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But what if we need more performance? Can we do it better than GCC?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_simd&quot;&gt;SIMD&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is possible to write the same code, but using AVX2 (256 byte registers) instructions. (Thanks, &lt;a href=&quot;https://twitter.com/kellylittlepage&quot;&gt;@kellylittlepage&lt;/a&gt;, for an &lt;a href=&quot;https://www.klittlepage.com/2013/12/10/accelerated-fix-processing-via-avx2-vector-instructions/&quot;&gt;awesome article&lt;/a&gt; where I&amp;#8217;ve read how to do it).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;C++ function that will be compiled and putted in CodeSnippet&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;int avxChecksumAVX2(const char *const target, size_t targetLength) {
    const __m256i zeroVec = _mm256_setzero_si256();
    short d[16] = {1, 1, 1, 1, 1, 1, 1, 1,
                   1, 1, 1, 1, 1, 1, 1, 1};
    const __m256i oneVec = *((__m256i *) d);
    __m256i accum = _mm256_setzero_si256();
    unsigned int checksum = 0;
    size_t offset = 0;

    if (targetLength &amp;gt;= 32) {
        for (; offset &amp;lt;= targetLength - 32; offset += 32) {
            __m256i vec = _mm256_loadu_si256(
                    reinterpret_cast&amp;lt;const __m256i *&amp;gt;(target + offset));
            __m256i vl = _mm256_unpacklo_epi8(vec, zeroVec);
            __m256i vh = _mm256_unpackhi_epi8(vec, zeroVec);

            accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vl, oneVec));
            accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vh, oneVec));
        }
    }

    for (; offset &amp;lt; targetLength; ++offset) {
        checksum += (int) target[offset];
    }

    accum = _mm256_add_epi32(accum, _mm256_srli_si256(accum, 4));
    accum = _mm256_add_epi32(accum, _mm256_srli_si256(accum, 8));
    return (_mm256_extract_epi32(accum, 0) + _mm256_extract_epi32(accum, 4) +
            checksum) % 256;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is how a simple checksum function looks like after rewriting for vectorizing execution. Here, some GCC intrinsics like &lt;code&gt;&lt;a href=&quot;https://software.intel.com/en-us/node/524002&quot;&gt;_mm256_unpacklo_epi8&lt;/a&gt;&lt;/code&gt; and &lt;code&gt;&lt;a href=&quot;https://software.intel.com/en-us/node/513929&quot;&gt;_mm256_add_epi32&lt;/a&gt;&lt;/code&gt; are used. GCC has a special implementation for this functions that uses AVX2 instructions. Almost always it is just one instruction.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://software.intel.com/sites/landingpage/IntrinsicsGuide/&quot;&gt;Here&lt;/a&gt; you can find a full guide of Intel intrinsics&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This functions isn&amp;#8217;t so easy to understand, but how fast is it?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;literalblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Result&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;ChecksumBenchmark.JNI_Checksum                4  avgt    4   0.008 ±  0.001  us/op
ChecksumBenchmark.JNI_Checksum             8096  avgt    4   3.128 ±  0.024  us/op
ChecksumBenchmark.JNI_Checksum           129536  avgt    4  49.629 ±  0.694  us/op
ChecksumBenchmark.avx2Impl                    4  avgt    4   0.014 ±  0.001  us/op
ChecksumBenchmark.avx2Impl                 8096  avgt    4   0.239 ±  0.018  us/op
ChecksumBenchmark.avx2Impl               129536  avgt    4   4.128 ±  0.052  us/op
ChecksumBenchmark.codeSnippetChecksum         4  avgt    4   0.005 ±  0.001  us/op
ChecksumBenchmark.codeSnippetChecksum      8096  avgt    4   2.795 ±  0.044  us/op
ChecksumBenchmark.codeSnippetChecksum    129536  avgt    4  49.656 ±  0.733  us/op
ChecksumBenchmark.codeSnippetChecksumO3       4  avgt    4   0.006 ±  0.001  us/op
ChecksumBenchmark.codeSnippetChecksumO3    8096  avgt    4   0.630 ±  0.004  us/op
ChecksumBenchmark.codeSnippetChecksumO3  129536  avgt    4   9.810 ±  0.100  us/op
ChecksumBenchmark.plainJava                   4  avgt    4   0.006 ±  0.001  us/op
ChecksumBenchmark.plainJava                8096  avgt    4   2.224 ±  0.122  us/op
ChecksumBenchmark.plainJava              129536  avgt    4  35.042 ±  0.252  us/op&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Awesome it is 8x times faster than our original code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;_java_way&quot;&gt;Java way&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s say, now we met our performance requirements, but can we make it more readable than just an ugly blob of ASM code produced by GCC? It is possible to save the main loop inside Java and use Long4 vectors to pass data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Java version of that scary function&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class VectorIntrinsics {
    ...
    private static final MethodHandle _mm256_loadu_si256 = jdk.internal.panama.CodeSnippet.make(
            &quot;_mm256_loadu_si256&quot;, MethodType.methodType(Long4.class, long.class),
            true,
            0xC5, 0xFE, 0x6F, 0x06 // vmovdqu ymm0, YMMWORD PTR [rdi]
    );
    public static Long4 _mm256_loadu_si256(long address) throws Throwable {
        return (Long4) _mm256_loadu_si256.invoke(address);
    }
    ...
}

private static int JAVA_avxChecksumAVX2(ByteBuffer buffer, long target, int targetLength)
    throws Throwable {
        Long4 zeroVec = Long4.ZERO;
        Long4 oneVec = ones;
        Long4 accum = Long4.ZERO;
        int checksum = 0;
        int offset = 0;

        if (targetLength &amp;gt;= 32) {
            for (; offset &amp;lt;= targetLength - 32; offset += 32) {
                Long4 vec = _mm256_loadu_si256(target + offset);
                Long4 vl = _mm256_unpacklo_epi8(vec, zeroVec);
                Long4 vh = _mm256_unpackhi_epi8(vec, zeroVec);

                accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vl, oneVec));
                accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vh, oneVec));
            }
        }

        for (; offset &amp;lt; targetLength; ++offset) {
            checksum += (int) buffer.get(offset);
        }

        accum = _mm256_add_epi32(accum, _mm256_srli_si256_4(accum));
        accum = _mm256_add_epi32(accum, _mm256_srli_si256_8(accum));
        long finalChecksum = _mm256_extract_epi32_0(accum) + _mm256_extract_epi32_4(accum)
                        + checksum;
        return (int) (Integer.toUnsignedLong((int) finalChecksum) % 256);
    }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now it is written in the right way. We wrote a lot of small methods; every method represents one small AVX2 instruction. And the main loop is written in Java. This code is reusable; it is much easier to write and understand than trying to write one big ASM blob. But, a big surprise, it is much slower than the ugly ASM blob.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And again, JMH will help us to find answer with gc profiler.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;literalblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;That&amp;#8217;s why&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;JAVA_avx2Impl                                129536  avgt    4      30.394 ±     6.813   us/op
JAVA_avx2Impl:·gc.alloc.rate                 129536  avgt    4         NaN              MB/sec
JAVA_avx2Impl:·gc.count                      129536  avgt    4      34.000              counts
JAVA_avx2Impl:·gc.time                       129536  avgt    4      39.000                  ms
avx2Impl                                     129536  avgt    4       4.192 ±     0.246   us/op
avx2Impl:·gc.alloc.rate                      129536  avgt    4         NaN              MB/sec
avx2Impl:·gc.count                           129536  avgt    4         ≈ 0              counts&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;code&gt;JAVA_avxChecksumAVX2&lt;/code&gt; produces high allocation rate. Despite the fact that vector types work with escape analysis really well, this loop breaks our hopes. Because Long4 is immutable, we have to save &lt;code&gt;accum&lt;/code&gt; to the same variable on every loop iteration. Escape analysis can&amp;#8217;t understand this and we are getting a lot of allocations of boxed vector values.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Problematic code for Escape Analysis&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;Long accum = Long4.ZERO;
for (; offset &amp;lt;= targetLength - 32; offset += 32) {
    Long4 vec = _mm256_loadu_si256(target + offset);
    accum = operation(accum, vec); // EA, you are drunk, go home
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This problem is known issue. Very probably it will be fixed soon, but how can it be solved now?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As a workaround, we may try to create a temporary buffer and use a pair of &lt;code&gt;_mm256_loadu_si256&lt;/code&gt; and &lt;code&gt;_mm256_storeu_si256&lt;/code&gt; instructions on every iteration. That intrinsics use &lt;code&gt;vmovdqu&lt;/code&gt; instruction to load/store register value to the memory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;GC free solution&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;static final ByteBuffer tmpBuf = ...
...
for (; offset &amp;lt;= targetLength - 32; offset += 32) {
    Long4 vec = _mm256_loadu_si256(target + offset);
    Long4 accum = _mm256_loadu_si256(tmpBuffAddr);
    Long4 result = operation(accum, vec);
    _mm256_storeu_si256(tmpBuffAddr, result);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;literalblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Results&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;Benchmark                                       (size)  Mode  Cnt   Score   Error   Units
ChecksumBenchmark.JAVA_avx2Impl                 129536  avgt    4  23.837 ± 0.064   us/op
ChecksumBenchmark.JAVA_avx2Impl:·gc.alloc.rate  129536  avgt    4     NaN          MB/sec
ChecksumBenchmark.JAVA_avx2Impl:·gc.count       129536  avgt    4     ≈ 0          counts&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now function is GC free; there is no garbage anymore and it is faster, but actually it&amp;#8217;s still quite slow. To understand why we should use a profiler, but simple solutions like Yourkit or JProfiler won&amp;#8217;t help us, we must work on instruction level. Thank goodness, JMH has an excellent support of perf profiler, you need just to pass an option to it (don&amp;#8217;t forget to install perf on your system before).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-x86asm&quot; data-lang=&quot;x86asm&quot;&gt; 12.39%   26.58%    vmovdqu YMMWORD PTR [rsp+0x40],ymm0
 12.88%    2.85%    movabs r10,0x6d61010e8
           0.01%    vmovdqu ymm1,YMMWORD PTR [r10+0x10]
  0.01%             vmovdqu ymm0,YMMWORD PTR [rsp+0x20]
                    vpunpcklbw ymm0,ymm0,ymm1
  4.42%    0.03%    movabs r10,0x6d61010b8
  0.01%             vmovdqu ymm1,YMMWORD PTR [r10+0x10]
  0.02%    0.01%    vpmaddwd ymm0,ymm0,ymm1
  0.02%    0.01%    vpmaddwd ymm0,ymm0,ymm1
           0.02%    vmovdqu ymm1,ymm0
  4.20%    2.95%    vmovdqu ymm0,YMMWORD PTR [rsp+0x40]
  8.45%   22.88%    vpaddd ymm0,ymm1,ymm0
 12.91%    5.79%    vmovdqu YMMWORD PTR [rsp+0x40],ymm0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you can see, we are spending an enormous amount of time just to load out the temporary buffer and store it back just to avoid GC. So, we can rewrite algorithm a little bit instead. We&amp;#8217;ll be saving a final result to &lt;code&gt;checksum&lt;/code&gt; variable right in the loop instead of using it further in vector calculations.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Here the code&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;for (; offset &amp;lt;= targetLength - 32; offset += 32) {
    Long4 vec = _mm256_loadu_si256(target + offset);
    Long4 lVec = _mm256_unpacklo_epi8(vec, zeroVec);
    Long4 hVec = _mm256_unpackhi_epi8(vec, zeroVec);
    Long4 sum = _mm256_add_epi16(lVec, hVec);
    sum = _mm256_hadd_epi16(sum, sum);
    sum = _mm256_hadd_epi16(sum, sum);
    sum = _mm256_hadd_epi16(sum, sum);
    checksum += _mm256_extract_epi16_0(sum) + _mm256_extract_epi16_15(sum);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;literalblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Benchmark results&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre&gt;Benchmark                        (size)  Mode  Cnt   Score    Error  Units
ChecksumBenchmark.JAVA_avx2Impl       4  avgt    4   0.005 ±  0.001  us/op
ChecksumBenchmark.JAVA_avx2Impl    8096  avgt    4   1.245 ±  0.028  us/op
ChecksumBenchmark.JAVA_avx2Impl  129536  avgt    4  20.095 ±  0.314  us/op
ChecksumBenchmark.avx2Impl            4  avgt    4   0.013 ±  0.001  us/op
ChecksumBenchmark.avx2Impl         8096  avgt    4   0.211 ±  0.004  us/op
ChecksumBenchmark.avx2Impl       129536  avgt    4   3.317 ±  0.077  us/op
ChecksumBenchmark.plainJava           4  avgt    4   0.005 ±  0.001  us/op
ChecksumBenchmark.plainJava        8096  avgt    4   2.109 ±  0.035  us/op
ChecksumBenchmark.plainJava      129536  avgt    4  33.503 ±  0.227  us/op&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This version of the code is even faster, but it can&amp;#8217;t achieve the performance of big ugly assembly blob yet because escape analysis is like a big stone on our way. However this code can be maintained easily, and this API is under active development; there are a lot of experiments happening right now. So, you will have fought this ugly blob when these features are released.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Moreover, all that machine snippets and direct Long* vector parameters are really low-level API. Prototypes of high-level API you can find &lt;a href=&quot;http://hg.openjdk.java.net/panama/panama/jdk/file/c5a104d33632/test/panama/vector-api-boxed-variant/src/test/java/com/oracle/vector/BytesLong2Test.java&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://hg.openjdk.java.net/panama/panama/jdk/file/c5a104d33632/test/panama/vector-api-patchable/src/test/java/SnippetTest.java&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I think that&amp;#8217;s a perfect point to end a journey through the jungle of Panama. We have seen enough crazy things. I&amp;#8217;ll be glad to hear any comments from you. You can find all the experiments &lt;a href=&quot;https://github.com/SerCeMan/panama-article&quot;&gt;here&lt;/a&gt; (don&amp;#8217;t forget to build your own JDK before running the benchmarks). I&amp;#8217;ll be glad to hear any comments from you.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Project Panama will bring us great features, but these are likely to arrive much further down the line&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nothing is impossible, even running an inline assembler from Java&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are a lot of features that can be done in Java with Vector API and Machine Code Snippets already, although it is only the beginning of the story.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compiler can optimize your code really well, most probably better than you.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is very important to measure performance while you are doing optimizations. Or else you can make it even worse.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Seeing how your code will work in the future will help you to better understand how it works now.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_thanks_to&quot;&gt;Thanks to&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/kellylittlepage&quot;&gt;@kellylittlepage&lt;/a&gt; for an awesome article about AVX instruction&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/harrigan_shane&quot;&gt;@harrigan_shane&lt;/a&gt; for comments about my writing style&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/iwan0www&quot;&gt;@iwan0www&lt;/a&gt; for comments and suggestions regarding this post&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You for reading it&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_share_this_article&quot;&gt;Share this article&lt;/h3&gt;
&lt;hr&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Just wrote a new blog post about current state of Machine Code Snippets (and little bit of Vector API) in Panama &lt;a href=&quot;https://t.co/Qdqihp2Mrj&quot;&gt;https://t.co/Qdqihp2Mrj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sergey Tselovalnikov (@SerCeMan) &lt;a href=&quot;https://twitter.com/SerCeMan/status/737889841132752896&quot;&gt;June 1, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;hr&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Wed, 01 Jun 2016 00:00:00 +1000
</pubDate>
</item>
<item>
<guid>
http://serce.me/posts/22-06-2015-jnr-fuse/
</guid>
<link>
http://serce.me/posts/22-06-2015-jnr-fuse/
</link>
<title>
JNR-FUSE: library for using FUSE from Java
</title>
<description>
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hi!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this article, I&amp;#8217;ll tell you how to implement userspace file system using Java without a line of kernel space code.
I&amp;#8217;ll also show you how to connect Java and native code without writing C code and save maximum performance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Originally, I posted this article on &lt;a href=&quot;https://habrahabr.ru/post/260801/&quot;&gt;habrahabr&lt;/a&gt; (in Russian).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/jnr-fuse/jackie.jpg&quot; alt=&quot;jackie&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_fuse&quot;&gt;FUSE&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;First of all, it is important to understand what FUSE is.
FUSE - FileSystem in Userspace - helps users without any privileges to create their file system without a necessity to write code in a kernel space.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is possible because the filesystem code runs in userspace. And the FUSE module is just a bridge between the kernel API and your code. FUSE was officially included in the Linux source tree in 2.6.14.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/jnr-fuse/bridge.png&quot; alt=&quot;bridge&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, you can easily create your own filesystem (&lt;a href=&quot;https://github.com/libfuse/libfuse/blob/master/example/hello.c&quot;&gt;here is the simplest example&lt;/a&gt;). There are a lot of areas where you can use it. For example, you can quickly write a FS where Github or DropBox would be the backend.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Or, let&amp;#8217;s imagine, you have a business application where all user files are stored in a database. But your client wants to have access to them from a filesystem on the server. Of course to duplicate files in the filesystem and the database is the wrong decision. And here FUSE comes in; you just need a little FUSE program which handles all user requests to the directory and redirects them to the database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_java_and_native_code&quot;&gt;Java and native code&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So far so good. But implementation of FUSE starts from &quot;include header &amp;lt;fuse.h&amp;gt;&quot;. But your business application is written in Java. Obviously, it is necessary to communicate with the native code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_jni&quot;&gt;JNI&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The standard tool for a native communication in Java is JNI. But it brings a lot of complexity. Especially because for using FUSE we need a lot of callbacks from the native code to Java. And &quot;write once&quot; is suffering in this case (However, in the case of FUSE it is not so important).
If you try to find projects that implement a FUSE wrapper using JNI you will find a few projects, but they have been obsolete for a long time. And their API is very inconvenient.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_jna&quot;&gt;JNA&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another option is &lt;a href=&quot;https://github.com/java-native-access/jna&quot;&gt;JNA library&lt;/a&gt;.  JNA (Java Native Access) gives you the possibility to get access to the native code easily without using JNI only by using Java code. It is very easy; you just need to declare an interface which matches the native code and get an implementation through &quot;Native.loadLibrary&quot;. And that&amp;#8217;s all.
Another advantage of JNA is very detailed documentation. The project is alive, and it is in active development.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Moreover, a good project implementing FUSE using JNA already exists.
But JNA has a lot of problems with performance. JNA is reflection based, so jumping from native code to Java code with data conversion is very expensive. It is not so important if native calls are rare. But a FS has a lot of native calls. The only way to speed up fuse-jna is to read files using big chunks, but it doesn&amp;#8217;t always work. For example, when you have no access to a client&amp;#8217;s code or when all files are small.
Obviously, we need a library that combines JNI performance and JNA convenience.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_jnr&quot;&gt;JNR&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And here is where JNR (Java Native Runtime) comes in. JNR, like JNA, is based on libffi library, but it uses a bytecode generation instead of reflection. And as a result, JNR achieves excellent performance.
The information about JNR is very limited. The most detailed piece of information is &lt;a href=&quot;http://medianetwork.oracle.com/video/player/2630340184001&quot;&gt;Charles Nutter&amp;#8217;s talk on JVMLS 2013&lt;/a&gt;. But despite the lack of information, JNR is already a big ecosystem actively used by JRuby. A lot of jnr-based libraries such as unix-sockets and posix-api are actively used by different projects.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/jnr-fuse/jnr.png&quot; alt=&quot;jnr&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;JNR is a library which became a basis for the development of &lt;a href=&quot;http://openjdk.java.net/jeps/191&quot;&gt;JEP 191 - Foreign Function Interface&lt;/a&gt;, which is targeted for Java 10.
In comparison to JNA, JNR has no proper documentation, and you need to look for answers in the source code. That is the main reason for this mini-guide.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_specialties_of_writing_code_using_java_native_runtime&quot;&gt;Specialties of writing code using Java Native Runtime&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_function_binding&quot;&gt;Function binding&lt;/h3&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;The simplest binding&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;import jnr.ffi.*;
import jnr.ffi.types.pid_t;

/**
 * Gets the process ID of the current process, and that of its parent.
 */
public class Getpid {
    public interface LibC  {
        public @pid_t long getpid();
        public @pid_t long getppid();
    }

    public static void main(String[] args) {
        LibC libc = LibraryLoader.create(LibC.class).load(&quot;c&quot;);

        System.out.println(&quot;pid=&quot; + libc.getpid() + &quot; parent pid=&quot; + libc.getppid());
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here we are the loading java library which matches the native interface by name.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the case of FUSE, we need an interface with method fuse_main_real where FuseOperations structure with all callbacks passes as an argument.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public interface LibFuse {
    int fuse_main_real(int argc, String argv[], FuseOperations op, int op_size, Pointer user_data);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_implementation_of_structures&quot;&gt;Implementation of structures&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Often you need to work with structure which is located at a certain address, for example with fuse_bufvec structure:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;struct fuse_bufvec {
    size_t count;
    size_t idx;
    size_t off;
    struct fuse_buf buf[1];
};&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For its implementation, we need to make a successor of jnr.ffi.Struct.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;import jnr.ffi.*;

public class FuseBufvec extends Struct {
    public FuseBufvec(jnr.ffi.Runtime runtime) {
        super(runtime);
    }
    public final size_t count = new size_t();
    public final size_t idx = new size_t();
    public final size_t off = new size_t();
    public final FuseBuf buf = inner(new FuseBuf(getRuntime()));
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After that, you have to set proper callback implementation into the getattr field.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;fuseOperations.getattr.set((path, stbuf) -&amp;gt; 0);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_enum&quot;&gt;Enum&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Enum implementation is not so obvious as other parts of the library.
You need to inherit your enum from jnr.ffi.util.EnumMapper.IntegerEnum and implement method intValue&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;enum fuse_buf_flags {
    FUSE_BUF_IS_FD    = (1 &amp;lt;&amp;lt; 1),
    FUSE_BUF_FD_SEEK    = (1 &amp;lt;&amp;lt; 2),
    FUSE_BUF_FD_RETRY    = (1 &amp;lt;&amp;lt; 3),
};&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public enum FuseBufFlags implements EnumMapper.IntegerEnum {
    FUSE_BUF_IS_FD(1 &amp;lt;&amp;lt; 1),
    FUSE_BUF_FD_SEEK(1 &amp;lt;&amp;lt; 2),
    FUSE_BUF_FD_RETRY(1 &amp;lt;&amp;lt; 3);

    private final int value;

    FuseBufFlags(int value) {
        this.value = value;
    }

    @Override
    public int intValue() {
        return value;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_work_with_memory&quot;&gt;Work with memory&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For working with direct memory wrapper jnr.ffi.Pointer exists.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can allocate memory using jnr.ffi.Memory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The entry point of JNR API learning is jnr.ffi.Runtime&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This knowledge is enough to implement a simple cross-platform wrapper for some native library.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_jnr_fuse&quot;&gt;JNR-FUSE&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What I&amp;#8217;ve implemented is a FUSE wrapper in my project jnr-fuse. Previously I used fuse-jna, but it was a bottleneck in the FS implementation. During the development, I tried to save compatibility with the native interface (&amp;lt;fuse.h&amp;gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For implementing your own file system, you need to extend ru.serce.jnrfuse.FuseStubFS and implement several methods.
Fuse_operations contain &lt;a href=&quot;http://fuse.sourcearchive.com/documentation/2.8.4-1.4ubuntu1/structfuse__operations.html&quot;&gt;a lot of methods&lt;/a&gt;, but for getting your FS up and running, you just need to implement several methods.
It is very easy, &lt;a href=&quot;https://github.com/SerCeMan/jnr-fuse/tree/master/src/main/java/ru/serce/jnrfuse/examples&quot;&gt;here are some examples&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Currently, only Linux is supported (x86 and x64).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Library exists in JCenter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Gradle&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-groovy&quot; data-lang=&quot;groovy&quot;&gt;repositories {
    jcenter()
}

dependencies {
    compile 'com.github.serceman:jnr-fuse:0.1'
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Maven&lt;/div&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;    &amp;lt;repositories&amp;gt;
        &amp;lt;repository&amp;gt;
            &amp;lt;id&amp;gt;central&amp;lt;/id&amp;gt;
            &amp;lt;name&amp;gt;bintray&amp;lt;/name&amp;gt;
            &amp;lt;url&amp;gt;http://jcenter.bintray.com&amp;lt;/url&amp;gt;
        &amp;lt;/repository&amp;gt;
    &amp;lt;/repositories&amp;gt;

    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;com.github.serceman&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;jnr-fuse&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;0.1&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_jnr_fuse_and_fuse_jna_performance_comparison&quot;&gt;JNR-FUSE and FUSE-JNA performance comparison&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In my case the FS was read-only, and I was interested in throughput.
The performance will mostly depend on your FS implementation, so if you already use fuse-jna, you&amp;#8217;ll be able to change it easily to jnr-fuse, write a test, and to see the performance difference on your workload. (It will be helpful anyway because we all love to achieve new levels of performance, right?)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to show performance the difference, I moved MemoryFS implementation from fuse-jna to jnr-fuse with minimal changes and ran a reading test. For the test, I used &lt;a href=&quot;http://freecode.com/projects/fio&quot;&gt;fio&lt;/a&gt; framework.&lt;/p&gt;
&lt;/div&gt;
&lt;details&gt;
  &lt;summary&gt;&lt;span style=&quot;color: #527bbd&quot;&gt;Test configuration&lt;/span&gt;&lt;/summary&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-properties&quot; data-lang=&quot;properties&quot;&gt;[readtest]
blocksize=4k
directory=/tmp/mnt/
rw=randread
direct=1
buffered=0
ioengine=libaio
time_based=60
size=16M
runtime=60&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;details&gt;
  &lt;summary&gt;&lt;span style=&quot;color: #527bbd&quot;&gt;The result of of fuse-jna&lt;/span&gt;&lt;/summary&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-none&quot; data-lang=&quot;none&quot;&gt;serce@SerCe-FastLinux:~/git/jnr-fuse/bench$ fio read.ini
readtest: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1
fio-2.1.3
Starting 1 process
readtest: Laying out IO file(s) (1 file(s) / 16MB)
Jobs: 1 (f=1): [r] [100.0% done] [24492KB/0KB/0KB /s] [6123/0/0 iops] [eta 00m:00s]
readtest: (groupid=0, jobs=1): err= 0: pid=10442: Sun Jun 21 14:49:13 2015
read: io=1580.2MB, bw=26967KB/s, iops=6741, runt= 60000msec
slat (usec): min=46, max=29997, avg=146.55, stdev=327.68
clat (usec): min=0, max=69, avg= 0.47, stdev= 0.66
lat (usec): min=47, max=30002, avg=147.26, stdev=327.88
clat percentiles (usec):
| 1.00th=[ 0], 5.00th=[ 0], 10.00th=[ 0], 20.00th=[ 0],
| 30.00th=[ 0], 40.00th=[ 0], 50.00th=[ 0], 60.00th=[ 1],
| 70.00th=[ 1], 80.00th=[ 1], 90.00th=[ 1], 95.00th=[ 1],
| 99.00th=[ 2], 99.50th=[ 2], 99.90th=[ 3], 99.95th=[ 12],
| 99.99th=[ 14]
bw (KB /s): min=17680, max=32606, per=96.09%, avg=25913.26, stdev=3156.20
lat (usec): 2=97.95%, 4=1.96%, 10=0.02%, 20=0.06%, 50=0.01%
lat (usec): 100=0.01%
cpu: usr=1.98%, sys=5.94%, ctx=405302, majf=0, minf=28
IO depths: 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &amp;gt;=64=0.0%
submit: 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
complete: 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
issued: total=r=404511/w=0/d=0, short=r=0/w=0/d=0

Run status group 0 (all jobs):
READ: io=1580.2MB, aggrb=26967KB/s, minb=26967KB/s, maxb=26967KB/s, mint=60000msec, maxt=60000msec&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;details&gt;
  &lt;summary&gt;&lt;span style=&quot;color: #527bbd&quot;&gt;The result of jnr-fuse&lt;/span&gt;&lt;/summary&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-properties&quot; data-lang=&quot;properties&quot;&gt;serce@SerCe-FastLinux:~/git/jnr-fuse/bench$ fio read.ini
readtest: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1
fio-2.1.3
Starting 1 process
readtest: Laying out IO file(s) (1 file(s) / 16MB)
Jobs: 1 (f=1): [r] [100.0% done] [208.5MB/0KB/0KB /s] [53.4K/0/0 iops] [eta 00m:00s]
readtest: (groupid=0, jobs=1): err= 0: pid=10153: Sun Jun 21 14:45:17 2015
read: io=13826MB, bw=235955KB/s, iops=58988, runt= 60002msec
slat (usec): min=6, max=23671, avg=15.80, stdev=19.97
clat (usec): min=0, max=1028, avg= 0.37, stdev= 0.78
lat (usec): min=7, max=23688, avg=16.29, stdev=20.03
clat percentiles (usec):
| 1.00th=[ 0], 5.00th=[ 0], 10.00th=[ 0], 20.00th=[ 0],
| 30.00th=[ 0], 40.00th=[ 0], 50.00th=[ 0], 60.00th=[ 0],
| 70.00th=[ 1], 80.00th=[ 1], 90.00th=[ 1], 95.00th=[ 1],
| 99.00th=[ 1], 99.50th=[ 1], 99.90th=[ 2], 99.95th=[ 2],
| 99.99th=[ 10]
lat (usec): 2=99.88%, 4=0.10%, 10=0.01%, 20=0.01%, 50=0.01%
lat (usec): 100=0.01%, 250=0.01%
lat (msec): 2=0.01%
cpu: usr=9.33%, sys=34.01%, ctx=3543137, majf=0, minf=28
IO depths: 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &amp;gt;=64=0.0%
submit: 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
complete: 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%
issued: total=r=3539449/w=0/d=0, short=r=0/w=0/d=0

Run status group 0 (all jobs):
READ: io=13826MB, aggrb=235955KB/s, minb=235955KB/s, maxb=235955KB/s, mint=60002msec, maxt=60002msec&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/details&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/img/jnr-fuse/table.png&quot; alt=&quot;table&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The only information this test shows us is the difference in reading a file using fuse-jna and jnr-fuse, but it gives us an understanding of the level of performance difference of JNA and JNR.
If you are interested, you can write a more detailed test for native calls using the &lt;a href=&quot;http://openjdk.java.net/projects/code-tools/jmh/&quot;&gt;JMH&lt;/a&gt; tool.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The performance differences in throughput and latency are about ~10 times. Charles Nutter in his presentation gave us the same numbers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_links&quot;&gt;Links&lt;/h3&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/libfuse/libfuse&quot;&gt;FUSE on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/jnr&quot;&gt;JNR on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://www.oracle.com/technetwork/java/jvmls2013nutter-2013526.pdf&quot;&gt;Charles Nutter presentation about JNR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;http://openjdk.java.net/jeps/191&quot;&gt;JEP 191&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/SerCeMan/jnr-fuse/blob/master/src/main/java/ru/serce/jnrfuse/examples/HelloFuse.java&quot;&gt;Java HelloFuse&lt;/a&gt;/&lt;a href=&quot;https://github.com/libfuse/libfuse/blob/master/example/hello.c&quot;&gt;C HelloFuse&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/SerCeMan/jnr-fuse&quot;&gt;jnr-fuse&lt;/a&gt; project is located on GitHub. I&amp;#8217;ll appreciate stars, pull-requests, and comments.
I&amp;#8217;ll be glad to answer any questions you have about JNR or jnr-fuse.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<enclosure>

</enclosure>
<pubDate>
Mon, 22 Jun 2015 00:00:00 +1000
</pubDate>
</item>
</channel>
</rss>
