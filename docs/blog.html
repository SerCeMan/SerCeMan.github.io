<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>SerCe&#x27;s blog</title><meta content="SerCe&#x27;s blog: Here is my personal blog where I share my thoughts and experiences with different technologies." name="description"/><meta property="og:url" content="https://serce.me/blog"/><link rel="canonical" href="https://serce.me/blog"/><meta property="og:type" content="website"/><meta property="og:site_name" content="SerCe&#x27;s blog"/><meta property="og:description" content="SerCe&#x27;s blog: Here is my personal blog where I share my thoughts and experiences with different technologies."/><meta property="og:title" content="SerCe&#x27;s blog"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@SerCeMan"/><meta name="twitter:title" content="SerCe&#x27;s blog"/><meta name="twitter:description" content="SerCe&#x27;s blog: Here is my personal blog where I share my thoughts and experiences with different technologies."/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/59694621ebf097fb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/59694621ebf097fb.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6d771e94e21114f2.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6d771e94e21114f2.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-114634acb84f8baa.js" defer=""></script><script src="/_next/static/chunks/main-70187573dbb2adba.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9d9bd6946354a189.js" defer=""></script><script src="/_next/static/chunks/c16184b3-09e9b1493522fa97.js" defer=""></script><script src="/_next/static/chunks/105-7a67388502189cf8.js" defer=""></script><script src="/_next/static/chunks/603-a0d7b083540e3524.js" defer=""></script><script src="/_next/static/chunks/pages/blog-14489d28f7acdb42.js" defer=""></script><script src="/_next/static/Pcyft9E01JztMB997G3j_/_buildManifest.js" defer=""></script><script src="/_next/static/Pcyft9E01JztMB997G3j_/_ssgManifest.js" defer=""></script></head><body style="background-color:#f2f2f2" class="bg-white text-gray-900"><div id="__next"><script>!function(){try{var d=document.documentElement,c=d.classList;c.remove('light','dark');var e=localStorage.getItem('theme');if(e){c.add(e|| '')}else{c.add('light');}if(e==='light'||e==='dark'||!e)d.style.colorScheme=e||'light'}catch(t){}}();</script><header style="background-color:#f8f8f8" class="relative shadow-md"><div class="max-w-5xl px-8 py-1 mx-auto"><div class="flex items-center justify-between"><nav class="flex justify-between items-center w-full text-lg"><a style="font-family:Alegreya" class="text-gray-500 pr-6 py-4 float-left w-1/2 text-4xl" href="/">SerCe&#x27;s blog</a><div class="float-right"><a class="text-gray-600 px-6 py-4" href="/">Home</a><a class="text-gray-600 px-6 py-4" href="/blog">Blog</a><a class="text-gray-600 px-6 py-4" href="/talks">Talks</a></div></nav></div></div></header><main><div><div><div class="container max-w-4xl mx-auto px-12 py-6 text-base"><h1 style="font-family:Alegreya" class="text-2xl font-normal">Blog Posts</h1><div class="pl-4"><ul class="list-disc"><li><div><h4 style="font-family:Alegreya" class="text-lg py-1">14 October 2021</h4><a href="/posts/14-10-2021-the-five-lies-analysis">The Five Lies Analysis</a></div></li><li><div><h4 style="font-family:Alegreya" class="text-lg py-1">27 May 2021</h4><a href="https://canvatechblog.com/enabling-real-time-collaboration-with-rsocket-92416fe52650"><span class="flex flex-row items-center">Enabling real-time collaboration with RSocket<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="arrow-up-right-from-square" class="svg-inline--fa fa-arrow-up-right-from-square h-4 pl-1" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M320 0c-17.7 0-32 14.3-32 32s14.3 32 32 32h82.7L201.4 265.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L448 109.3V192c0 17.7 14.3 32 32 32s32-14.3 32-32V32c0-17.7-14.3-32-32-32H320zM80 32C35.8 32 0 67.8 0 112V432c0 44.2 35.8 80 80 80H400c44.2 0 80-35.8 80-80V320c0-17.7-14.3-32-32-32s-32 14.3-32 32V432c0 8.8-7.2 16-16 16H80c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16H192c17.7 0 32-14.3 32-32s-14.3-32-32-32H80z"></path></svg></span></a></div></li><li><div><h4 style="font-family:Alegreya" class="text-lg py-1">18 November 2020</h4><a href="/posts/18-11-2020-allocate-direct">Indirect Effects of Allocate Direct</a></div></li><li><div><h4 style="font-family:Alegreya" class="text-lg py-1">23 July 2020</h4><a href="/posts/23-07-2020-you-dont-need-no-service-mesh">You don&#x27;t need no Service Mesh</a></div></li><li><div><h4 style="font-family:Alegreya" class="text-lg py-1">16 May 2019</h4><a href="/posts/16-05-2019-the-matter-of-time">The matter of time()</a></div></li><li><div><h4 style="font-family:Alegreya" class="text-lg py-1">29 June 2017</h4><a href="/posts/29-06-2017-fantastic-dsls">Fantastic DSLs and where to find them</a></div></li><li><div><h4 style="font-family:Alegreya" class="text-lg py-1">01 June 2016</h4><a href="/posts/01-06-2016-wild-panama">Pure assembly in the forest of Panama</a></div></li><li><div><h4 style="font-family:Alegreya" class="text-lg py-1">22 June 2015</h4><a href="/posts/22-06-2015-jnr-fuse">JNR-FUSE library for using FUSE from Java</a></div></li></ul></div></div></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"The Five Lies Analysis","date":"2021-10-14","content":"\nimport Quote from \"../components/Quote\";\nimport Image from \"next/image\";\n\n\nThe Five Whys analysis is a popular root cause investigation technique\nwith a simple premise that is asking why five times can yield the\nanswer, which is the root cause. While doing so could be a helpful\nexercise, blindly applying the technique often leads to a suboptimal\nresult. In this article, I’ll explore some of its tradeoffs with a\nfictional story of an incident in production at\n[Acme Corp](https://en.wikipedia.org/wiki/Acme_Corporation).\n\n![](/images/five-lies/crew.png)\n\n\u003cQuote\n    quote=\"I’m sorry. The words made sense, but the sarcastic tone did not.\"\n    attribution=\"Lana Kane\"\n/\u003e\n\n## Meet the crew\n\nLet’s start by setting the scene. A team of engineers is working on a\nnew Acme News website. Suddenly, an incident occurs, which gets resolved\nquickly. A lengthy investigation then follows the incident to understand\nwhat happened and how similar issues can be prevented in the future.\n\nThe standard process of achieving this would be to write a postmortem,\nwhich can outline the story and propose a set of action items. A\npostmortem template can simplify the process by including all necessary\nsteps to identify the root cause and might suggest using an analysis,\nfor example, the five whys analysis. The question of who is ultimately\nresponsible for authoring the postmortem is tricky, and the answer\ndepends on many factors. Let’s take a look at what everyone on the team\nwould write if they were the author.\n\n### Meet Alice\n\n\u003cImage src=\"/images/five-lies/alice.png\" alt=\"Alice\" height=\"300\" width=\"300\"/\u003e\n\nAs we know, Alice is an on-call engineer. Her official title is SRE, and\nher job involves a deep understanding of the service state based on the\navailable metrics. She has a solid understanding of service health\nindicators and a good grasp of the monitoring infrastructure. When she\nstarted the investigation, she noticed that the application was\noverloaded due to high CPU usage. As recommended by the postmortem\ntemplate, she began the five whys analysis.\n\nFive Whys analysis:\n\n- Why were the users not able to open the website? Because the\napplication was overloaded, and it started returning errors.\n\n- Why was it overloaded? Because the CPU went to 100%, and the instances\ndidn’t have enough capacity to process the requests.\n\n- Why did the CPU usage jump so high? Because we didn’t notice it\nearlier when it climbed to 90% and didn’t fix the issue in time.\n\n- Why did we not notice it? Because there were no alerts for 90% CPU\nusage.\n\n- Why were there no alerts? Because the alerts were removed two years\nago after a migration to a new monitoring system.\n\nAs the Five Whys analysis was completed, the root cause was\n\"identified\". The main action item in the postmortem was to\nre-instantiate high utilisation - 90% CPU alerts to catch similar\nsituations earlier and mitigate the issue before users are effect. Now\nthat all of the questions are answered, and the lessons are learned,\nAlice is off to another incident.\n\n### Meet Bob\n\n\u003cImage src=\"/images/five-lies/bob.png\" alt=\"Bob\" height=\"300\" width=\"300\"/\u003e\n\nBob is the backend engineer who implemented the feature in the first\nplace and flipped the flag. He has the most context around the change,\nso it would make sense for Bob to write a postmortem. Bob can use the\nFive Whys analysis to complete the postmortem and understand how to roll\nout all future changes smoother.\n\nFive Whys analysis:\n\n- Why were the users not able to open the website? Because the\napplication was overloaded.\n\n- Why was it overloaded? Because CPU jumped to 100%.\n\n- Why did it jump? Because the application went into an infinite GC\nloop.\n\n- Why did GC start consuming all of the CPU? Because the application ran\nout of heap memory.\n\n- Why did it run out of heap memory? Because a new request-level cache\nwas added with unlimited size.\n\nThe new feature that Bob implemented enabled users to see article\nrecommendations on the Acme News website. To reduce the load to the\nrecommendation service, Bob added a cache. However, Bob forgot to add a\nmaximum size parameter. Bob then started thinking about how this could\nbe prevented in the future. He decided to change the Cache interface to\nensure that max size always has to be provided. This task was added as\nthe main action item to the postmortem.\n\n### Meet Charlie\n\n\n\u003cImage src=\"/images/five-lies/charlie.png\" alt=\"Charlie\" height=\"300\" width=\"300\"/\u003e\n\nCharlie is a frontend engineer who implemented the feature on the\nfrontend side and is responsible for Acme Corp News overall look.\nCharlie is also a product owner of the new Acme Corp Recommendations ™,\nwho cares a lot about the product’s availability, so it might make sense\nfor her to start a postmortem.\n\nFive Whys analysis:\n\n- Why were the users not able to open the website? Because the\napplication was overloaded.\n\n- Why was it overloaded? Because there was an influx of requests that\ncaused high CPU usage.\n\n- Why was there an influx? Because the users started sending a lot of\nrequests.\n\n- Why did they do this? Because each request was retried up to 10 times.\n\n- Why were there so many retries at the same time? Because there was no\nexponential backoff.\n\nCharlie’s mental model of the application is primarily based around the\nrequest flows. When looking at the application logs, she noticed many\nretries from the same set of users. When looking at the code that\nfetches the recommendations, she saw that all of the requests were\nretried immediately without any exponential backoff or jitter. Hence,\nshe put it as an action item with the plan to implement it by Friday.\n\n### Meet Dave\n\n\u003cImage src=\"/images/five-lies/dave.png\" alt=\"Dave\" height=\"300\" width=\"300\"/\u003e\n\nDave is an engineer on the cloud infrastructure team, and he is\nresponsible for the overall health of the Acme News Corp cluster. Dave\ncares a lot about reliability, so he might volunteer to perform the\ninvestigation and complete the postmortem.\n\nFive Whys analysis:\n\n- Why were the users not able to open the website? Because the\napplication was overloaded.\n- Why was it overloaded? Because the load on every instance increased\nsharply.\n- Why was the load per instance so high? Because we didn’t have enough\ninstance capacity to distribute the load.\n- Why not? Because our autoscaling policies didn’t scale fast enough and\nonly added a few more instances.\n- Why? Because the autoscaling cooldown time was too large.\n\nFor Dave, each task in the cluster is essentially a black box that can\nhandle a certain number of requests. For each type of task, Dave manages\npolicies that define how the applications can scale up or down. After\nlooking at the graphs, Dave notices that even though a few instances\njoined the fleet when the load increased sharply, the cooldown prevented\nautoscaling from bringing up even more tasks to handle the load. Dave\nwrites down a task to update autoscaling policies to allow for very\naggressive scale-ups to handle the load as the main action item.\n\n### Meet Erin\n\n\u003cImage src=\"/images/five-lies/erin.png\" alt=\"Erin\" height=\"300\" width=\"300\"/\u003e\n\nErin is an engineer on the core libraries team, and she is working on a\nshared set of core libraries. When she first heard about the incident\nand that it was related to the cache library, she volunteered to write a\npostmortem as the owner who doesn’t shy away from responsibility.\n\nFive Whys analysis:\n\n- Why were the users not able to open the website? Because the\napplication was overloaded.\n- Why was it overloaded? Because it was out of memory\n- Why was the application out of memory? Because the cache took too much\nmemory.\n- Why did the cache take up that much memory? Because our cache\nlibraries are not optimised for memory usage.\n- Why is the cache library not optimised for memory usage? Because we\nalways valued throughput above memory usage.\n\nErin wrote the original cache library. The library has been a great\nsuccess as it is incredibly optimised for Acme Corp’s use cases.\nNonetheless, so far, the focus was the throughput and not the memory\nusage. After looking at the library from a different perspective, Erin\nnoticed a few opportunities to share the underlying data structures and\nwrote down an action item to provide a version of the cache explicitly\noptimised for memory usage.\n\n## The analysis\n\nLet’s zoom out and take a look at the results of the exercise. There are\nfive people and five different stories. The key takeaway here is the\nresults of the why the Five Whys analysis are not repeatable. The\noutcome heavily depends on the angle at which the person performing the\nanalysis looks at the incident. Dave’s point of view on what happened is\nvery different to Charlie’s point of view as they work on very different\nparts of the system day-to-day, which heavily influences the analysis.\n\nBut even looking at a single investigation, we can notice that the\nnumber “five” itself is interesting. It’s catchy, it’s easy to remember,\nbut it unnecessarily limits the depth of the investigation. Maybe Alice\nshould’ve done some code archeology to figure out why the alerts were\nremoved after the migration. Bob could have dug deeper to collect some\nof the heap dumps to understand the distribution of data in the cache.\nWhy stop at five if it makes sense to go further?\n\n## The root cause\n\nAs we saw, the Five Whys analysis has downsides like any other approach.\nYet, it can be used successfully to dive deeper! So are these issues big\nenough to invalidate the approach?\n\nThe real problem reveals itself when the technique becomes a part of a\ntemplate. Once it’s in the template, the shape of the analysis becomes\nsolidified. Whether it’s a part of an engineering [postmortem\ntemplate](https://www.atlassian.com/incident-management/postmortem/templates)\nor even a [government\nworksheet](https://www.justice.act.gov.au/sites/default/files/2019-08/Root_Cause_Analysis_Template.pdf) -\nthe template restricts the analysis by forcing its limits onto the\ninvestigator. The downsides of the analysis become the downsides of the\npostmortem.\n\nWell, if not using five whys, then how would we find the root cause?\nFirst, it’s important to understand that rarely there is such thing as a\nroot cause. Often there would be multiple causes that can be seen when\nlooking at the incident from different angles.\n\nSecond, and more importantly, it’s not about finding the root cause at\nall. It’s about getting an inside out understanding of what happened —\nand then, based on this understanding, defining what actions can be\ntaken to ensure that the incident doesn’t repeat in the future. Often,\nthese action items can be very distant from the root cause. For\ninstance, maybe Acme News could restrict the blast radius and ensure\nthat the page loads even if recommendations are not available. These\nmitigations might not always come up when searching for a root cause.\nThis poses a question - if not the Five Whys analysis, then what should\nbe in the template? It can’t be empty after all!\n\nBefore looking for a replacement, it’s important to understand that the\ntemplate should encourage building an understanding and a detailed story\nrather than searching for a root cause. In [The Infinite\nHows](https://www.kitchensoap.com/2014/11/14/the-infinite-hows-or-the-dangers-of-the-five-whys/),\nJohn Allspaw highlights that \"Learning is the goal. Any prevention\ndepends on that learning.\" and proposes using [Debriefing Facilitation\nPrompts](http://www.kitchensoap.com/wp-content/uploads/2014/09/Velocity2014-PM-Fac-Handout-Debrief.pdf)\nfrom *The Field Guide To Understanding Human Error*, by Sidney Dekker.\n\nHaving the prompts could be a great starting point as it asks you to\ntake a look at the system from multiple angles and understand how each\nindividual part of the system behaved during the incident. Moreover, the\napproach could further be expanded with the questions from the\nenvironment - the questions that can help build the story and facilitate\nlearning. There could even be different prompts for different components\nthat were affected. If Alice or Bob, or any other member of the crew\nwere to use the prompts, they would immediately have to consider how the\nsystem behaved from multiple angles giving the investigation the\nnecessary depth.\n\n## Conclusion\n\nFive Whys analysis is a useful technique that is easy to remember and a\nhelpful reminder to always look deeper. However, using it directly would\nonly yield a cause, not the root cause, as rarely there is such thing as\nthe root cause. The approach could still be used to deepen the scope of\nsearching for action items. Still, structuring this tool into a template\nas the primary driver of a root cause analysis can cause more harm than\ngood.\n\nInstead, the template could facilitate learning and help build a story\nby asking questions that can help understand the whole story deeply.\nThen, a comprehensive list of action items can stem from the story\npreventing not only incidents with a similar \"root cause\" but a whole\nclass of failures.\n\n## Thank you to\n\n- [Paul Tune](https://twitter.com/ptuls) for reviewing the article.\n- You for reading the article.\n\n## Discuss on\n\n* [Twitter](https://twitter.com/SerCeMan/status/1448641518370103303)\n\n","description":"Empty","slug":"14-10-2021-the-five-lies-analysis","kind":"mdx"},{"kind":"external","title":"Enabling real-time collaboration with RSocket","date":"2021-05-27","description":"This post describes how we empowered our millions of users at Canva to collaborate at scale by introducing services that support bidirectional streaming using RSocket.","canonicalLink":"https://canvatechblog.com/enabling-real-time-collaboration-with-rsocket-92416fe52650"},{"title":"Indirect Effects of Allocate Direct","date":"2020-11-18","content":"\nimport Quote from \"../components/Quote\";\n\nHi, folks!\n\nEvery single program allocates memory. Byte buffers are at the core of\nmany essential libraries which power the services the modern internet is\nbuilt upon. If you’re building such a library, or even just copying data\nbetween different files, chances are you’ll need to allocate a buffer.\n\nIn Java, `ByteBuffer` is the class that allows you to do so. Once you’ve\ndecided to allocate a buffer, you’ll be presented with two methods\n`allocate()` and `allocateDirect()`. Which one to use? The answer is, as\nalways, it depends. If there were no tradeoffs, there wouldn’t be two\nmethods. In this article, I’ll explore some of these tradeoffs,\ndemystify this behaviour, and I hope that the answer will be clear for\nyou by the end of it.\n\n![](/images/allocatedirect/itsgone.png)\n\n\u003cQuote\n  quote=\"Yeah well, sometimes the things we do don’t matter right now. Sometimes they matter later. We have to care more about later sometimes, you know.\"\n  attribution=\"Stan Marsh\"\n/\u003e\n\n# Two buffers\n\nAt first glance, the two methods `allocate()` and `allocateDirect()` are\nvery simple. The `allocate()` allocates a buffer in the managed heap of\nthe Java process, a part of this exact space which size is specified\nwith the `Xmx` option. The `allocateDirect()` method allocates a buffer\nresiding outside of the managed heap.\n\n![](/images/allocatedirect/server3.png)\n\nThis difference, however, creates a number of significant runtime\nimplications, which I’m going to dive into here. But first, let me start\nby telling a debugging story where direct byte buffers were the\nmurderer.\n\n## The story\n\nEvery story needs a protagonist. In this case, the protagonist was a\nJava application built on top of RSocket, a modern application protocol.\nHere’s the oversimplified version of the app which you can find on\n[Github](https://github.com/SerCeMan/allocatedirect/blob/master/src/main/java/me/serce/allocatedirect/Main.java).\nLet’s call this app an echo app. The echo code isn’t trying to do\nanything complicated, it is a simple echo service built with an awesome\n[rsocket-java](https://github.com/rsocket/rsocket-java) library. All it\ndoes is spins up a client and a server, where the client sends messages,\nand the server echoes them back.\n\n``` java\nvar server = RSocketServer.create(echo) //...\nvar client = RSocketConnector.create() //...\nwhile (true) {\n  assert Objects.equals(client.send(), client.receive())\n}\n```\n\n| ❗️ | The supporting code for this article is available [on Github](https://github.com/SerCeMan/allocatedirect). You can, and I highly encourage you to, choose to go through each step yourself by cloning the code and running each example with a simple bash script. All measurements were taken on an EC2 AWS [m5.large](https://aws.amazon.com/ec2/instance-types/m5/) instance. Unless specified otherwise, *Java 13* is used. The point of this article is not to show the numbers but rather demonstrate the techniques that you can use to debug your own application. |\n|----|---|\n\nThe code is useless if it’s just sitting in the repo and doing nothing,\nso let’s clone the repo and start the app.\n\n``` bash\ngit clone https://github.com/SerCeMan/allocatedirect.git\ncd allocatedirect \u0026\u0026 ./start.sh\n```\n\nThe app starts, and you should see that the logs are flowing. As\nexpected, now it’s processing a large number of messages. However, if\nthe echo app was exposed to users, they would start noticing significant\npauses every now and then. All Java developers know that the first thing\nto look at in the case of spurious pauses is GC.\n\n| ❗️ | You can find GC logs of the app are stored in `/tmp/${gcname}`. The example logs for each run are also available in the [repo](https://github.com/SerCeMan/allocatedirect/tree/master/logs). In this article, gceasy.io was used for visualisation. It’s a great free online tool which supports the log format of multiple garbage collectors. Even though you can always visualise GC logs using a tool like gceasy, as we’ll see later, the raw logs often contain a lot more information than most of the tools can display. |\n|----|---|\n\nIndeed, GC logs show that GC is to blame here. The application is\nrunning under G1, which is the default collector since JDK 9. There are\nmultiple young GC pauses on the graph. Young GC is a stop-the-world\npause in GC in G1. The application stops completely to perform a\ncleanup. For the echo server, the graph shows multiple young GC pauses\nthat last for 100-130ms and occur every 10 seconds.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _G1 GC_\n\n\u003c/div\u003e\n\n![](/images/allocatedirect/g1before.png)\n\nLuckily for us, in the last few years, there has been an amazing\ndevelopment in the GC space. There are not just one but two new fully\nconcurrent garbage collectors,\n[ZGC](https://wiki.openjdk.java.net/display/zgc/Main) and\n[Shenandoah](https://wiki.openjdk.java.net/display/shenandoah/Main).\n\nWhile I’ve had [great\nsuccess](https://twitter.com/SerCeMan/status/1246676501925224449) with\nZGC before, Shenandoah has a great advantage of being much friendlier to\napplication memory consumption. Many applications, especially simple\nJSON in, JSON out stateless services are not memory-constrained. Some\napplication, on the other hand, especially the ones that process a large\nnumber of connections might be very sensitive to memory usage.\n\nEven though the echo app only has a single client and a server in its\ncurrent state, it could as well handle tens of thousands of connections.\nIt’s time to enable Shenandoah, and run the echo app again.\n\n``` bash\n./start.sh shen # starts with -XX:+UseShenandoahGC\n```\n\nAfter enabling Shenandoah, the GC logs start showing an interesting\npicture. There is definitely a huge improvement in the pause frequency.\nThe pauses now only occur every minute or so. However, the pauses are\nstill around 90ms long, which is far away from the desired\nsub-millisecond pauses.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _Shenandoah GC_\n\n\u003c/div\u003e\n\n![](/images/allocatedirect/shenandoah.png)\n\nNow that the symptoms are clear, and the problem is reproducible, it’s\ntime to look at the cause. GC graphs don’t show much more information.\nLooking at the raw logs directly, on the other hand, reveals the cause\nwhich is clearly stated right on the pause line.\n\n```\n...\n[info][gc] GC(15) Pause Final Mark (process weakrefs) 86.167ms\n...\n```\n\nTurns out, weak references are to blame. Put simply, weak references are\na way to keep an object in memory until there is a demand for this\nmemory. Large in-memory caches is a common use-case for weak references.\nIf there is enough free heap, a weak reference cache entry can stay\nthere. As soon as GC figures out that there is not enough memory, it’ll\ndeallocate weak references. In most of the cases, this is a much better\noutcome than the application failing with an out of memory exception\nbecause of a cache.\n\nA frantic search across the repository doesn’t show any usages of weak,\nsoft or phantom references. Not even the search through the third party\nlibraries can show anything. After staring at the metrics for a while,\none of the graphs gives a clue! The long GC pauses correlate with a\nsudden drop in the number of direct byte buffers.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _GC vs DirectByteBuffer count_\n\n\u003c/div\u003e\n\n![](/images/allocatedirect/jmc-gc.png)\n\n| ❗️ | You can get a similar graph by running the echo app and connecting to the JMX port. For this screenshot, I used Java Mission Control (JMC). The [start.sh](https://github.com/SerCeMan/allocatedirect/blob/master/start.sh#L53) script contains the options that you can enable to connect to an app with JMX remotely. |\n|----|---|\n\nAt first, the correlation might not make any sense. Byte buffers are not\nweak references, are they? They are not weak references themselves.\nHowever, you might notice, that creating a new direct byte buffer gives\nyou back a plain `ByteBuffer` interface which doesn’t have a `close`\nmethod or any other way of deallocating the buffer.\n\n``` java\nByteBuffer buf = ByteBuffer.allocateDirect(42);\n```\n\nThe underlying buffer needs to go away once the last reference to this\nbuffer goes away. The modern API for this in Java is\n[`java.lang.ref.Cleaner`](https://docs.oracle.com/en/java/javase/13/docs/api/java.base/java/lang/ref/Cleaner.html).\nAs we can see, it’s exactly what `DirectByteBuffer` class uses to\ndetermine when the underlying buffer should be deallocated.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[DirectByteBuffer](https://github.com/openjdk/jdk13/blob/dcd4014cd8a6f49a564cbb95387ad01a80a20bed/src/java.base/share/classes/java/nio/Direct-X-Buffer.java.template#L113-L141)\n    constructor_\n\n\u003c/div\u003e\n\n``` java\nDirectByteBuffer(int cap) {\n    // ...\n    base = UNSAFE.allocateMemory(size); // malloc call\n    UNSAFE.setMemory(base, size, (byte) 0);\n    // ...\n    cleaner = Cleaner.create(this, new Deallocator(base, size, cap));\n}\n```\n\nYet, there are no usages of direct buffers in the code of the echo app\neither, so how could we find them? One way would be to search through\nthe third party libraries using IntelliJ. The approach would work very\nwell for the echo example but would completely fail for any real\napplications of a decent size. There are just way too many places where\nbyte buffers are used. Looking at the graphs, one can notice that the\nnumber of created buffers per minute is huge, literally millions of\nthem.\n\nInstead of searching through the code to find all byte buffer\nreferences, it is easier to find the place at runtime. One way to find\nout where the majority of the buffers is created is to fire up the async\nprofiler and profile the\n[`malloc`](https://man7.org/linux/man-pages/man3/malloc.3.html) calls\nwhich are used by direct buffers to allocate memory.\n\n``` bash\n# async profiler can be downloaded from https://github.com/jvm-profiling-tools/async-profiler\n./profiler.sh -d 30 -f /tmp/flamegraph.svg $(pgrep -f java) -e malloc\n```\n\nWhile running, the profiler managed to sample more than 500000 malloc\ncalls which non-ambiguously show where all of the buffers were created\nfrom.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _malloc calls_\n\n\u003c/div\u003e\n\n\u003cobject type=\"image/svg+xml\" data=\"/images/allocatedirect/mallocflame.svg\"/\u003e\n\n| ❗️ | The flame graph above visualises the code paths where most of the captured malloc calls occur. The wider the column is, the larger the number of times the code path appeared in the sample. This graph, as well as other flame graphs in this article, is clickable. You can read more on how to read flame graphs [here](http://www.brendangregg.com/flamegraphs.html). |\n|----|---|\n\nAs it turned out, there was a place in the code which was using direct\nbuffers. With this rich knowledge of where exactly the direct buffer\nallocations occur, creating a fix is easy. All that’s needed is to make\na one line change and to replace `allocateDirect` with `allocate` and\nsend a [PR upstream](https://github.com/rsocket/rsocket-java/pull/945).\n\nRunning the same app on shenandoah after applying the single line change\nproduces a completely different graph which pleases the eyes with\nsub-millisecond GC pauses.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _Shenandoah GC_\n\n\u003c/div\u003e\n\n![](/images/allocatedirect/shenandoah-heap.png)\n\n## The costs\n\nThe story revealed a dark side of direct byte buffers. If there is a\ndark side, there must be a bright side as well! There is. But before we\nlook at the bright side, we need to explore a few more sides which also\nappeared to be grey.\n\n### Allocations\n\nPreviously, we’ve observed implicit deallocations costs, so now it’s\ntime to take a look at allocations. Could direct buffers be much cheaper\nto create? After all, going off-heap has been a performance trend for a\nwhile. A small benchmark can help to estimate the costs.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[AllocationBenchmark.java](https://github.com/SerCeMan/allocatedirect/blob/master/bench/src/main/java/me/serce/AllocateBuffer1.java)_\n\n\u003c/div\u003e\n\n``` java\n@Param({\"128\", \"1024\", \"16384\"})\nint size;\n\n@Benchmark\npublic ByteBuffer heap() {\n  return ByteBuffer.allocate(size);\n}\n\n@Benchmark\npublic ByteBuffer direct() {\n  return ByteBuffer.allocateDirect(size);\n}\n```\n\nAfter cloning the repo, you can run the benchmark yourself with the\ncommand below.\n\n``` bash\n# Don't just read! Clone the repo and try yourself! 🤓\n./bench.sh alloc1\n```\n\nThe absolute numbers are not that interesting. Even the slowest\noperation only takes a few microseconds. But the difference between the\nheap buffers and direct buffers is fascinating.\n\n``` bash\nBenchmark               (size)  Mode  Cnt     Score     Error  Units\nAllocateBuffer1.direct     128  avgt    5  1022.137 ± 148.510  ns/op\nAllocateBuffer1.heap       128  avgt    5    23.969 ±   0.051  ns/op\n\nAllocateBuffer1.direct    1024  avgt    5  1228.785 ± 127.090  ns/op\nAllocateBuffer1.heap      1024  avgt    5   179.350 ±   2.989  ns/op\n\nAllocateBuffer1.direct   16384  avgt    5  3039.485 ± 111.714  ns/op\nAllocateBuffer1.heap     16384  avgt    5  2620.722 ±   5.395  ns/op\n```\n\nEven though direct buffers lose in all of the runs, the difference is\nmuch more noticeable on small buffers while on large buffers, the\noverhead is almost negligible. Due to the 50x difference on a small\nbuffer, it’s a much more compelling example to look into. Let’s start a\nbenchmark again, make it run for much longer, and use async profiler to\nsee what where the time is spent.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _ByteBuffer.allocateDirect()_\n\n\u003c/div\u003e\n\n\u003cobject type=\"image/svg+xml\" data=\"/images/allocatedirect/alloc_direct_perf.svg\"/\u003e\n\nThe flame graph already hints towards some of the overhead. Not only the\ndirect buffers need to allocate memory, but it also needs to reserve it\nto check the maximum native memory limit. On top of this, the buffer\nneeds to be zeroed as `malloc` can’t guarantee that it doesn’t return\nyou some garbage while the buffer needs to be ready to use. And finally,\nit needs to register itself for deallocation as a soft reference. All of\nthis seems like a lot of work, but the actual allocation still takes a\nhalf of the time! So, even if the heap buffer doesn’t need to do any\nwork other than calling `malloc`, it should only be as twice as slow,\nnot 50 times! Profiling heap buffer allocations can hopefully reveal\nwhere such a vast difference is coming from.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _ByteBuffer.allocate()_\n\n\u003c/div\u003e\n\n\u003cobject type=\"image/svg+xml\" data=\"/images/allocatedirect/alloc_heap_perf.svg\"/\u003e\n\nThe heap buffer flame graph is surprisingly blank. There isn’t much\nhappening on the graph. Yet, there are still some allocations in the\nyellow flame tower on the right. However, the whole allocation path only\ntakes 2% of the time, and the rest is nothing? Exploring the yellow\ntower gives a further clue. Most of its time is taken by a function\nthat’s called `MemAllocator::allocate_inside_tlab_slow`. The meaning of\nthe `allocate_slow` part is self-explanatory, but it’s `inside_tlab`\nthat is the answer.\n\nTLAB stands for Thread Local Allocation Buffer. TLAB is a space in the\nEden, the space where all new objects are born, dedicated for each\nthread to allocate objects. When different threads allocate memory, they\ndon’t have to contend on the global memory. Every thread allocates\nobjects locally, and because the buffer is not shared with other\nthreads, there is no need to use call `malloc`. All that’s needed is to\nmove the pointer by a few bytes. The fact that most of the allocations\nhappen in TLAB could explain why heap buffers are so much faster when\ntheir size is small. When the size is large, the allocations won’t occur\nin TLAB due to the limits on its size, which will result in buffer\nallocation times being almost on par.\n\nNow that we’ve assumed that we know why it’s so much faster, can we jump\nto the next section? Not so fast!\n\nSo far, TLAB is just a theory, and we need to conduct an experiment to\nvalidate it. One of the easiest ways is to simply disable TLAB with the\n`-XX:-UseTLAB` options.\n\n```java\n// run with ./bench.sh alloc4\n@Fork(jvmArgsAppend = { \"-XX:-UseTLAB\" })\n@Benchmark\npublic ByteBuffer heap() {\n  return ByteBuffer.allocate(size);\n}\n```\n\n```\nBenchmark             (size)  Mode  Cnt    Score   Error  Units\nAllocateBuffer2.heap     128  avgt    5  151.999 ± 8.477  ns/op\n```\n\nWere we right? Yes and no. The performance results with disabled TLAB\nare not as impressive anymore. Though, the pure allocation time is still\nabout three times faster even considering that the benchmark needs to\nnot only allocate memory for the buffer itself but also for the\n`ByteBuffer` class. The still significant difference shows the cost of\ngoing back to the operating system with a syscall every time to ask for\nmore memory with occasional page faults.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _ByteBuffer.allocate(), -XX:-UseTLAB_\n\n\u003c/div\u003e\n\n\u003cobject type=\"image/svg+xml\" data=\"/images/allocatedirect/alloc_heap_no_tlab.svg\"/\u003e\n\nAs a rule of thumb, if your buffers are mostly short-lived and small,\nusing heap byte buffers will likely be a more performant choice for you.\nConveniently, it’s exactly what the javadoc of the ByteBuffer class is\nwarning us about.\n\n\u003e It is therefore recommended that direct buffers be allocated primarily\n\u003e for large, long-lived buffers that are subject to the underlying\n\u003e system’s native I/O operations.\n\u003e\n\u003e —  ByteBuffer.java\n\n#### Memory costs\n\nSo far, we’ve only been measuring allocations. Still, looking at the\nflame graphs, we can also see the de-allocation path which is frequently\ninvoked by JMH that runs the benchmarks by explicitly invoking\n[`System.gc()`](https://github.com/openjdk/jmh/blob/4264de9486c32b48da8161e3ac076a0187b4176f/jmh-core/src/main/java/org/openjdk/jmh/runner/BaseRunner.java#L273)\nand finalization before each iteration. That way, the previously\nallocated buffers will be deallocated.\n\nHowever, in the real applications, as we saw in the debugging story,\nwe’re at a mercy of the GC to deallocate those buffers. In this case,\nthe amount of memory consumed by the app might be hard to predict as it\ndepends on the GC and how the GC behaves on this workload. For how long\nwould the following code run?\n\n``` java\npublic class HeapMemoryChaser {\n  public static void main(String[] args) {\n    while (true) {\n      ByteBuffer.allocate(1024);\n    }\n  }\n}\n```\n\nThe code above contains an infinite loop, so you can say forever, and\nyou will be completely right. There are no conditions, so there is\nnothing that can prevent the loop from running. Will this snippet run\nforever too?\n\n``` java\npublic class DirectMemoryChaser {\n  public static void main(String[] args) {\n    while (true) {\n      ByteBuffer.allocateDirect(1024);\n    }\n  }\n}\n```\n\nWill the above code run forever as well? It depends on how lucky we are.\nThere are no guarantees on when a GC would clean up the allocated direct\nbuffers. Various JVM options can vary the result from run forever with\nno issues to a crash after a few seconds. Running the above code with\n`-Xmx6G` on a VM with 8GB RAM runs for about 20 seconds until it gets\nkilled by the operating system.\n\n```\n$ time java -Xmx6G DirectMemoryChaser\nKilled\n\nreal    0m24.211s\n```\n\n`dmesg` shows an insightful message explaining that the process was\nkilled due to lack of memory.\n\n```\n[...] Out of memory: Killed process 4560 (java) total-vm:10119088kB, anon-rss:7624780kB, file-rss:1336kB, shmem-rss:0kB, UID:1000 pgtables:15216kB oom_score_adj:0\n[...] oom_reaper: reaped process 4560 (java), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB\n```\n\nAllocating direct buffers without care can cause the app to go far\nbeyond the expected memory usage as there are no guarantees on when the\nsoft references are going to be cleaned. Crashing right after the start\nis counterintuitively a good result. At least, you can observe the\nfailure, reproduce it, understand it and fix it. A crash after a few\nhours of running is much worse, and without a clear feedback loop, it’s\nmuch harder to resolve the issue.\n\nWhen debugging such issues, or as a preventative measure, consider using\n`-XX:+AlwaysPreTouch` to at least exclude the heap growth out of the\nequation. One way to prevent this is to run the infinite growth of\ndirect buffers is to use `-XX:MaxDirectMemorySize=${MAX_DIRECT_MEM}` to\nensure that the usage of direct memory doesn’t grow uncontrollably.\n\n## The benefits\n\nSo far, the direct byte buffers have only caused troubles. In the story,\nswitching to heap-based byte buffers was a clear win, though there are a\nlot of hidden dangers in using them. Would it be reasonable to use heap\nbyte buffers only and never use direct buffers? The choice exists for a\nreason, and there reasons to use direct buffers.\n\n### Off Heap Graal\n\nWe have observed problems with allocations and deallocations, so what if\na buffer is only allocated once and never deallocated? One buffer is not\nenough most of the time, so you can create a pool of buffers, borrow\nthem for some time and then return back. It is what Netty does with\ntheir [`ByteBuf`](https://netty.io/wiki/using-as-a-generic-library.html)\nclasses which are built to fix some of the downsides of the `ByteBuffer`\nclass. Nevertheless, it’s still not clear why one should prefer direct\nbuffers over heap buffers.\n\nAvoiding GC altogether could be one of the reason. You could be managing\nterabytes of memory without any GC overhead. While you could manage\nlarge amounts of memory with direct byte buffers, there is a limit of\n2^31 on the indices that you can use with a single buffer. A solution is\ncoming in the form of a [Foreign-Memory Access\nAPI](https://openjdk.java.net/jeps/383) which is available for the\nsecond preview in JDK 15. But avoiding GC is not the main reason.\n\n### IO\n\nThe IO is where direct byte buffers shine! Let’s say we need to copy\nsome memory between two files. Heap byte buffers are obviously backed by\nmemory in a heap, so the contents of the files would have to be copied\nto be sent back seconds later. This can be avoided completely with\ndirect byte buffers. Direct byte buffers excel when you don’t need a\nbuffer per se but rather a pointer to a piece of memory somewhere\noutside of the heap. Again, at this point, this is only a hypothesis of\na random person on the internet. Let’s prove or disprove it with the\nfollowing benchmark.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[CopyFileBenchmark.java](https://github.com/SerCeMan/allocatedirect/blob/master/bench/src/main/java/me/serce/CopyFileBenchmark.java)_\n\n\u003c/div\u003e\n\n``` java\n@Benchmark // ./bench.sh reverse\npublic void reverseBytesInFiles() throws Exception {\n  ByteBuffer buf = this.buffer;\n  buf.clear();\n  try (FileChannel channel1 = FileChannel.open(Paths.get(DIR + \"file1\"), READ);\n       FileChannel channel2 = FileChannel.open(Paths.get(DIR + \"file2\"), WRITE)) {\n    while (buf.hasRemaining()) {\n      channel1.read(buf);\n    }\n    buf.put(0, buf.get(SIZE - 1));\n    buf.flip();\n    while (buf.hasRemaining()) {\n      channel2.write(buf);\n    }\n  }\n}\n```\n\nThe code above reads 64 MB of random data from the first files, reverses\nthe byte order of each long in the array and then puts it back.\nReversing the first and the last bytes here is a tiny operation which\nthe only goal is to modify the contents of the file in some way as\ncopying could simply be done by calling `channel.transferTo`.\n\nThe results show that the direct buffer is the clear winner, almost\ntwice as fast!\n\n```\nBenchmark                              (bufferType)  Mode  Cnt   Score   Error  Units\nCopyFileBenchmark.reverseBytesInFiles        direct  avgt    5  36.383 ± 0.683  ms/op\nCopyFileBenchmark.reverseBytesInFiles          heap  avgt    5  59.816 ± 0.834  ms/op\n```\n\nThe next step is to understand where the time was spent to validate our\nhypothesis. Taking a flamegraph for the direct buffer shows what we\nexpected — all of the time spent in kernel reading and writing files.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _Directy Buffer_\n\n\u003c/div\u003e\n\n\u003cobject type=\"image/svg+xml\" data=\"/images/allocatedirect/reverse_offheap.svg\"/\u003e\n\nFor the heap buffer, for both operations, reading and writing, the\nmemory has to be copied first between the buffers which we can clearly\nsee from the flamegraph. A solid chunk of it is taken by the\n`copyMemory` function.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _Heap Buffer_\n\n\u003c/div\u003e\n\n\u003cobject type=\"image/svg+xml\" data=\"/images/allocatedirect/reverse_heap.svg\"/\u003e\n\nThe IO here does not only refer to writing to disk, but it can also be\nwriting to a socket which is what a considerable portion of the java\napplications is performing all day non-stop. As you can see, carefully\nchoosing your buffers can significantly affect performance.\n\n### Endianness\n\nCan direct byte buffers be even faster? While reading the javadoc for\ncreate methods, note an important remark:\n\n\u003e The new buffer’s position will be zero, its limit will be its\n\u003e capacity, its mark will be undefined, each of its elements will be\n\u003e initialized to zero, and its byte order will be ByteOrder#BIG_ENDIAN.\n\u003e\n\u003e —  ByteBuffer.java\n\nThe byte order of byte buffers created in java is always big endian by\ndefault. While having an always predictable default is great, it also\nmeans that sometimes, it might not match the endianness of the\nunderlying platform. In the case of an `m5.large` AWS instance, this is\nindeed the case.\n\n``` shell\njshell\u003e java.nio.ByteOrder.nativeOrder()\n$1 ==\u003e LITTLE_ENDIAN\n```\n\nThis fact immediately raises the question if, or rather when changing\nendianness can yield any significant performance wins. The only way to\nfind out is to measure it.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[OrderBenchmark.java](https://github.com/SerCeMan/allocatedirect/blob/master/bench/src/main/java/me/serce/OrderBenchmark.java)_\n\n\u003c/div\u003e\n\n``` java\nstatic final int SIZE = 1024 * 1024 * 1024;\n\n@Param({\"direct-native-order\", \"direct\"})\nString bufferType;\n\n@Setup\npublic void setUp() throws Exception {\n  switch (bufferType) {\n    case \"direct\":\n      buffer = ByteBuffer.allocateDirect(SIZE); break;\n    case \"direct-native-order\":\n      buffer = ByteBuffer.allocateDirect(SIZE).order(ByteOrder.nativeOrder()); break;\n  }\n  channel = FileChannel.open(Paths.get(\"/dev/urandom\"), READ);\n  while (buffer.hasRemaining()) { channel.read(buffer); }\n  buffer.flip();\n  this.buffer = buffer.asLongBuffer();\n}\n\n@Benchmark // run with ./bench.sh order\npublic long sumBytes() {\n  long sum = 0;\n  for (int i = 0; i \u003c SIZE / 8; i++) {\n    sum += buffer.get(i);\n  }\n  return sum;\n}\n```\n\nThe above benchmark measures a specific use-case. We load a gigabyte\nworth of random longs into memory. Then, we simply read them one by one.\nIt’s interesting that depending on the endianness, the result will be\ndifferent as it affects the order of the bytes. We don’t care about the\nbyte order for this use-case, however, as a random value with reversed\nbyte order is still a random value.\n\n```\nBenchmark                       (bufferType)  Mode  Cnt    Score   Error  Units\nOrderBenchmark.sumBytes  direct-native-order  avgt    5  136.025 ± 2.262  ms/op\nOrderBenchmark.sumBytes               direct  avgt    5  195.980 ± 8.360  ms/op\n```\n\nThe first impression is that iterating through a gigabyte worth of\nrandom memory is pretty darn fast. The second is that the native order\nbyte buffer is performing 1.5 times faster! As before, running async\nprofiler helps to reveal the reason why the native order is more\nperformant.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _Non-native (Big Endian)_\n\n\u003c/div\u003e\n\n\u003cobject type=\"image/svg+xml\" data=\"/images/allocatedirect/sum_big_endian.svg\"/\u003e\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _Native (Little Endian)_\n\n\u003c/div\u003e\n\n\u003cobject type=\"image/svg+xml\" data=\"/images/allocatedirect/sum_native_endian.svg\"/\u003e\n\nComparing the graphs above, the first difference that stands out is that\nbyte buffer classes are actually different depending on the byte order.\nThe native buffer is `DirectLongBufferU` while the non-native one is\n`DirectLongBufferS`. The main difference between them is the presence of\nthe `Bits.swap` method.\n\nLooking further into the method, we can see that it delegates directly\nto `Long.reverseBytes`. While its implementation in Java is quite\ncomplex, one can notice the `@HotSpotIntrinsicCandidate` annotation. The\nannotation is a signal that at runtime, JIT could replace the method\nwith pre-prepared assembly code. Adding a set of JVM options,\n`-XX:CompileCommand=print,\\*OrderBenchmark.sumBytes*`, to the benchmark\nallows us to peek at the resulting assembly code to understand how\nexactly the `reverseBytes` affects the resulting code.\n\n\u003ctable\u003e\n    \u003ccolgroup\u003e\n        \u003ccol style={{width: \"50%\"}} /\u003e\n        \u003ccol style={{width: \"50%\"}} /\u003e\n    \u003c/colgroup\u003e\n    \u003cthead\u003e\n    \u003ctr className=\"header\"\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003eNon-native (Big Endian)\u003c/th\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003eNative (Little Endian)\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n    \u003ctr className=\"odd\"\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre\u003e\n```x86asm\n....\nloop:\n mov    r10d,DWORD PTR [rdx+0x14]\n mov    ecx,DWORD PTR [r10+0x8]\n mov    r8,r10                  ; \u0026lt;- buffer\n cmp    ecx,0x16577b\n jne    0x00007f9d5c277070\n mov    ebx,DWORD PTR [r8+0x1c] ; \u0026lt;- limit\n cmp    r11d,ebx\n jge    0x00007f9d5c277078  ; checkIndex(i)\n mov    r10,QWORD PTR [r8+0x10]\n movsxd r8,r11d\n shl    r8,0x3\n add    r8,r10\n mov    r10,r8\n mov    r10,QWORD PTR [r10] ; r10 = get(i)\n bswap  r10                 ; reverseBytes(r10)\n add    rax,r10             ; sum += r10\n inc    r11d                ; i+=1\n cmp    r11d,0x8000000      ; i \u0026lt; SIZE/8\n jl     loop\n...\n```\n        \u003c/pre\u003e\u003c/td\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre\u003e\n```x86asm\n...\nloop:\n mov    r11d,DWORD PTR [r8+0x14] ; \u0026lt;- buffer\n mov    r9d,DWORD PTR [r11+0x1c] ; \u0026lt;- limit\n cmp    ecx,r9d\n jge    0x00007f268c274f57 ; checkIndex(i)\n mov    r10,QWORD PTR [r11+0x10]\n movsxd r11,ecx\n shl    r11,0x3\n add    r11,r10\n mov    r10,r11\n add    rbx,QWORD PTR [r10] ; sum += get(i)\n inc    ecx                 ; i+=1\n cmp    ecx,0x8000000       ; i \u0026lt; SIZE/8\n jl     loop\n...\n```\n\u003c/pre\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\nComparing the compilations listings of these two implementations, we can\nnotice that the biggest difference between them is the `bswap`\ninstruction which is the essence of the `Bytes.swap` method. As\nexpected, it reverses the byte order every time a long is read from the\nbuffer.\n\nReading a gigabyte of memory into longs is an interesting workload, but\nit’s not necessarily the one that you’re likely to encounter in\nproduction. Endianness can be a useful thing to remember about, but\nunless working with native libraries or working with massive files, it’s\nunlikely to be a concern.\n\n## Conclusion\n\nEvery non-trivial Java application directly or indirectly uses byte\nbuffers. On the surface, ByteBuffer is a simple class. It’s just a\npointer to a chunk of memory. Nevertheless, even by looking at such a\nsimple class, you can discover a deep rabbit hole. Even though we’ve\nonly looked at the tip of the iceberg, I hope that you have a clear idea\nnow of when you could use a heap buffer, and when you would choose a\ndirect buffer.\n\nModern JVM runtimes are complicated environments. While they provide\nsane defaults, they also present multiple options. The choice is always\nthere, it’s up to you to make that choice, but it’s crucial to be aware\nof the consequences. Fortunatelly, JVM runtimes also come with a whole\nlot of various observability tools, JMX metrics, GC logs, profilers, and\nif you really want it’s not even that hard to look at the generated\nassembly code. Using techniques shown in this article, you can make a\nchoice not for the workload of a guy from the internet, but for *your*\nworkload, which can result in amazing results in production later. We\nhave to care more about later sometimes, you know.\n\n## Thank you to\n\n- Uri Baghin and [Paul Tune](https://twitter.com/ptuls) for reviewing\nthe article.\n- You for reading the article.\n\n## References\n\n- [Async Profiler](https://github.com/jvm-profiling-tools/async-profiler/)\n- [Beyond ByteBuffers by Brian Goetz](https://www.youtube.com/watch?v=iwSCtxMbBLI)\n- [GC Easy GC Analyser](https://gceasy.io/)\n- [JVM Anatomy Quark \\#4: TLAB allocation](https://shipilev.net/jvm/anatomy-quarks/4-tlab-allocation/)\n- [Netty - One Framework to rule them all by Norman Maurer](https://www.youtube.com/watch?v=DKJ0w30M0vg)\n- [Netty’s ByteBuf API](https://netty.io/wiki/using-as-a-generic-library.html)\n\n## Discuss on\n\n* [Twitter](https://twitter.com/SerCeMan/status/1328999241541328897)\n","description":"Empty","slug":"18-11-2020-allocate-direct","kind":"mdx"},{"title":"You don't need no Service Mesh","date":"2020-07-23","content":"\nimport Quote from \"../components/Quote\";\n\nHi!\n\nService meshes have attracted an enormous amount of hype around them.\nWith at least a few talks about service meshes during each tech\nconference, one can easily be convinced that having a service mesh in\ntheir infrastructure is a must. However, hype isn’t a good indicator of\nwhether the new shiny tech is the right solution for your problems. So\nbelow, I’ll try to express an anti-hype opinion on service meshes to\nhopefully make it less confusing when you want to decide whether you may\nor may not need one.\n\n![](/images/servicemesh/rick.png)\n\n\u003cQuote\n    quote=\"There’s a lesson here, and I’m not going to be the one to figure it out.\"\n    attribution=\"Rick Sanchez\"\n/\u003e\n\n## The invention\n\nLet’s take a step back in history and take a look at one of the [early\narticles](https://eng.lyft.com/envoy-7-months-later-41986c2fd443) about\nintroducing Envoy at Lyft.\n\n\u003e As it turns out, almost every company with a moderately-sized service\n\u003e oriented architecture is having the same problems that Lyft did prior\n\u003e to the development and deployment of Envoy:\n\u003e\n\u003e - An architecture composed of a variety of languages, each containing\n\u003e   a half-baked RPC library, including partial (or zero)\n\u003e   implementations of rate limiting, circuit breaking, timeouts,\n\u003e   retries, etc.\n\u003e\n\u003e - Differing or partial implementations of stats, logging, and ….\n\nWhile Envoy is not a service mesh by itself, the outlined problems\ndescribe the exact reason why service meshes were invented. They add\n“rate limiting, circuit breaking, …” and other reliability,\nobservability, and security features to the services by enforcing the\ncommunication to go through the service mesh proxies, a data plane.\nAdditionally, they require a separate component, a control plane, to\ncontrol the configuration.\n\nHowever, at this point, a lot of people miss the context in which\nservice meshes were introduced. Service meshes are able to solve the\nproblem not because it’s impossible to solve them in any other way.\nThere are many battle-proof RPC libraries that take on the challenges of\na separate data plane layer,\n[Finagle](https://github.com/twitter/finagle),\n[gRPC](https://github.com/grpc),\n[Armeria](https://github.com/line/armeria),\n[Servicetalk](https://github.com/apple/servicetalk), to name a few.\nAfter all, the very first service mesh - Linkerd 1.0 [is powered by\nFinagle](https://github.com/linkerd/linkerd). The RPC libraries will\nneed a component which provides service discovery and configuration\nmanagement to make it a true mesh. For instance, Zookeeper, or Consul, a\ncomponent that service meshes call a control plane.\n\nWhy introduce a new concept to solve the problems that have been solved\nbefore? The service mesh concept wasn’t introduced to address problems\nthat hadn’t been addressed before but rather address them in a way that\ndoesn’t require any modifications to the application code, which is\nincredibly convenient when it’s hard to introduce an RPC layer into an\nexisting heterogeneous microservice environment.\n\nWhen you hear service mesh, Istio with Envoy might be the first thing\nthat comes to mind, but it wasn’t the first service mesh to enter the\nmarket. Linkerd authors who pioneered the space, described exactly this\nsituation in the [\"why is the service mesh necessary\"](https://linkerd.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/#why-is-the-service-mesh-necessary).\nInterestingly, in many hype-y articles on the Internet this context is\noften forgotten, or omitted.\n\nSolving a problem well, even if it’s a problem that a lot of people\nhave, doesn’t magically provide the tech with a lot of hype. There is\nalways a sponsor behind it. I don’t know who the sponsor was here, and\nI’m going to speculate, but it’s hard to sell an RPC library in the\nworld where open source is a fundamental requirement. There is no clear\nbusiness model there, that’s why most of the mature RPC libraries were\nopen-sourced by large tech companies for which it’s not a part of the\ncore business model. A library is just code, not a piece of\ninfrastructure. Service meshes are a different story. It’s an isolated\nnon-trivial piece of infrastructure. As a vendor, not only can you\nprovide consultancy around the configuration and deployment, but you can\nalso sell complete hosted solutions around it.\n\n## Disillusionments\n\nNow that we’ve established the problems, the solution, and most\nimportantly, the context in which the solution was made, let’s take a\nlook at the alternatives. The most obvious one, in the spirit of KISS,\nis to use an RPC library for your preferred language. Here is where the\ncontext is crucial: if you have a large fleet of services, each written\nin its own language/ecosystem, and the only language that they share is\nHTTP then having a single shared RPC library is going to be hard.\nPerhaps, you’ve got a fabric of deployed and running services, but\neveryone is afraid of touching them, no one knows how they work, and\neach redeploy is an adventure. A service mesh is here to help you,\nbecause at least you’ll be able to roll out new infrastructure features\nto the mesh regularly.\n\nOn the other hand, if you have a fleet of healthy services written in a\nsingle application stack, then it’s a good idea to think twice before\nintroducing a service mesh. By simply introducing or evolving a shared\nRPC library, you’ll get the exact same benefits and avoid dealing with\nthe downsides of maintaining service meshes. By studying the service\nmesh limitations thoroughly, you can avoid finding yourself in the\ntrough of disillusionment.\n\n![Hype Cycle](/images/servicemesh/curve.png)\n\n### Different ecosystem\n\nThe ecosystem of the service mesh of your choice will likely be\ndifferent from the ecosystem of your services. Beautiful websites always\nmake you believe that the solution is plug’n'play, always works and\nnever goes down. In reality, sooner or later problems, bugs, quirks in\nbehaviour will reveal themselves, as they always do. At that point,\nyou’ll need to have engineers who work on the service-mesh’s ecosystem\nwhich when it’s different from the main app, effectively limits the set\nof people who can introduce changes or fix problems. This is likely to\nreintroduce silos, which is against the whole DevOps spirit. Yes, having\na DevOps team of engineers who are doing DevOps-y things [is against\nDevOps](https://continuousdelivery.com/2012/10/theres-no-such-thing-as-a-devops-team/).\n\n### Unnecessary overhead\n\nNot only having a proxy in front of each service adds overhead (often\nsignificant, talking about\n[90pt](https://istio.io/latest/docs/ops/deployment/performance-and-scalability/)\nrather than 99pt in the performance summary [doesn’t make software run\nfaster](https://www.infoq.com/presentations/latency-response-time/)) and\nconsumes resources, but you also requires time (or rather a team of\npeople) to manage them. Yes, it can help to make some of the tasks\npotentially easier - yay, you can now add canary deployments with a few\nlines of YAML to simple applications now. However, you still need to\nmanage canary deployments of the proxies themselves which don’t have a\nproxy in front of them. The problems just get pushed up the stack.\n\n### Limiting your architecture to what The Proxy supports.\n\nAs you’re reading this paragraph, HTTP/3 is slowly being rolled out to\nthe Internet. It uses UDP as transport. Why use UDP rather than create a\ncompletely new protocol you ask? That’s because anything but TCP and UDP\nis simply “blocked” by the boxes, various proxies on the internet -\nrouters, gateways, etc. This phenomenon got named\n[ossification](https://http3-explained.haxx.se/en/why-quic/why-ossification).\nSo, only TCP or UDP are left is the practical chose, and even UDP is\npartially blocked by various corporate proxies which slows down the\nadoption.\n\nEven though your microservice environment is probably much smaller\ncompared to the Internet, you can draw parallels with service meshes.\nProxies can ossify your application architecture by limiting how your\nservices talk to each other, and there is not much benefit in having\nproxies if you can bypass them. Suppose you want to build a reactive\napplication which is using RSocket over pure tcp? Or perhaps a\nmessage-driven application using an actor model? Or maybe push the\nperformance boundaries with Aeron? Not going to happen until the box in\nthe middle becomes aware of the protocol.\n\n## Do I need one?\n\nWhat does it all mean for you as an engineer? The answer to whether you\nneed to adopt the service mesh approach comes down to the state of the\nmicroservice environment you’re trying to improve. As we have\nestablished, compared to an RPC framework, service meshes allow you to:\n\n1.  Deploy the infra changes more often than deploying your services.\n2.  Introduce infra changes without touching the service code.\n\nThe point 1. is important when for whatever reason you can’t redeploy\nyour services very often, e.g. maybe no one remembers how it’s done\nanymore, or maybe there are other restrictions. The point 2. is\nimportant when your stack is heterogeneous, e.g. some services are built\nin Go, some in Java, some in Haskell, etc. Where are you on the interval\nfrom a huge set of heterogeneous services with unknown deployment\nschedules to a set of homogenous regularly deployed services defines\nwhether a service mesh is the best solution for you.\n\n## Conclusion\n\nService meshes have a lot of hype around them, and way too much in my\nopinion. However, before committing to a piece of technology, it’s\ncrucial to understand the problems it solves, and the context in which\nthe solution was made. A service mesh is not an ultimate “good practice”\nbut simply one of the patterns to solve a set of issues, and it’s quite\na heavy one.\n\nRather than jumping on board, look carefully - the last thing you want\nis to find out that you have invested in a solution for a problem that\nyou don’t have. Service meshes are an amazing piece of tech solving a\nwhole lot of problems. Not in every case, it is the best solution.\n\n## Thank you to\n\n- You for reading this article.\n- [Paul Tune](https://twitter.com/ptuls) for reviewing the article.\n\n## References\n\n- https://eng.lyft.com/envoy-7-months-later-41986c2fd443\n- https://github.com/linkerd/linkerd/\n- https://servicemesh.io/\n- https://continuousdelivery.com/2012/10/theres-no-such-thing-as-a-devops-team/\n- https://http3-explained.haxx.se/en/why-quic/why-ossification\n- https://linkerd.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/#why-is-the-service-mesh-necessary\n\n## Discuss on\n\n* [Twitter](https://twitter.com/SerCeMan/status/1286242507664191488)\n* [Hacker News](https://news.ycombinator.com/item?id=25148642)\n","description":"Empty","slug":"23-07-2020-you-dont-need-no-service-mesh","kind":"mdx"},{"title":"The matter of time()","date":"2019-05-16","content":"\nimport Quote from \"../components/Quote\";\nimport CanvaEmbed from \"../components/CanvaEmbed\";\nimport CheckResults from \"../components/matteroftime/CheckResults\";\nimport TimeQuizChooser from \"../components/matteroftime/TimeQuizChooser\";\n\nHi!\n\nAs software engineers, we all rely on the notion of time: a crucial\nconcept in ensuring that events in our programs follow a chronological\norder. Yet, invoking a simple call to “get the current time” can\npotentially yield unexpected results and lead to unforeseen consequences\nif not used correctly. Moreover, the invariants about time we observe on\nour local development machine may not necessarily hold in the cloud, or\nin any distributed system.\n\nIn this article, I’ll go through the different ways we can obtain the\ncurrent time in our programs, and present cases where our intuitions and\nexpectations of time from these clocks may mislead us at best or cause\ncatastrophic failures at worst.\n\n![](/images/time/time_1.png)\n\n\u003cQuote\n    quote=\"What would be the biological reality of planet earth rotating once every\n    eighteen hours instead of twenty-four? You have less time, but you have more days\n    in the year. So there’s a sense of losing something, and also gaining something.\n    With an 18-hour clock there’s a lot more yesterdays.\"\n    attribution=\"Untitled (Clock) 2014\"\n/\u003e\n\n\n| ⚠️  | This article is illustrated with examples of code in Java. However, most of the content of this article is applicable to any language or runtime. |\n|-----|---------------------------------------------------------------------------------------------------------------------------------------------------|\n\n## Your local clocks\n\nLet me start by asking you six questions. Here is a set of code\nsnippets. Is it possible that the expression passed to `isThisPossible`\nis true? Take a guess.\n\n\u003cdiv className=\"formalpara-title\"\u003e\n\n    \u003cspan id=\"aq1\"\u003e\u003c/span\u003e1. Is this possible?\n\n\u003c/div\u003e\n\n``` java\nlong t1 = System.currentTimeMillis();\nlong t2 = System.currentTimeMillis();\n\nisThisPossible(t2 - t1 == 0);\n```\n\n\u003cTimeQuizChooser id=\"q1\" /\u003e\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    \u003cspan id=\"aq2\"\u003e\u003c/span\u003e2. Is this possible?\n\n\u003c/div\u003e\n\n``` java\nlong t1 = System.nanoTime();\nlong t2 = System.nanoTime();\n\nisThisPossible(t2 - t1 == 0);\n```\n\n\u003cTimeQuizChooser id=\"q2\" /\u003e\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    \u003cspan id=\"aq3\"\u003e\u003c/span\u003e3. Is this possible?\n\n\u003c/div\u003e\n\n``` java\nlong t1 = System.currentTimeMillis();\nlong t2 = System.currentTimeMillis();\n\nisThisPossible(t2 \u003c t1);\n```\n\n\u003cTimeQuizChooser id=\"q3\" /\u003e\n\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    \u003cspan id=\"aq4\"\u003e\u003c/span\u003e4. Is this possible?\n\n\u003c/div\u003e\n\n``` java\nlong t1 = System.nanoTime();\nlong t2 = System.nanoTime();\n\nisThisPossible(t2 \u003c t1);\n```\n\n\u003cTimeQuizChooser id=\"q4\" /\u003e\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    \u003cspan id=\"aq5\"\u003e\u003c/span\u003e5. Is this possible?\n\n\u003c/div\u003e\n\n``` java\nlong t1 = System.currentTimeMillis();\n\nisThisPossible(t1 \u003c 0);\n```\n\n\u003cTimeQuizChooser id=\"q5\" /\u003e\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    \u003cspan id=\"aq6\"\u003e\u003c/span\u003e6. Is this possible?\n\n\u003c/div\u003e\n\n``` java\nlong t1 = System.nanoTime();\n\nisThisPossible(t1 \u003c 0);\n```\n\n\u003cTimeQuizChooser id=\"q6\" /\u003e\n\n\u003cCheckResults/\u003e\n\nWas the result surprising? My sincere kudos if it was not. The next\nsection of the article will explain why certain behaviour can or can not\nbe observed.\n\nBut why do we even care? Very often we don’t need to, but the snippets\nof code where the business logic relies on the observed timestamps are\ntypically critical pieces of infrastructure code where correctness is a\nmust. False assumptions in these parts of the code can lead to huge\nincidents. This, for instance, happened to [Cloudflare in\n2017](https://blog.cloudflare.com/how-and-why-the-leap-second-affected-cloudflare-dns/),\nwhere the root cause \"was the belief that time cannot go backwards\".\nCloudflare is one of the few companies that openly publishes incident\nreports, but it’s not uncommon to suffer from such false assumptions, as\na few Google searches can confirm, and we can all learn from these\nmistakes. To understand why certain clocks behave in a certain way, we\nfirst need to understand what properties different clocks can give us.\n\n### Monotonicity\n\nThe first property is monotonicity. A monotonically increasing function\nmeans that for every subsequent invocation of such a function the\nproduced value is never smaller than any of the previous values. So, a\nmonotonic clock is a clock that never goes backwards. Sadly, and\nsurprisingly, this property is not a feature of many clocks.\n\n### Resolution\n\nResolution is the second property. It is the smallest observable\ndifference between two clock ticks. The resolution of a simple\nmechanical watch with a second hand is one second. When you’re staring\nat the watch, the meaningful watch hand position can be at 12 seconds or\n13 seconds, but never 12 and a half.\n\n### Latency\n\nVery often latency is overlooked when we’re talking about clocks, but\nit’s quite important when we’re considering other properties like\nresolution. For instance, it doesn’t matter if you have the most precise\natomic watch on your hand with picosecond resolution ‒ if I ask you what\ntime it is and it takes you roughly a second, sometimes less, sometimes\nmore, to take a look and respond, all of this precision fades away.\n\nSo, what properties do Java clocks have, and how do they apply to the\nquestions that we looked at the beginning?\n\n## Clocks on the wall\n\nLet’s start with `System.currentTimeMillis()`. Usually, the best place\nto start the exploration is the documentation written in the Javadoc,\nand there is a lot there to take in. Here is an excerpt of what is\nimportant to us right now.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[Javadoc](http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/lang/System.java#l375)_\n\n\u003c/div\u003e\n\n``` java\n/**\n * Returns the current time in milliseconds. Note that\n * while the unit of time of the return value is a millisecond,\n * the granularity of the value depends on the underlying\n * operating system and may be larger.  For example, many\n * operating systems measure time in units of tens of\n * milliseconds.\n *\n * ...\n *\n * @return  the difference, measured in milliseconds, between\n *          the current time and midnight, January 1, 1970 UTC.\n */\npublic static native long currentTimeMillis();\n```\n\nAs we can see, the clock provides us with a millisecond precision value\nbut the actual resolution depends on the operating system. Moreover, if\nwe measure the latency by measuring the execution time, it will be way\nbelow 1 millisecond, so it’s maybe not a surprise that the answer to the\n[first question](#aq1) was yes.\n\nBut can it go backwards? The Javadoc doesn’t mention anything about\nmonotonicity, so we need to dig deeper, and take a look at the\nimplementation.\n\n\n| ⚠️ | This article only explores the native implementation for Linux and MacOS. However, similar techniques can be applied to other operating systems as well.|\n|----|---|\n\nThe method is native, so the implementation depends on the underlying\nOS. The native implementation for Linux and MacOS look almost identical.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[Linux](http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/linux/os_linux.cpp#l1204)_\n\n\u003c/div\u003e\n\n``` cpp\njlong os::javaTimeMillis() {\n  timeval time;\n  int status = gettimeofday(\u0026time, NULL);\n  assert(status != -1, \"linux error\");\n  return jlong(time.tv_sec) * 1000  +  jlong(time.tv_usec / 1000);\n}\n```\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[MacOS](http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/bsd/os_bsd.cpp#l893)_\n\n\u003c/div\u003e\n\n``` cpp\njlong os::javaTimeMillis() {\n  timeval time;\n  int status = gettimeofday(\u0026time, NULL);\n  assert(status != -1, \"bsd error\");\n  return jlong(time.tv_sec) * 1000  +  jlong(time.tv_usec / 1000);\n}\n```\n\nThe functions invoke exactly the same syscall, `gettimeofday`. The man\npage can provide us with more info, but more importantly with some\nvaluable notes:\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[man page](http://man7.org/linux/man-pages/man2/gettimeofday.2.html)_\n\n\u003c/div\u003e\n\n```\nNAME\n       gettimeofday, settimeofday - get / set time\n\nNOTES\n       The time returned by gettimeofday() is affected by discontinuous\n       jumps in the system time (e.g., if the system administrator manually\n       changes the system time).  If you need a monotonically increasing\n       clock, see clock_gettime(2).\n```\n\nAs noted above, the time is affected by discontinuous jumps in the\nsystem time, which could be backwards, hence the clock is not monotonic.\nThe answer to the [third question](#aq3) was yes which does make sense:\nif we change the current time to one hour ago, we still want\n`currentTimeMillis` to return current time, even though the definition\nof the current time has changed. That’s why it’s often called wall-clock\ntime, the clock on the wall can also jump back in time if we adjust it.\n\n### The nanos of the current time\n\nThe same exploration path can be taken for `System.nanoTime()`. Let’s\nstart from the Javadoc which has even more intriguing details than the\nprevious one; here is an excerpt.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[Javadoc](http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/java.base/share/classes/java/lang/System.java#l394)_\n\n\u003c/div\u003e\n\n``` java\n/**\n * Returns the current value of the running Java Virtual Machine's\n * high-resolution time source, in nanoseconds.\n *\n * This method can only be used to measure elapsed time and is\n * not related to any other notion of system or wall-clock time.\n * The value returned represents nanoseconds since some fixed but\n * arbitrary \u003ci\u003eorigin\u003c/i\u003e time (perhaps in the future, so values\n * may be negative) ...\n *\n * \u003cp\u003eThis method provides nanosecond precision, but not necessarily\n * nanosecond resolution ...\n *\n * \u003cp\u003eThe values returned by this method become meaningful only when\n * the difference between two such values, obtained within the same\n * instance of a Java virtual machine, is computed.\n *\n * ...\n */\npublic static native long nanoTime();\n```\n\nApparently, the time returned by this clock isn’t related to any\nreal-world time; it can only be used to compare the timestamps within\nthe same JVM instance, and it’s relative to an arbitrary “origin” which\ncan be in the future, and therefore it might be negative – which answers\nthe [sixth question](#aq3). Similar to `currentTimeMillis`, this method\nprovides nanosecond precision, but not necessarily nanosecond\nresolution.\n\nNano time can only be used to measure time intervals, so it ought to be\nmonotonic, right? Unfortunately, the Javadoc doesn’t say anything about\nmonotonicity, so the next step is the implementation.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[Linux](http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/linux/os_linux.cpp#l1301)_\n\n\u003c/div\u003e\n\n``` cpp\njlong os::javaTimeNanos() {\n  if (os::supports_monotonic_clock()) {\n    struct timespec tp;\n    int status = Linux::clock_gettime(CLOCK_MONOTONIC, \u0026tp);\n    assert(status == 0, \"gettime error\");\n    jlong result = jlong(tp.tv_sec) * (1000 * 1000 * 1000) + jlong(tp.tv_nsec);\n    return result;\n  } else {\n    timeval time;\n    int status = gettimeofday(\u0026time, NULL);\n    assert(status != -1, \"linux error\");\n    jlong usecs = jlong(time.tv_sec) * (1000 * 1000) + jlong(time.tv_usec);\n    return 1000 * usecs;\n  }\n}\n```\n\nHere comes the first surprise: nano time is indeed monotonic but\n_only_ if the underlying operating system supports it. To be fair, any\nmodern Linux server supports `CLOCK_MONOTONIC`; there are, however, some\n[rare situations](https://stackoverflow.com/a/51345008/1542319) in which\nit might not hold true.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[MacOS](http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/bsd/os_bsd.cpp#l893)_\n\n\u003c/div\u003e\n\n``` cpp\njlong os::javaTimeNanos() {\n  const uint64_t tm = mach_absolute_time();\n  const uint64_t now = (tm * Bsd::_timebase_info.numer) / Bsd::_timebase_info.denom;\n  const uint64_t prev = Bsd::_max_abstime;\n  if (now \u003c= prev) {\n    return prev;   // same or retrograde time;\n  }\n  const uint64_t obsv = Atomic::cmpxchg(now, \u0026Bsd::_max_abstime, prev);\n  assert(obsv \u003e= prev, \"invariant\");   // Monotonicity\n  // If the CAS succeeded then we're done and return \"now\".\n  // If the CAS failed and the observed value \"obsv\" is \u003e= now then\n  // we should return \"obsv\".  If the CAS failed and now \u003e obsv \u003e prv then\n  // some other thread raced this thread and installed a new value, in which case\n  // we could either (a) retry the entire operation, (b) retry trying to install now\n  // or (c) just return obsv.  We use (c).   No loop is required although in some cases\n  // we might discard a higher \"now\" value in deference to a slightly lower but freshly\n  // installed obsv value.   That's entirely benign -- it admits no new orderings compared\n  // to (a) or (b) -- and greatly reduces coherence traffic.\n  // We might also condition (c) on the magnitude of the delta between obsv and now.\n  // Avoiding excessive CAS operations to hot RW locations is critical.\n  // See https://blogs.oracle.com/dave/entry/cas_and_cache_trivia_invalidate\n  return (prev == obsv) ? now : obsv;\n}\n```\n\nThe first thing that stands out is the giant wall of comments. As\nsoftware engineers, we know that if there is a long comment then\nsomething dodgy must be going on. Indeed, the comment is quite\ninteresting. The call to\n[`mach_absolute_time`](https://opensource.apple.com/source/Libc/Libc-320.1.3/i386/mach/mach_absolute_time.c.auto.html)\nuses the [RDTSC](https://en.wikipedia.org/wiki/Time_Stamp_Counter)\ninstruction underneath which can _potentially_ lead to non-monotonic\nbehaviour on machines with multiple CPU sockets, which recently span up\nanother thought-provoking discussion on the [mechanical\nsympathy](https://groups.google.com/forum/#!topic/mechanical-sympathy/7WnH37dA6Yc)\nmailing list.\n\nSo, at least, we can be confident that nano time is always monotonic on\nMacOS, right? Actually, it depends on the JVM version. The code listed\nabove was introduced in JDK9 in\n[JDK-8040140](https://bugs.openjdk.java.net/browse/JDK-8040140) and\nbackported to JDK8. Before, all you could hope for was non-monotonic\ntime which provided at best microsecond resolution because\n`gettimeofday` was used. If we run some\n[benchmarks](https://shipilev.net/blog/2014/nanotrusting-nanotime/#_latency),\nwe’ll see that the latency for these calls can be as small as 30ns, so\nsuddenly the answer to the [second](#aq2) and the [fourth](#aq4)\nquestions is true, or rather \"it depends\".\n\n### When milliseconds are not enough\n\nThe microsecond precision in the case of `gettimeofday` is much more\nthan `System.currentTimeMillis()` can give us, but in the process of\nconversion precision is lost.\n\n``` cpp\njlong os::javaTimeMillis() {\n  timeval time;\n  int status = gettimeofday(\u0026time, NULL);\n  assert(status != -1, \"linux error\");\n  return jlong(time.tv_sec) * 1000  +  jlong(time.tv_usec / 1000);\n                                                      // ^^ precision loss\n}\n```\n\nThe OS can give us additional information which we violently discard in\norder to fit it into a single long. What if we really want to know these\nmicros? In JDK 8, the new JSR 310 arrived which made it possible to\nobtain an instance of `Instant` class which contains the number of\nseconds since the epoch and the number of nanoseconds since the last\nsecond started.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[JSR 310: Date and Time API](https://jcp.org/en/jsr/detail?id=310)_\n\n\u003c/div\u003e\n\n``` java\nInstant instant = Clock.systemUTC().instant();\nlong epochSecond = instant.getEpochSecond();\nint nanoSinceSecond = instant.getNano();\n```\n\nFinally, all Java developers got access to wall-clock time with high\nprecision, right? Not so fast, if we take a look at the implementation\nin JDK8, we’ll find out that it simply delegates straight to\n`System.currentTimeMillis()`.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[JDK8\n    Clock](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/time/Clock.java#l469)_\n\n\u003c/div\u003e\n\n``` java\n@Override\npublic long millis() {\n    return System.currentTimeMillis();\n}\n@Override\npublic Instant instant() {\n    return Instant.ofEpochMilli(millis());\n}\n```\n\nEvidently, this is not optimal and there is a corresponding issue\n[JDK-8068730](https://bugs.openjdk.java.net/browse/JDK-8068730) which\nhas already been resolved and as a result, the precision was increased.\nIt requires an update to JDK9+ where the method delegates to a native\ncall with the following implementation on Linux. Assuming that your OS\ncan provide microsecond resolution, this clock is a great example of a\nclock with nanosecond precision, but only microsecond resolution.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _[JDK9+\n    Clock](http://hg.openjdk.java.net/jdk/jdk11/file/1ddf9a99e4ad/src/hotspot/os/linux/os_linux.cpp#l1211)_\n\n\u003c/div\u003e\n\n``` java\nvoid os::javaTimeSystemUTC(jlong \u0026seconds, jlong \u0026nanos) {\n  timeval time;\n  int status = gettimeofday(\u0026time, NULL);\n  assert(status != -1, \"linux error\");\n  seconds = jlong(time.tv_sec);\n  nanos = jlong(time.tv_usec) * 1000;\n}\n```\n\n## Time exchange\n\nThe possibility to get the current wall-clock time with microsecond\nresolution is great, but is needed often? One of the reasons to use\nwall-clock time is to be able to relate an event that happened on one\nmachine to another event that happened on a different machine, or more\nprecisely, to decide on the order of these events. The events can be\nvery different in nature. Some of them might not be very critical, like\nthe timestamp on a log line, but some of them must be correct, like when\nthere is a conflict in a database due to two values being written\nconcurrently and timestamps are used to determine which event was last.\nThis strategy is called Last Write Wins, or simply LWW.\n\n\u003cCanvaEmbed designID=\"DADZmyL_mRw\"/\u003e\n\nOn the slides above, two clients Alice and Bob are trying to write\nsimultaneously into an eventually consistent webscale database with two\nnodes. While the first value written by Alice was successfully\nsynchronized, Alice’s second write happened to be at approximately the\nsame time as Bob’s. In this situation, the database must resolve the\nconflict so that the data is consistent between all of the nodes. In the\ncase of LWW, the latest write will be chosen by comparing the timestamps\nof each write. LWW works perfectly if the clocks are perfectly\nsynchronised, however, if the clocks are poorly synchronised and the\nclock of the first node has drifted ahead of the second node, LWW\nbecomes Lucky Write Wins – the client connected to the lucky node always\nwins the conflict.\n\n### NTP\n\nThe standard approach to make sure that the clocks on different nodes in\nthe cluster are synchronized is to use Network Time Protocol (NTP). Not\nonly does NTP help synchronise clocks, it also helps propagate a leap\nsecond flag. Leap second is an occasional event where an additional\nsecond is introduced in between 23:59:59 of a chosen day and 00:00:00 of\nthe following day. It’s often implemented as playing the same second\ntwice which from the observer’s point of view might look like a jump 1\nsecond back in time. The last leap second was introduced on the 31st of\nDecember 2016 which resulted in the above-mentioned DNS incident.\n\n![](/images/time/time_2.png)\n\nThe conventional way of dealing with leap seconds is \"leap smearing\".\nThe NTP server which is responsible for leap smearing can distribute the\nadditional second amongst 12 hours before and 12 hours after the second\nis introduced. The wall-clock time during these 24 hours is ticking\nslower and every second is 1/86400 longer which might be surprising,\nhowever less surprising than a jump back in time. The catch is that not\nmany NTP servers support leap smearing, the public NTP servers most\ndefinitely don’t.\n\nThe major cloud providers,\n[Google](https://developers.google.com/time/smear) and\n[AWS](https://aws.amazon.com/blogs/aws/look-before-you-leap-the-coming-leap-second-and-aws/)\nboth provide NTP services with leap smearing support. If your\napplication is hosted on a platform that provides an NTP service and you\ncare about clock synchronisation it’s worthwhile checking that NTP\nsynchronisation is set up with the provider’s NTP service. Not only can\nit help avoid the nasty consequences of applying leap seconds naïvely,\nbut it also dramatically decreases the synchronisation error since the\nnetwork latency is typically much lower within a single datacenter.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    _AWS NTP with chrony_\n\n\u003c/div\u003e\n\n``` shell\nsergey:~$ chronyc sources -v\n210 Number of sources = 9\n\n  .-- Source mode  '^' = server, '=' = peer, '#' = local clock.\n / .- Source state '*' = current synced, '+' = combined , '-' = not combined,\n| /   '?' = unreachable, 'x' = time may be in error, '~' = time too variable.\n||                                                 .- xxxx [ yyyy ] +/- zzzz\n||      Reachability register (octal) -.           |  xxxx = adjusted offset,\n||      Log2(Polling interval) --.      |          |  yyyy = measured offset,\n||                                \\     |          |  zzzz = estimated error.\n||                                 |    |           \\\nMS Name/IP address         Stratum Poll Reach LastRx Last sample\n===============================================================================\n^* 169.254.169.123               3  10   377   433    -25us[  -36us] +/-  356us\n```\n\nUsing a local NTP server can reduce the clock drift down to milliseconds\nor even microseconds in the best case, but what is the worst case? There\nis not much research on this topic, however some notable results were\nmentioned in the Google Spanner paper.\n\n\u003e Between synchronizations, a daemon advertises a slowly increasing time\n\u003e uncertainty. ε is derived from conservatively applied worst-case local\n\u003e clock drift. ε also depends on time-master uncertainty and\n\u003e communication delay to the time masters. In our production\n\u003e environment, ε is typically a sawtooth function of time, varying from\n\u003e about 1 to 7 ms over each poll interval. ε̅\u000f is therefore 4 ms most of\n\u003e the time. The daemon’s poll interval is currently 30 seconds, and the\n\u003e current applied drift rate is set at 200 microseconds/second, which\n\u003e together accounts for the sawtooth bounds from 0 to 6 ms.\\\n\u003e —  Spanner: Google’s Globally-Distributed Database\n\n## Logical conclusion\n\nEven if the monitoring in our cluster shows that the clocks are\nsynchronised with microsecond precision, we need to be cautious and\nshouldn’t rely on this in our software if a failure of this assumption\nis unacceptable. So, if a failure is unacceptable and we need to know\nthe order of the events in a distributed system, is there anything we\ncan do? As always, there is a number of solutions suggested by academia.\n\n### Lamport clocks\n\nWhat we need is a reliable replacement for our system clocks, so that\nfor every two events *A* and *B* we can say that either *A* happened\nbefore *B*, or *B* happened before *A*. Such order between events is\ncalled total order. In the [\"Time, Clocks, and the Ordering of Events in\na Distributed\nSystem\"](https://lamport.azurewebsites.net/pubs/time-clocks.pdf) paper\nLeslie Lamport described the \"happens before\" relation and logical\nclocks that can be used to define total order for a set of events using\nthe following algorithm.\n\n\u003ctable\u003e\n    \u003ccolgroup\u003e\n        \u003ccol style={{width: \"50%\"}}/\u003e\n        \u003ccol style={{width: \"50%\"}}/\u003e\n    \u003c/colgroup\u003e\n    \u003ctbody\u003e\n    \u003ctr class=\"odd\"\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cp\u003eSending a message\u003c/p\u003e\u003c/td\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cp\u003eReceiving a message\u003c/p\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr class=\"even\"\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"pseudocode\"\u003e\n```\ntime = time + 1;\nsend(message, time);\n```\n        \u003c/pre\u003e\n        \u003c/td\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"pseudocode\"\u003e\n```\n(message, ts) = receive();\ntime = max(ts, time) + 1;\n```\n\u003c/pre\u003e\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\nEvery actor, in this case, Alice and Bob, will maintain a shared view of\nthe current time by maintaining a `time` counter which increases every\ntime a message is sent, and when a message is received, the `time` is\nalways bigger than the last observed counter. That way if Alice updates\nthe Database as shown below with the value 2 and tells Bob about the\nlast known state, Bob’s final write carries with it the knowledge of\nseeing Alice’s counter, so it’s chosen as the final state of the\ndatabase.\n\n| ❗️ | In the slides below, Alice tells Bob the value she wrote to the first node. Alternatively, Bob could have read the same value from the first node, leading to the same result – Alice and Bob don't have to communicate directly. |\n|----|---|\n\n\u003cCanvaEmbed designID=\"DADZnGWHlAU\"/\u003e\n\nThis works perfectly as long as we need to define some total order of\nthe events in the system which captures the causality. It’s important to\nnote that having total order means that concurrent events will be\nordered in some way, not necessarily the most logical way. On the slides\nbelow, Alice never talked to Bob, but her counter is bigger which leads\nto her write being chosen in the case of a conflict.\n\n\u003cCanvaEmbed designID=\"DADZnC8PNnM\"/\u003e\n\n\n### Vector clocks\n\nTo deal with truly concurrent events, we need a new definition or order\nwhich is able to express the situation in which events can happen\nconcurrently. Such order is called partial order. Basically, this means\nthat for any two events *A* and *B*, it’s possible to say whether *A*\nhappened before *B*, *B* happened before *A* or *A* and *B* happened\nconcurrently. To determine partial order the following algorithm can be\nused this, where every actor has a separate time counter, and keeps\ntrack of the latest timestamp of any other actor in the system.\n\n\u003ctable\u003e\n    \u003ccolgroup\u003e\n        \u003ccol style={{width: \"50%\"}}/\u003e\n        \u003ccol style={{width: \"50%\"}}/\u003e\n    \u003c/colgroup\u003e\n    \u003ctbody\u003e\n    \u003ctr class=\"odd\"\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cp\u003eSending a message\u003c/p\u003e\u003c/td\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cp\u003eReceiving a message\u003c/p\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr class=\"even\"\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"pseudocode\"\u003e\n```\nV[myId] = V[myId] + 1\nsend(message, V);\n```\n        \u003c/pre\u003e\n        \u003c/td\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"pseudocode\"\u003e\n```\n(message, Vr) = receive();\nfor (i, v) in Vr {\n    V[i] = max(V[i], v);\n}\nV[myId] = V[myId] + 1;\n```\n    \u003c/pre\u003e\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\nThe algorithm was described in 1988, and later using vector clocks for\nconflict resolution in a database was described in the Dynamo paper. On\nthe following slides, Alice keeps track of her own time counter as well\nas Bob’s last known time counter. That way when Alice sends a message to\nBob, he updates his counters and the next message sent to the database\nis chosen during the conflict resolution because each component of Bob’s\ntime vector is larger than the respective component of the previous\nvector.\n\n\u003cCanvaEmbed designID=\"DADZnKp0nOE\"/\u003e\n\nWhen there is a real conflict, vector clocks can help to determine\nwhether the events were truly concurrent. In the scenario below, two\nnodes end up with the events, `[0, 1]` and `[0, 1]` which cannot be\nordered. In this situation, the database can keep both values, and\nreturn them the next time it is read, to let either Alice or Bob decide\nwhich one to keep so that the data is not lost.\n\n\u003cCanvaEmbed designID=\"DADZnBQkwxE\"/\u003e\n\nThese properties, however, do not come for free. The metadata needs to\nbe exchanged with every message, and multiple versions need to be\nstored. After all, some databases, like Cassandra don’t use vector\nclocks [for a reason](https://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks).\n\n## Conclusion\n\n- Use `System.nanoTime()` for measuring time intervals\n\n- Use `System.currentTimeMillis()` for obtaining wall-clock time\n\n- Use `Clock.systemUTC().instant()` for getting wall-clock time with ns\n*precision*\n\n- Not every clock can give you the resolution you want even if its\nprecision is high\n\n- The wall-clock time can be off by dozens of milliseconds (or more, or\nless)\n\n- Use NTP from your cloud provider if time synchronisation matters\n\n- Logical clocks might be more appropriate than the real clocks but they\nhave associated costs\n\n## Thanks\n\n- You for reading this article\n- Uri Baghin for reviewing the article\n\n## References\n\n- [Nanotrusting the Nanotime](https://shipilev.net/blog/2014/nanotrusting-nanotime/)\n- [Spanner: Google’s Globally-Distributed Database](https://www.usenix.org/system/files/conference/osdi12/osdi12-final-16.pdf)\n- [AWSNTP](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html) / [Google NTP](https://developers.google.com/time/)\n- [Video PWLSF - Bryan Fink on \"A Brief History of NTP Time: Memoirs of an Internet Timekeeper\"](https://www.youtube.com/watch?v=un1AHZBgFfk)\n- [Internet time synchronization: the network time protocol](https://ieeexplore.ieee.org/abstract/document/103043/)\n- [How and why the leap second affected Cloudflare DNS](https://blog.cloudflare.com/how-and-why-theleap-second-affected-cloudflare-dns/)\n- [The trouble with timestamps](https://aphyr.com/posts/299-the-trouble-with-timestamps)\n- [Time, Clocks, and the Ordering of Events in a Distributed System](http://lamport.azurewebsites.net/pubs/time-clocks.pdf)\n- [Timestamps in Message-Passing Systems That Preserve the Partial Ordering](http://zoo.cs.yale.edu/classes/cs426/2012/lab/bib/fidge88timestamps.pdf)\n- [Synchronizing Clocks In a Cassandra Cluster](https://blog.rapid7.com/2014/03/14/synchronizing-clocksin-a-cassandra-cluster-pt-1-the-problem/)\n- [Why Cassandra doesn’t need vector clocks](https://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks)\n- [Dynamo: Amazon’s Highly Available Key-value Store](https://www.allthingsdistributed.com/files/amazondynamo-sosp2007.pdf)\n- [Why Vector Clocks Are Easy](http://basho.com/posts/technical/why-vector-clocks-are-easy/)\n- [Why Vector Clocks Are Hard](http://basho.com/posts/technical/why-vector-clocks-are-hard/)\n\n## Discuss on\n\n* [Twitter](https://twitter.com/SerCeMan/status/1128963307753287680)\n* [Hacker News](https://news.ycombinator.com/item?id=20173673)\n* [Reddit](https://www.reddit.com/r/java/comments/bpc0im/the_matter_of_time/)\n\n\n\n","description":"Empty","slug":"16-05-2019-the-matter-of-time","kind":"mdx"},{"title":"Fantastic DSLs and where to find them","date":"2017-06-29","content":"\nimport Quote from \"../components/Quote\";\n\nHi!\n\nKotlin is a very rich language. Unlike many other languages, it allows\ndevelopers to create another language inside it. For example, to mimic\nHTML syntax or to build a completely typed SQL query. But Kotlin’s power\nisn’t limited to simple DSLs. With some Kotlin-fu, it’s possible to\nwrite a DSL that allows manipulating untyped data structures in a typed\nmanner. In this article, we’ll go through different ways to define DSL\nin Kotlin, from very simple to fantastically powerful.\n\n\n\n![](/images/fantastic/kotlin_island.png)\n\n\u003cQuote\n    quote=\"Peter the Great at one time even considered moving the capital of\nRussia to Kotlin Island, proof of the sovereign’s great affinity with\nwater. This utopian idea failed, but many of the fantasies of this\nbaroque autocrat still managed to become implemented.\"\n/\u003e\n\n\u003cdiv class=\"warning\"\u003e\n\n    Some parts of this article might be hard to understand without knowledge\n    of Kotlin syntax. I tried to explain every feature I showed, but the\n    general ability to speak Kotlin is strongly suggested.\n\n\u003c/div\u003e\n\nSo, let’s begin the journey.\n\n## What is s DSL\n\n\u003e Domain-specific language (noun): a computer programming language of\n\u003e limited expressiveness focused on a particular domain\n\u003e\n\u003e —  Martin Fowler Domain-Specific Languages\n\nHere though, I prefer to give DSLs a slightly different definition which\nreflects what is written in the article\n\n\u003e a language (or a set of abstractions) that’s built to deal with a\n\u003e specific domain\n\nThe main difference is that a DSL might not only be a separate language\nbut also a subset of a language which is used to work on a specific\ndomain. This kind of DSL can even be built in Java with some fluent API,\nbut very often it’s indistinguishable from a plain good code. To\ncontrast in Kotlin, many remarkable features might make an internal DSL\nlook different.\n\n## Calling convention\n\nThe first feature actively used by DSLs in Kotlin is a special calling\nconvention. If the last parameter of a method is a function, and you’re\npassing a lambda expression there, you can specify it outside of\nparentheses.\n\nFor example, if one wants to create a function `dotimes` that takes a\nnumber `n`, a function `f` and applies it, the easiest way to do that is\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **good old dotimes**\n\n\u003c/div\u003e\n\n``` kotlin\nfun dotimes(n: Int, f: () -\u003e Unit) {\n    for (i in 0..n-1) {\n        f()\n    }\n}\n```\n\nThe `dotimes` can be called in this way\n\n``` kotlin\ndotimes(5, {\n    println(\"Hello, Kotlin!\")\n})\n```\n\nOr, using the lambda parameter convention and placing lambda function\noutside parentheses.\n\n``` kotlin\ndotimes(5) {\n    println(\"Hello, Kotlin!\")\n}\n```\n\nMoreover, the parentheses can be omitted completely if a lambda is the\nonly parameter of a function. E.g. `do5times` function that only takes a\nlambda as a parameter can be defined and called as\n\n``` kotlin\nfun do5times(f: () -\u003e Unit) = dotimes(5, f)\n\ndo5times {\n    println(\"Hello, Kotlin!\")\n}\n```\n\nBut despite being important, that calling convention is just a tiny\ncontribution to DSLs when compared to extension functions.\n\n## Extension functions\n\nExtension functions simply allow you to extend the functionality of a\nclass from the outside.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **Simple extension function**\n\n\u003c/div\u003e\n\n``` kotlin\nfun String.removeSpaces(): String {\n    return this.filter({ c -\u003e c != ' ' })\n}\n\nprint(\"Hi ! , ext\".removeSpaces()) // \"Hi!,ext\"\n```\n\nHere, the `removeSpace` function is defined on the class String which\nenables an ability to call `removeSpaces` on any `String` instance.\nUnsurprisingly, it removes all the spaces from it. Inside the functions,\n`this` refers to the instance of a receiver class and can be omitted\nlike you do when you’re writing a member function. That might sound\ncomplicated if you have never heard about extension functions before,\nbut looking at the result of the compilation might make it much easier\nto understand.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **Decompiled java code**\n\n\u003c/div\u003e\n\n``` java\npublic static String removeSpaces(String $receiver) {\n  StringBuilder sb = new StringBuilder();\n  for (int i = 0; i \u003c $receiver.length(); i++) {\n    char c = $receiver.charAt(i);\n    if (c != ' ') {\n      sb.append(c);\n    }\n  }\n  return sb.toString();\n}\n```\n\nExtension functions are not some kind of magic. It’s not a Groovy-like\nmonkey patching, they get compiled to simple static functions. But that\nexample shows us a very important caveat - extension functions are\nresolved statically because there is no dispatch mechanism for static\nmethods\n\nEven though this snippet is very simple, it can raise another question -\n\"where did the `StringBuilder` came from?\". An close look at the first\nsnippet through `Java` glasses gives the answer - there is no function\ncalled `filter` defined in the class String. `filter` is also an\nextension function defined in the Kotlin standard library.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **filter function from kotlin stdlib**\n\n\u003c/div\u003e\n\n``` kotlin\npublic inline fun String.filter(predicate: (Char) -\u003e Boolean): String {\n  val destination = StringBuilder()\n  for (index in 0..length - 1) {\n    val element = get(index)\n    if (predicate(element))\n      destination.append(element)\n  }\n  return destination.toString()\n}\n```\n\nKotlin defines a lot of extension functions for Java classes in the\nstandard library. That’s why Kotlin is so convenient to use. One might\nnotice that the function has an `inline` modifier on it which explains\nwhy decompiled `removeSpaces` has a `StringBuilder` inside and not a\ncall to `filter`.\n\nMany newcomers to Kotlin use the `inline` modifier everywhere under the\nimpression that inlining can improve performance. It can, but in many\ncases, inline functions don’t improve performance at all, they even can\nmake it worse. There is an inspection for that in IntelliJ IDEA.\n\n![](/images/fantastic/inspection.png)\n\nThere are also some other uses for `inline` which can be found in\n[docs](https://kotlinlang.org/docs/reference/inline-functions.html).\n\n### Extension function on generic type\n\nThe Kotlin compiler is smart enough to allow for the definition of\nextension functions on a certain generic type. In this example,\n`toIntArray` function can be called only on a collection that contains\nintegers. This makes extension functions truly unique, there is no way\n(without subclassing) to define a method for `Collection` class that can\nbe called only on an `Int` collection.\n\n``` kotlin\nfun Collection\u003cInt\u003e.toIntArray(): IntArray {\n  val result = IntArray(size)\n  var index = 0\n  for (element in this)\n    result[index++] = element\n  return result\n}\n```\n\n``` kotlin\nlistOf(1, 2, 3).toIntArray()       // works\nlistOf(\"1\", \"2\", \"3\").toIntArray() // type error\n```\n\nIf Kotlin has become your native language, you might be wondering now,\nwhy I’m talking about these simple features in an article about DSLs.\nThe thing is, the majority of Kotlin DSLs are based on the\nexpressiveness of the two features mentioned above.\n\n### First simple DSL\n\nGiven the aforementioned features, it’s very easy to write a first very\nsimple DSL.\n\nLet’s say we need to write an event-based droid fighting platform so\nthat users can provide their own strategies and register them on the\nplatform. For each event the user is interested in, they must provide a\ncallback with the droid’s behaviour. A droid has an interface with a few\nmethods for defeating other droids. Or, humans if you will.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **the droid**\n\n\u003c/div\u003e\n\n``` kotlin\ninterface Droid {\n  val peopleAround: Boolean\n  val gun: Gun\n\n  fun fire(gun: Gun)\n  fun moveLeft()\n  fun moveRight()\n}\n```\n\nThis sounds like an ideal case for DSL and now we need to define a\npublic API which the clients will be happy to use. To provide the\ndroid’s behaviour we’ll write a public function.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **API**\n\n\u003c/div\u003e\n\n``` kotlin\nprivate val droid: Droid = getDroid() // inaccessible from the public API\n\npublic fun on(cmd: String, f: Droid.() -\u003e Unit) {\n// ...\n  droid.f()\n// ...\n}\n```\n\nThe type of the argument `f` might look weird, but it’s just the type of\n0-arity extension function on the type Droid. And finally, the APIs\nconsumers can register events in the platform.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **strategy example**\n\n\u003c/div\u003e\n\n``` kotlin\non(\"back\") {\n  moveLeft()\n  if (peopleAround) {\n    fire(gun)\n  }\n}\n```\n\nHere, the anonymous extension function is a second parameter and\ntherefore can be written outside parentheses. `this` in the function has\na type `Droid` and therefore `moveLeft()` as well as other functions and\nproperties can be called by themselves without providing an explicit\nreceiver type..\n\nThe strategy looks very natural, it clearly says that if our droid\nreceives a `back` command, it should move left and try to shoot some\nfolks around him. The next snippet shows to what it can be compiled to\nin order to make it even clearer for those who don’t speak kotlin well\nyet.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **decompiled java call**\n\n\u003c/div\u003e\n\n``` java\non(\"back\", new Function1\u003cDroid, Unit\u003e() {\n  public Unit invoke(Droid droid) {\n    droid.moveLeft();\n    if (droid.getPeopleAround()) {\n      droid.fire(droid.getGun());\n    }\n    return Unit.INSTANCE;\n  }\n});\n```\n\n## HTML builders\n\nBuilding DSLs using extension functions isn’t limited to simple droid\nfighting strategies. For example, it allows us to build a completely\ntyped HTML syntax; HTML builders are even mentioned in the [official\ndocumentation](https://kotlinlang.org/docs/reference/type-safe-builders.html).\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **html builders**\n\n\u003c/div\u003e\n\n``` kotlin\nval list = listOf(\"Kotlin\", \"is\", \"awesome\")\nval result: HTML =\n  html {\n    head {\n      title { +\"HTML DSL in Kotlin\" }\n    }\n    body {\n      p {\n        +\"a line about Kotlin\"\n      }\n      a(href = \"jetbrains.com/kotlin\") {\n        +\"Kotlin\"\n      }\n      p {\n        +\"Kotlin is:\"\n        ul {\n          for (arg in list)\n            li { +arg }\n        }\n      }\n    }\n  }\nprintln(result)\n```\n\nAnd these type-safe builders aren’t a Kotlin invention, on the JVM land\nthey were originated in Groovy. But Groovy is a dynamic language,\nbuilders there are not type-safe.\n\n\u003cdiv class=\"important\"\u003e\n\n    It wouldn’t be completely fair to say that even though it’s what\n    Kotlin’s documentation says, Groovy supports static compilation\n    optionally and there are some ways to compile builders statically as\n    well.\n\n[//]: # (\u0026#40;\u003chttp://melix.github.io/blog/2013/02/13/static_builders_inception.html\u003e\u0026#41;)\n\n\u003c/div\u003e\n\nThe implementation of a DSL in dynamically typed languages is often very\ndifferent to statically typed languages. In Kotlin, in order to build a\nDSL, you need to describe the whole schema of the future language. And\ngiven that the result is a deeply nested data structure, the easiest way\nto convert it to string is to traverse the whole data-structure\nrecursively.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **base interface**\n\n\u003c/div\u003e\n\n``` kotlin\ninterface Element {\n  fun render(builder: StringBuilder, indent: String)\n}\n```\n\nThe simplest line of text can be represented as\n\n``` kotlin\nclass TextElement(val text: String) : Element {\n  override fun render(builder: StringBuilder, indent: String) {\n    builder.append(\"$indent$text\\n\")\n  }\n}\n```\n\nThe real tag representation is a bit more complex\n\n``` kotlin\nabstract class Tag(val name: String) : Element {\n  val children = arrayListOf\u003cElement\u003e()\n  val attributes = hashMapOf\u003cString, String\u003e()\n\n  // open tag\n  // render attributes\n  // render children recursively\n  // close tag\n  override fun render(builder: StringBuilder, indent: String) {\n    builder.append(\"$indent\u003c$name${renderAttributes()}\u003e\\n\")\n    for (c in children) {\n      c.render(builder, indent + \"  \")\n    }\n    builder.append(\"$indent\u003c/$name\u003e\\n\")\n  }\n\n  private fun renderAttributes() = attributes.map { (k, v) -\u003e \" $k=\\\"$v\\\"\" }.joinToString(\"\")\n\n  protected fun \u003cT : Element\u003e initTag(tag: T, init: T.() -\u003e Unit) {\n    tag.init()\n    children.add(tag)\n  }\n\n  operator fun String.unaryPlus() {\n    children.add(TextElement(this))\n  }\n\n  override fun toString(): String {\n    val builder = StringBuilder()\n    render(builder, \"\")\n    return builder.toString()\n  }\n}\n```\n\nIt contains a representation of attributes and a set of children. But\nthe main part that requires attention is the `initTag` function which\nlooks very similar to the function `on` from the \"robot fighting\" DSL\ndefinition.\n\nAnother interesting part is an extension function `unaryPlus` defined as\nan operator for class String inside the `Tag`. It allows us to use a\nconvenient (but let’s be honest not obvious at all) way to insert a line\nof text inside code like:\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **¯\\\\\\_(ツ)\\_/¯ unary plus to append a line of text**\n\n\u003c/div\u003e\n\n``` kotlin\nbody {\n  +\"just a random line\"\n  +\"another line\"\n}\n```\n\nAnd the rest of the DSL is an enumeration of all possible tags.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **\\\u003chead\\\u003e,\\\u003ctitle\\\u003e,\\\u003cbody\\\u003e,\\\u003ca\\\u003e,\\\u003cul\\\u003e,\\\u003cli\\\u003e,\\\u003cp\\\u003e**\n\n\u003c/div\u003e\n\n``` kotlin\nclass HTML : Tag(\"html\") {\n  fun head(init: Head.() -\u003e Unit) = initTag(Head(), init)\n\n  fun body(init: Body.() -\u003e Unit) = initTag(Body(), init)\n}\n\nclass Head : Tag(\"head\") {\n  fun title(init: Title.() -\u003e Unit) = initTag(Title(), init)\n}\n\nclass Title : Tag(\"title\")\n\nabstract class BodyTag(name: String) : Tag(name) {\n  fun p(init: P.() -\u003e Unit) = initTag(P(), init)\n  fun ul(init: UL.() -\u003e Unit) = initTag(UL(), init)\n  fun a(href: String, init: A.() -\u003e Unit) {\n    val a = A()\n    initTag(a, init)\n    a.href = href\n  }\n}\n\nclass Body : BodyTag(\"body\")\nclass UL : BodyTag(\"ul\") {\n  fun li(init: LI.() -\u003e Unit) = initTag(LI(), init)\n}\n\nclass LI : BodyTag(\"li\")\nclass P : BodyTag(\"p\")\n\nclass A : BodyTag(\"a\") {\n  var href: String\n    get() = attributes[\"href\"] ?: \"\"\n    set(value) {\n      attributes[\"href\"] = value\n    }\n}\n```\n\nAs you can see, all these classes define a possible hierarchy of calls.\nThis DSL is just a toy DSL, and therefore it covers a very small and\nlimited subset of HTML. It is extremely tedious to write the whole HTML\nDSL manually. The actual [HTML DSL\nimplementation](https://github.com/Kotlin/kotlinx.html) uses a real [XSD\nschema](https://github.com/Kotlin/kotlinx.html/blob/master/generate/src/main/resources/html_5.xsd)\nto generate all possible classes for the DSL.\n\n### There is always a problem\n\nThis could already be awesome, but the example demonstrates a very weird\nbehaviour — nobody stops you from defining tags inside each other\nmultiple times.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **the problem**\n\n\u003c/div\u003e\n\n``` kotlin\nhead {\n  head {\n    head {\n      // stil possible to write head because implicit receiver html is available\n    }\n  }\n  title { +\"XML encoding with Kotlin\" }\n}\n```\n\nPrior to Kotlin 1.1, the only solution was to redefine function with\ndeprecation.\n\n``` kotlin\nclass Head : Tag(\"head\") {\n  @Deprecated(message = \"wrong scope\", level = DeprecationLevel.ERROR)\n  fun head(init: Head.() -\u003e Unit) = initTag(Head(), init)\n\n  fun title(init: Title.() -\u003e Unit) = initTag(Title(), init)\n}\n```\n\n![](/images/fantastic/err1.png)\n\nThe problem with this approach is that it requires an incredible amount\nof boilerplate and a full understanding of all possible combinations. In\n1.1,\n[KEEP-57](https://github.com/Kotlin/KEEP/blob/master/proposals/scope-control-for-implicit-receivers.md)\nintroduced an alternative to that approach: the `@DslMarker` annotation\nwas introduced which allows us to define a `DSL marker` and introduces a\nset of rules for classes annotated with that marker:\n\n- an implicit receiver may belong to a DSL if marked with a\ncorresponding DSL marker annotation\n\n- two implicit receivers of the same DSL are not accessible in the same\nscope\n\n- the closest one wins\n\n- other available receivers are resolved as usual, but if the resulting\nresolved call binds to such a receiver, it’s a compilation error\n\nSo, the HTML DSL can be fixed by introducing a `@HtmlTagMarker` DSL\nmarker and annotating `Tag` with it.\n\n``` kotlin\n@HtmlTagMarker\nabstract class Tag(val name: String) : Element {\n // ...\n}\n```\n\n![](/images/fantastic/err2.png)\n\nDSLs that give us an ability to construct nested data structures such as\nHTML builders, different configurations, UI builders, etc. is where\nKotlin really shines. Kotlin took an awesome idea from Groovy and made\nit safe and easy to use.\n\nThere are a few more examples of DSLs of that kind:\n\n- [TeamCity\nDSL](http://blog.jetbrains.com/teamcity/2016/11/kotlin-configuration-scripts-an-introduction/)\n\n- [Gradle with Kotlin](http://github.com/gradle/gradle-script-kotlin)\n\n- [Anko](http://github.com/gradle/gradle-script-kotlin)\n\n- [Spek framework](http://spekframework.org)\n\nBut unsurprisingly, it’s not the only type of DSL that can be\nimplemented in Kotlin…\n\n## Fantastic DSL\n\nNot all domains are born the same. Let’s consider a completely different\ndomain. A system which handles transactions containing a payment in some\ncurrency and two people - a sender and a receiver.\n\n![](/images/fantastic/domain.svg)\n\nThe transaction structure has to be immutable to make it safer. But\nsometimes, we might need to create a new transaction with an updated\nfield. For example, the name of the receiver (from) person might need to\nbe changed to let’s say \"John\". There are a few ways to implement that\nin Kotlin\n\n### Data classes\n\nLet’s start with an idiomatic Kotlin way. The class hierarchy can be\nconcisely represented as\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **data**\n\n\u003c/div\u003e\n\n``` kotlin\ndata class Transaction(val payment: Payment, val parts: Parts)\ndata class Payment(val currency: String, val amount: Int)\ndata class Parts(val from: Person, val to: Person)\ndata class Person(val id: Int, val name: String)\n```\n\nAn instance of the `Transaction` can easily be created as well\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **create**\n\n\u003c/div\u003e\n\n``` kotlin\nval trs = Transaction(\n  Payment(\"AUD\", 15),\n  Parts(\n    Person(0, \"Alex\"),\n    Person(1, \"Ben\")\n  )\n)\n```\n\nBut problems start when we need to update this nested data structure.\nGenerally, there two ways to do that. The first option is to completely\nrecreate the transaction which doesn’t look good.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **update \\[1\\]**\n\n\u003c/div\u003e\n\n``` kotlin\nval trans = Transaction(trs.payment, Parts(\n  Person(trs.parts.from.id, \"John\"),\n  trs.parts.to)\n)\n```\n\nAnother is to use\n[copy](https://kotlinlang.org/docs/reference/data-classes.html#copying)\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **update \\[2\\]**\n\n\u003c/div\u003e\n\n``` kotlin\nval stansTrs2 = trs.copy(\n  parts = trs.parts.copy(\n    from = trs.parts.from.copy(\n      name = \"John\"\n    )\n  )\n)\n```\n\nAnd the copy version doesn’t look good either. Even though it’s\ntolerable now, the bigger the data structure, the uglier the code look\nlike. On a deeply nested immutable data structure, it looks like a\ntriangle instead of a simple call chain from the mutable world.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **ohhhh**\n\n\u003c/div\u003e\n\n``` kotlin\nval stansTrs2 = trs.copy(\n  parts = trs.parts.copy(\n    from = trs.parts.from.copy(\n      person = trs.parts.from.person.copy(\n        parts = trs.parts.from.person.parts.copy(\n          from = trs.parts.from.person.parts.from.copy(\n            person = trs.parts.from.person.parts.from.person.copy(\n              parts = trs.parts.from.person.parts.from.person.parts.copy(\n                from = trs.parts.from.person.parts.from.person.parts.from.copy(\n                  person = trs.parts.from.person.parts.from.person.parts.from.person.copy(\n                    parts = trs.parts.from.person.parts.from.person.parts.from.person.parts.copy(\n                      from = trs.parts.from.person.parts.from.person.parts.from.person.parts.from.copy(\n                        name = \"jonh\"\n                      ))))))))))))\n```\n\nDon’t get me wrong, I like parentheses. It feels like a lisp (which I\nlike a lot), but what no one likes is the wall of boilerplate above.\n\n### Persistent Data Structures\n\nBut talking about lisps, there is another awesome language called\nClojure. It’s a lisp running on JVM where every data structure is\npersistent (don’t confuse with\n[immutable](https://stackoverflow.com/questions/10034537/persistent-vs-immutable-data-structure)).\nIn Clojure, the same problem can be solved by defining the transaction\nstructure as a persistent map.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **create**\n\n\u003c/div\u003e\n\n``` clojure\n(def ts {:payment {:currency \"AUD\"\n                   :amount   15}\n         :parts   {:from {:id   0\n                          :name \"Alex\"}\n                   :to   {:id   1\n                          :name \"Ben\"}}})\n```\n\nNot as concise as Kotlin’s version, but still pretty good. What is\ncompletely different to Kotlin, is the update function\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **update**\n\n\u003c/div\u003e\n\n``` clojure\n(def ts2 (assoc-in ts [:parts :from :name] \"John\"))\n```\n\nIt’s only one line! And it’s exactly what we aimed for. The next picture\nmight be essential for understanding how it works.\n\n![](/images/fantastic/domain_clj.svg)\n\nGiven that each node has a known type - `clojure.lang.APersistentMap` -\nand the universal way of traversing is `map.get(\"key\")`, it’s possible\nto write a function `assoc-in` which can change a value under a given\n\"path\" and to recreate the data structure [node by\nnode](http://cjohansen.no/clojure-to-die-for/). But Clojure’s internals\nare plain java classes that can be used from Kotlin easily just with a\nfew \"convenience\" adapters to keep familiar syntax.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **create**\n\n\u003c/div\u003e\n\n``` kotlin\nval tran = pArrayMap(\n  \"payment\" to pArrayMap(\n    \"currency\" to \"AUD\",\n    \"amount\" to 15\n  ),\n  \"parts\" to pArrayMap(\n    \"from\" to pArrayMap(\n      \"id\" to 0,\n      \"name\" to \"Alex\"\n    ),\n    \"to\" to pArrayMap(\n      \"id\" to 1,\n      \"name\" to \"Ben\"\n    )\n  )\n)\n```\n\nYes, the creation looks rather ugly. It’s untyped, all the key names are\nrepresented as strings, but let’s look at the update function.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **update**\n\n\u003c/div\u003e\n\n``` kotlin\nval trans2 = trans.pUpdate(listOf(\"parts\", \"from\", \"name\"), \"John\")\n```\n\nIt’s still as concise and beautiful as Clojure’s one.\n\nBut is it possible to build a DSL which keeps types from Kotlin types\nand provides the conciseness of Clojure?\n\n## Cursor DSL\n\nIt is possible! Using a special DSL, you can define the structure of the\n\"transactional\" domain in a following way.\n\n``` kotlin\ninterface Transaction\nval \u003cF\u003e Cursor\u003cTransaction, F\u003e.payment by Node\u003cPayment\u003e()\nval \u003cF\u003e Cursor\u003cTransaction, F\u003e.parts by Node\u003cParts\u003e()\n\ninterface Payment\nval \u003cF\u003e Cursor\u003cPayment, F\u003e.currency by Leaf\u003cString\u003e()\nval \u003cF\u003e Cursor\u003cPayment, F\u003e.amount by Leaf\u003cInt\u003e()\n\ninterface Parts\nval \u003cF\u003e Cursor\u003cParts, F\u003e.to by Node\u003cPerson\u003e()\nval \u003cF\u003e Cursor\u003cParts, F\u003e.from by Node\u003cPerson\u003e()\n\ninterface Person\nval \u003cF\u003e Cursor\u003cPerson, F\u003e.id by Leaf\u003cInt\u003e()\nval \u003cF\u003e Cursor\u003cPerson, F\u003e.name by Leaf\u003cString\u003e()\n```\n\nThis looks scary, but it’s just a bit of necessary boilerplate. This\ncode should be read like\n\n\u003cpre style={{margin: 0, background: \"white\", lineHeight: \"125%\"}}\u003e\n\u003cstyle\u003e{`\n.prh-keyword {\n  color: #000080;\n  font-weight: bold;\n}\n.prh-boilerplate {\n  opacity: 0.2;\n}\n`}\u003c/style\u003e\n\u003cspan class=\"prh-keyword\"\u003einterface\u003c/span\u003e Transaction\n\u003cspan class=\"prh-keyword\"\u003eval\u003c/span\u003e\u003cspan class=\"prh-boilerplate\"\u003e \u0026lt;F\u0026gt; Cursor\u0026lt;\u003c/span\u003eTransaction\u003cspan class=\"prh-boilerplate\"\u003e, F\u0026gt;\u003c/span\u003e.payment \u003cspan class=\"prh-keyword\"\u003eby\u003c/span\u003e \u003cspan class=\"prh-boilerplate\"\u003eNode\u0026lt;\u003c/span\u003ePayment\u003cspan class=\"prh-boilerplate\"\u003e\u0026gt;()\u003c/span\u003e\n\u003cspan class=\"prh-keyword\"\u003eval\u003c/span\u003e\u003cspan class=\"prh-boilerplate\"\u003e \u0026lt;F\u0026gt; Cursor\u0026lt;\u003c/span\u003eTransaction\u003cspan class=\"prh-boilerplate\"\u003e, F\u0026gt;\u003c/span\u003e.parts \u003cspan class=\"prh-keyword\"\u003eby\u003c/span\u003e \u003cspan class=\"prh-boilerplate\"\u003eNode\u0026lt;\u003c/span\u003eParts\u003cspan class=\"prh-boilerplate\"\u003e\u0026gt;()\u003c/span\u003e\n\n\u003cspan class=\"prh-keyword\"\u003einterface\u003c/span\u003e Payment\n\u003cspan class=\"prh-keyword\"\u003eval\u003c/span\u003e\u003cspan class=\"prh-boilerplate\"\u003e \u0026lt;F\u0026gt; Cursor\u0026lt;\u003c/span\u003ePayment\u003cspan class=\"prh-boilerplate\"\u003e, F\u0026gt;\u003c/span\u003e.currency \u003cspan class=\"prh-keyword\"\u003eby\u003c/span\u003e \u003cspan class=\"prh-boilerplate\"\u003eLeaf\u0026lt;\u003c/span\u003eString\u003cspan class=\"prh-boilerplate\"\u003e\u0026gt;()\u003c/span\u003e\n\u003cspan class=\"prh-keyword\"\u003eval\u003c/span\u003e\u003cspan class=\"prh-boilerplate\"\u003e \u0026lt;F\u0026gt; Cursor\u0026lt;\u003c/span\u003ePayment\u003cspan class=\"prh-boilerplate\"\u003e, F\u0026gt;\u003c/span\u003e.amount \u003cspan class=\"prh-keyword\"\u003eby\u003c/span\u003e \u003cspan class=\"prh-boilerplate\"\u003eLeaf\u0026lt;\u003c/span\u003eInt\u003cspan class=\"prh-boilerplate\"\u003e\u0026gt;()\u003c/span\u003e\n\n\u003cspan class=\"prh-keyword\"\u003einterface\u003c/span\u003e Parts\n\u003cspan class=\"prh-keyword\"\u003eval\u003c/span\u003e\u003cspan class=\"prh-boilerplate\"\u003e \u0026lt;F\u0026gt; Cursor\u0026lt;\u003c/span\u003eParts\u003cspan class=\"prh-boilerplate\"\u003e, F\u0026gt;\u003c/span\u003e.to \u003cspan class=\"prh-keyword\"\u003eby\u003c/span\u003e \u003cspan class=\"prh-boilerplate\"\u003eNode\u0026lt;\u003c/span\u003ePerson\u003cspan class=\"prh-boilerplate\"\u003e\u0026gt;()\u003c/span\u003e\n\u003cspan class=\"prh-keyword\"\u003eval\u003c/span\u003e\u003cspan class=\"prh-boilerplate\"\u003e \u0026lt;F\u0026gt; Cursor\u0026lt;\u003c/span\u003eParts\u003cspan class=\"prh-boilerplate\"\u003e, F\u0026gt;\u003c/span\u003e.from \u003cspan class=\"prh-keyword\"\u003eby\u003c/span\u003e \u003cspan class=\"prh-boilerplate\"\u003eNode\u0026lt;\u003c/span\u003ePerson\u003cspan class=\"prh-boilerplate\"\u003e\u0026gt;()\u003c/span\u003e\n\n\u003cspan class=\"prh-keyword\"\u003einterface\u003c/span\u003e Person\n\u003cspan class=\"prh-keyword\"\u003eval\u003c/span\u003e\u003cspan class=\"prh-boilerplate\"\u003e \u0026lt;F\u0026gt; Cursor\u0026lt;\u003c/span\u003ePerson\u003cspan class=\"prh-boilerplate\"\u003e, F\u0026gt;\u003c/span\u003e.id \u003cspan class=\"prh-keyword\"\u003eby\u003c/span\u003e \u003cspan class=\"prh-boilerplate\"\u003eLeaf\u0026lt;\u003c/span\u003eInt\u003cspan class=\"prh-boilerplate\"\u003e\u0026gt;()\u003c/span\u003e\n\u003cspan class=\"prh-keyword\"\u003eval\u003c/span\u003e\u003cspan class=\"prh-boilerplate\"\u003e \u0026lt;F\u0026gt; Cursor\u0026lt;\u003c/span\u003ePerson\u003cspan class=\"prh-boilerplate\"\u003e, F\u0026gt;\u003c/span\u003e.name \u003cspan class=\"prh-keyword\"\u003eby\u003c/span\u003e \u003cspan class=\"prh-boilerplate\"\u003eLeaf\u0026lt;\u003c/span\u003eString\u003cspan class=\"prh-boilerplate\"\u003e\u0026gt;()\u003c/span\u003e\n\u003c/pre\u003e\n\n\nThe creation looks very similar to the untyped version, but it’s\ncompletely typed. It references properties defined above.\n\n``` kotlin\nval trans = domain\u003cTransaction\u003e {\n  (payment) {\n    currency.set(\"AUD\")\n    amount.set(15)\n  }\n  (parts) {\n    (from) {\n      id.set(0)\n      name.set(\"Alex\")\n    }\n    (to) {\n      id.set(1)\n      name.set(\"Ben\")\n    }\n  }\n}\n```\n\nIt’s possible to update the transaction easily. And not just one field,\nin fact, the code above creates an empty data structure and applies an\nupdate function to it.\n\n``` kotlin\nval trans2 = trans.cursor.parts.from.update {\n  name.set(\"John\")\n}\nprintln(trans.cursor.parts.from.name.value) // \"Alex\"\nprintln(trans2.cursor.parts.from.name.value) // \"John\"\n```\n\nor\n\n``` kotlin\nval trans3 = trans2.cursor.update {\n  (payment) {\n    currency.set(\"USD\")\n    amount.set(12)\n  }\n}\n```\n\nWhat is really awesome is that the `set` function can only be called\ninside the `update` block. It’s possible to think about the `update`\nblock as an open transaction where a few updates are applied.\n\n### Implementation\n\n#### Read\n\nThe easiest way to start implementing it is to imagine that the data\nstructure is already created and everything we need to do is to read a\nvalue from it. The obvious untyped solution will be to call\n`trans.get(\"parts\").get(\"from\").get(\"name\")`. And this approach works\nfine until we need to update it. After the first `get` call, the\nreference to the root transaction is lost and there’ll be no way to run\nthe update operation.\n\nInstead, it’s possible to focus on the way of traversing the data\nstructure without loosing the reference to the root. To accomplish this,\nit’s possible to implement `Focus` interface which holds the reference\nto the root and accumulates a path inside.\n\n``` kotlin\ninterface Focus\u003cout Op\u003e {\n  fun narrow(k: String): Focus\u003cOp\u003e\n  val op: Op\n}\n```\n\nThe interesting thing that `Focus` is parametrised over an operation.\nThat operation can be `Read` or `Write` depending on the context. When a\nleaf is reached, the typed version will finally perform an action using\nthat operation.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **narrow down the usage**\n\n\u003c/div\u003e\n\n``` kotlin\nval f = Focus(trans)       // {\"root\" -\u003e Transaction, path -\u003e []}\nval f2 = f.narrow(\"parts\") // {\"root\" -\u003e Transaction, path -\u003e [\"parts\"]}\nval f3 = f2.narrow(\"from\") // {\"root\" -\u003e Transaction, path -\u003e [\"parts\", \"from\"]}\n// ...\n```\n\n![](/images/fantastic/domain_focus.svg)\n\nBut even though the focus does its job very well, it’s completely\nuntyped, and strings have to be used to navigate through. The type must\nbe stored somewhere. As everyone knows that any problem can be solved\nwith an additional layer of abstraction! Let’s define a wrapper\nparametrised over the type of an underlying node.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **the missed layer**\n\n\u003c/div\u003e\n\n``` kotlin\nclass Cursor\u003cout T, out Op\u003e(val f: Focus\u003cOp\u003e)\n```\n\n`Cursor` is parametrised over a node type and the `operation` is derived\nfrom the focus. And now, the `Transaction` definition starts making\nsense. The narrowing can be delegated to the `Node` object that knows\nthe type and uses the name of a property to create a new `Cursor` with a\nnew `Focus` inside.\n\n``` kotlin\ninterface Transaction\nval \u003cF\u003e Cursor\u003cTransaction, F\u003e.payment by Node\u003cPayment\u003e()\n```\n\nHere, the `payment` is an extension property on the `Transaction` type\nwhich is just a marker interface. It will never be instantiated, instead\nby delegating property to `Node\u003cPayment\u003e`, the conversion\n`Cursor\u003cTransacton, F\u003e =\u003e Cursor\u003cPayment, F\u003e` will be made.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **how Node is defined**\n\n\u003c/div\u003e\n\n``` kotlin\nopen class Node\u003cout T\u003e {\n  open operator fun \u003cOp\u003e getValue(ref: Cursor\u003c*, Op\u003e, property: KProperty\u003c*\u003e): Cursor\u003cT, Op\u003e {\n    return Cursor(ref.f.narrow(property.name))\n  }\n}\n```\n\nInside `Node`, a new Cursor is created with the focus narrowing down\nusing a property name. Using this technique, by just calling extension\nproperties a focus can narrow down to the last node where the last node\nis delegated to `Leaf` instead of `Node`.\n\n``` kotlin\ninterface Person\nval \u003cF\u003e Cursor\u003cPerson, F\u003e.name by Leaf\u003cString\u003e()\n```\n\n`Leaf\u003cV\u003e` is defined in the same way as Node except for the return value\nof `getValue`.\n\n``` kotlin\nopen class Leaf\u003cout V\u003e {\n  open operator fun \u003cOp\u003e getValue(ref: Cursor\u003c*, Op\u003e, property: KProperty\u003c*\u003e): Cursor\u003cLeaf\u003cV\u003e, Op\u003e {\n    return Cursor(ref.f.narrow(property.name))\n  }\n}\n```\n\nLeaf is needed to define an extension property that allows reading a\nvalue from that node. The property has the following signature\n`val \u003cV, T\u003e Cursor\u003cLeaf\u003cV\u003e, Read\u003cT\u003e\u003e.value: V` which says: given the\ncursor focused on a leaf and parametrised over a read operation, provide\na value contained by the leaf.\n\n![](/images/fantastic/domain_red.png)\n\nThe remaining logic is described below\n\n``` kotlin\n// the main data structure where T type - is the root type\n// in our case, T is Transaction.\n// root is just an empty persisntent map\nclass Domain\u003cout T\u003e(val root: PMap = PHashMap.EMPTY)\n\n// The read operation that focus owns (Op)\ninterface Read\u003cout M\u003e {\n  val path: Path         // path to the current node (ex. [\"payment\", \"currency\"])\n  val domain: Domain\u003cM\u003e  // the reference to the root\n}\n\n// the implementation of the focus\nclass Reader\u003cout T\u003e(val p: Path, val dm: Domain\u003cT\u003e) : Focus\u003cRead\u003cT\u003e\u003e {\n  // this is how narrowing happens, just extend the path and keep the refernce to the root\n  override fun narrow(k: String): Focus\u003cRead\u003cT\u003e\u003e = Reader(p.append(k), dm)\n\n  override val op: Read\u003cT\u003e = object : Read\u003cT\u003e {\n    override val domain: Domain\u003cT\u003e = dm\n    override val path: Path = p\n  }\n}\n\n// take a focus, take a read operation from it and ask for value\n// by traversing the root using path\nval \u003cV, T\u003e Cursor\u003cLeaf\u003cV\u003e, Read\u003cT\u003e\u003e.value: V\n  get() = f.op.path.getIn(f.op.domain.root) as V\n\n// this is how cursor get's created, emtpy path and reference to the root\nval \u003cT\u003e Domain\u003cT\u003e.cursor: Cursor\u003cT, Read\u003cT\u003e\u003e\n  get() = Cursor(Reader(Path.EMPTY, this))\n```\n\n#### Update\n\nSo far we can traverse the data structure and read values from it. The\nnext step is to learn how to update it. Problems start when we realise\nthat the underlying data structure is persistent and there is no way to\nmutate it. To emulate mutation, a special wrapper has to be defined. It\nreassigns the reference after each mutation.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **immutable ⇒ mutable**\n\n\u003c/div\u003e\n\n``` kotlin\nclass Mutable(var m: PMap) {\n  fun write(p: Path, a: Any?) {\n    m = p.assocIn(m, a)\n  }\n\n  fun read(p: Path) = p.getIn(m)\n}\n```\n\nThen, we’ll need to implement the `Write` operation which supports\nreading and writing under a specific path. At first glance, `read`\noperation is unnecessary, but it’s needed to read the final result after\nall modification were applied using an empty path. Another application\nof the `read()` operation is node initialisation. E.g. if you create an\nempty domain and decide to write a value to leaf using a cursor, all the\nparent nodes need to be initialised first.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **Op for `Cursor\u003cT, Write\u003e`**\n\n\u003c/div\u003e\n\n``` kotlin\ninterface Write {\n  fun read(): Any?\n  fun write(a: Any?)\n}\n```\n\nAnd the corresponding cursor\n\n``` kotlin\nclass WriterCursor(val m: Mutable, val path: Path) : Focus\u003cWrite\u003e {\n  // exactly the same narrowing pattern\n  override fun narrow(k: String): Focus\u003cWrite\u003e = WriterCursor(m, path.append(k))\n\n  override val op: Write = object : Write {\n    override fun write(a: Any?) = m.write(path, a)\n    override fun read(): Any? = m.read(path)\n  }\n}\n```\n\nAnd at some point in time, we might want to switch from the `Read`\ncursor to the `Write` cursor. For that, a special function exists.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    **Cursor\\\u003cT, Read\\\u003e ⇒ Cursor\\\u003cT, Write\\\u003e**\n\n\u003c/div\u003e\n\n``` kotlin\nfun \u003cT, M\u003e Cursor\u003cM, Read\u003cT\u003e\u003e.update(update: Cursor\u003cM, Write\u003e.() -\u003e Unit): Domain\u003cT\u003e {\n  // take a root, make a mutable from it\n  val m = Mutable(f.op.domain.root)\n  // create a writer from mutable and apply `update` supplied from outside\n  // exactly the same pattern as any other DSL has\n  Cursor\u003cM, Write\u003e(WriterCursor(m, f.op.path)).update()\n  // read the final value from the root and return a new instance of Domain\n  return Domain(m.read(Path.EMPTY) as PMap)\n}\n\n// to simplify the initialisation\nfun \u003cM\u003e domain(f: Cursor\u003cM, Write\u003e.() -\u003e Unit) = Domain\u003cM\u003e().cursor.update(f)\n```\n\nAnd finally, a set of public typed operation that API consumers use\n\n``` kotlin\n// for each leaf initial value is null\n// for each node initial value is empty persistent map\nfun Write.init(k: KClass\u003c*\u003e) {\n  if (read() == null) {\n    write(when (k) {\n      Leaf::class -\u003e null\n      else -\u003e PArrayMap.EMPTY\n    })\n  }\n}\n\noperator inline fun \u003creified T\u003e Cursor\u003cT, Write\u003e.invoke(\n    updateFn: Cursor\u003cT, Write\u003e.() -\u003e Unit): Unit {\n  // init the current node (it might be null if we haven't visited it before)\n  f.op.init(T::class)\n  updateFn()\n}\n\nfun \u003cT\u003e Cursor\u003cLeaf\u003cT\u003e, Write\u003e.set(t: T): Unit {\n  // just delegate to write\n  f.op.write(t)\n}\n```\n\nThe `invoke` function is responsible for Node initialisation whereas\n`set` sets the Leaf’s value\n\n``` kotlin\ndomain\u003cTransaction\u003e {\n  (payment) {  // \u003c- here invoke is called\n    currency.set(\"AUD\")\n    amount.set(15)\n  }\n}\n\n// ↑ is equal to the desugarised version ↓\ndomain\u003cTransaction\u003e {\n  payment.invoke({\n    currency.set(\"AUD\")\n    amount.set(15)\n  })\n}\n```\n\nAnd at the end, a Path that does all the work, but in fact, it does\nnothing except for delegating functionality to functions from Clojure\nthat do all the work on untyped persistent data structures.\n\n``` kotlin\nimport clojure.`core$assoc_in` as assocIn\nimport clojure.`core$get_in` as getIn\nimport clojure.lang.*\n\ndata class Path(private val v: APersistentVector) {\n  companion object {\n    val EMPTY = Path(PersistentVector.EMPTY)\n  }\n\n  fun append(a: String): Path = Path(v.cons(a) as APersistentVector)\n  fun getIn(model: Any?): Any? = getIn.invokeStatic(model, v)\n  fun assocIn(m: Any?, a: Any?): Any? = assocIn.invokeStatic(m, v, a)\n}\n```\n\nUsing these primitives, we built a really powerful type safe DSL to work\non immutable data structures. Yes, it has a few downsides. E.g. data\nclasses solution has better performance. And most of the time it’s\nconcise enough, unless you have a really deeply nested tree. In that\ncase, you might also try to use [the lenses\npattern](https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/basic-lensing)\nwhich comes from the functional world and solves the same problem. But\nif you already have untyped data structures in your project and have to\nwork with them, Kotlin provides a truly unique set of features that\nallows you to build a powerful DSL to make your life safer and easier.\n\nIt’s very probable that some parts of the solution shown above might\nstill be unclear, in that case, I encourage you to clone [the code\nexample](https://github.com/SerCeMan/talk-fantastic-dsls-example) in\nyour IDE, run it and try to play with types. It will help a lot and can\ngive you some interesting ideas on how advanced Kotlin features can be\nused.\n\n## Conclusions\n\n- Kotlin provides many unique features to build DSLs easily\n\n- DSLs in Kotlin work best as configuration APIs\n\n- They can be a powerful abstraction over untyped data structures\n\n### Warnings\n\n- Most of the time plain code is better than DSL\nThere is no point in building DSL \"just because I can\", plain Kotlin\ncode is often much easier to read and understand.\n\n- Provide a way to extend and bypass your DSL\nIf you publish DSL as a part of your API, it’s always a good idea to\ngive a way to bypass or extend it. Of course, if it’s a Gradle-like\nDSL then you can cover everything. But in the case of a html DSL, a\nuser might want to introduce some tags that your DSL doesn’t support.\nOr, he can have an already rendered string which needs to be inserted\nsomewhere.\n\n## Links\n\n- [Cursor DSL source\ncode](https://github.com/SerCeMan/talk-fantastic-dsls-example)\n\n- Why you should use DSLs: [Building DSL Instead of an IDE\nPlugin](http://jonnyzzz.com/blog/2016/09/02/dsl-building/)\n\n- Why you shouldn’t: [DSLs in Kotlin: The Good, the Bad and the\nUgly](https://victor.kropp.name/blog/kotlin-dsls-good-bad-and-ugly/)\n\n## Thanks\n\n- Kotlin team for creating an awesome language!\nPlease, press a ★ button on the [Kotlin’s GitHub\nrepo](https://github.com/JetBrains/kotlin) if you haven’t done it yet.\n\n- [@JetZajac](https://twitter.com/jetzajac) who initially came up with\nthe idea of persistent data structure based DSLs\n\n- You for reading it\n\n## Share this article\n\n* [Twitter](https://twitter.com/SerCeMan/status/880365305314254848)\n* [Hacker News](https://news.ycombinator.com/item?id=14663115)\n","description":"Empty","slug":"29-06-2017-fantastic-dsls","kind":"mdx"},{"title":"Pure assembly in the forest of Panama","date":"2016-06-01","content":"\nHi!\n\nIn this article, I’ll tell you about some internal features of Project\nPanama. You’ll find out how to increase the performance of your Java\nprogram using a pure inline assembler.\n\n\u003cdiv style={{textAlign: \"center\"}}\u003e\n![](/images/wild-panama/panama.jpg)\n\u003c/div\u003e\n\n\u003cQuote\n    quote=\"We had two builds of jvm, seventy-five native functions, five sheets\nof high-powered method handles, a Panama repository full of crazy\nfeatures, and a whole galaxy of native data layouts, headers,\ncompilers, optimizations… and also a quart of heap, a case of\nwrappers, a pint of raw memory and two dozen AVX2 instructions. \u003cbr/\u003e\nNot that we needed all that for the trip to Panama, but once you get\nlocked into a serious jvm crash collection, the tendency is to push it\nas far as you can.\"\n/\u003e\n\n| ❗ | This article is written mostly about something that may never be released \u003cbr/\u003eAbout API that might never be seen \u003cbr/\u003eAbout code you shouldn’t use in production |\n|----|---|\n\n| ⚠️ | A lot of information in this article is based on my personal experiments. with the internal state of Panama forest in June 2016, so it may be deprecated when you are reading it. |\n|----|---|\n\n\nSo, let’s begin our journey.\n\n## Welcome to Panama\n\n[Panama](http://openjdk.java.net/projects/panama/) is a new project\nunder OpenJDK that tries to improve the connection between JVM and\nforeign APIs, including many interfaces commonly used by C programmers.\nIt is the missing piece in the Java ecosystem, a bridge between JAVA and\nnative code.\n\nThe primary features that will be introduced in Project Panama are:\n\n- Native function calling and data access, respectfully, with huge JIT\nsupport (see [JEP191](http://openjdk.java.net/jeps/191))\n(Similar problems but without huge runtime support can be solved using\nJNR as explained here [previous article](/posts/22-06-2015-jnr-fuse/))\n\n- New data layouts\n\n- Special tools for wrapping native libraries\n\nThe full overview of the problems that Panama tries to solve can be\nfound here: [blog post](https://blogs.oracle.com/jrose/entry/the_isthmus_in_the_vm)\n(written by John Rose). But some features in the mercurial forest of\nProject Panama don’t really belong to JEP 191. These features are Vector\nAPI and Machine Code Snippets.\n\nLast December, Vladimir Ivanov, one of the core contributors of Panama\nproject made a commit where he introduced an ability to call a snippet\nof machine code in runtime…\n\n\u003cdiv\u003e\n\u003cblockquote class=\"twitter-tweet\" data-lang=\"en\"\u003e\n    \u0026mdash; Vladimir Ivanov (@iwan0www) \u003ca href=\"https://twitter.com/iwan0www/status/672824680227708928\"\u003eDecember 4, 2015\u003c/a\u003e\n\u003c/blockquote\u003e\n\u003cscript async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\u003c/div\u003e\n\nThis is an amazing feature, you can make an\ninline assembler call, crazy stuff… It’s like the new Unsafe, but even\ncooler! It’s like writing your own intrinsic, but in runtime. In this\npost I’ll be primarily focused on Machine Code Snippets. So let’s\nexplore this opportunity.\n\n## The edge of the forest\n\nThe first program that every programmer writes in a new language is\n\"Hello, World!\". But it’s assembler, and it is called from Java. So\nlet’s make it simple.\nFor example, an A+B+C function looks like this in each:\n\n\u003cdiv class=\"formalpara-title\"\u003e\nPlain Java\n\u003c/div\u003e\n``` java\npublic static int sum(int a, int b, int c) {\n    return a + b + c;\n}\n```\n\n\u003cdiv class=\"formalpara-title\"\u003e\nX86 assembly\n\u003c/div\u003e\n\n```x86asm\n...\nmov rax, rsi ; res = arg1\nadd rax, rdi ; res += arg2\nadd rax, rdx ; res += arg3\n...\n```\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    CodeSnippet\n\n\u003c/div\u003e\n\n```java\nstatic final MethodHandle sum3 = jdk.internal.panama.CodeSnippet.make(\n            \"sum3\", MethodType.methodType(int.class,/*result*/\n                                          int.class /*rdi*/,\n                                          int.class /*rsi*/,\n                                          int.class /*rdx*/),\n            true, /* isSupported */\n            0x48, 0x89, 0xF0, // mov    rax,rsi\n            0x48, 0x01, 0xF8, // add    rax,rdi\n            0x48, 0x01, 0xD0  // add    rax,rdx\n    );\n```\n\nHere we used `jdk.internal.panama.CodeSnippet` class to get MethodHandle\nto native code. And yes, this package is functionally important, it\nactually means internal API, so you very probably won’t be able to use\nit.\nAs an\n[arguments](http://hg.openjdk.java.net/panama/panama/hotspot/file/6818b4b2e922/src/cpu/x86/vm/sharedRuntime_x86_64.cpp#l1141)\nof `MethodType#methodType` you can pass primitives and some special\nclasses like `Long2` (128 bit register), `Long4` (256 bit register) and\n`Long8` (512 bit register).\n\nBased on what you’ve seen above, you could say that we were able to use\nJNI before, so what’s the point of using inline ASM? This is true, but\nthe thing is the C2 compiler can easily inline the code snippet. So, it\ngives you an opportunity (if you’re crazy enough) to write your own JVM\nintrinsic without coding it in the JVM.\n\nLet’s compare assembly produced by the JVM after compiling and inlining\nfor every method.\n\n\u003ctable className=\"fit-table\"\u003e\n    \u003ccolgroup\u003e\n        \u003ccol style={{width: \"33%\"}}/\u003e\n        \u003ccol style={{width: \"33%\"}}/\u003e\n        \u003ccol style={{width: \"33%\"}}/\u003e\n    \u003c/colgroup\u003e\n    \u003cthead\u003e\n    \u003ctr class=\"header\"\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003ePlain Java\u003c/th\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003eCodeSnippet ASM\u003c/th\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003eJNI\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n    \u003ctr class=\"odd\"\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"x86asm\"\u003e\n```x86asm\n[Verified Entry Point]\n sub  rsp,0x18\n mov  QWORD PTR [rsp+0x10],rbp  ;*synch entry\n\njitresult:\n mov  eax,DWORD PTR [rsi+0x1c]\n add  eax,DWORD PTR [rsi+0x18]\n add  eax,DWORD PTR [rsi+0x20]  ;*iadd\n\n\nexit:\n add  rsp,0x10\n pop  rbp\n test DWORD PTR [rip+0x15b4ea60],eax\n```\n\u003c/pre\u003e\n        \u003c/td\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"x86asm\"\u003e\n\n```x86asm\n[Verified Entry Point]\n sub  rsp,0x18\n mov  QWORD PTR [rsp+0x10],rbp;*sync entry\n mov  r10,rsi\n mov  esi,DWORD PTR [rsi+0x1c] ;*field b\n mov  edx,DWORD PTR [r10+0x20] ;*field c\n mov  edi,DWORD PTR [r10+0x18] ;*field a\n\nsnippet:\n mov  rax,rsi\n add  rax,rdi\n add  rax,rdx\n\nexit:\n add  rsp,0x10\n pop  rbp\n test DWORD PTR [rip+0x16d21852],eax\n```\n        \u003c/pre\u003e\n        \u003c/td\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"x86asm\"\u003e\n```x86asm\n[Verified Entry Point]\n mov  DWORD PTR [rsp-0x14000],eax\n push rbp\n sub  rsp,0x10           ;*sync entry\n\n mov  edx,DWORD PTR [rsi+0x1c]  ;*field b\n mov  ecx,DWORD PTR [rsi+0x20]  ;*field c\n mov  esi,DWORD PTR [rsi+0x18]  ;*field a\n\nnative_call:\n xchg ax,ax\n call 0x00007f7ab5668738\n\nexit:\n add  rsp,0x10\n pop  rbp\n test DWORD PTR [rip+0x166add39],eax\n ret  ;*invokestatic s_nat\n\nruntime_call_rethrow_Java:\n mov    rsi,rax\n add    rsp,0x10\n pop    rbp\n jmp    0x00007f7aadc7b6e0\n```\n        \u003c/pre\u003e\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\nAs you can see here the only difference between the C2 JIT version and\nour CodeSnippet is the movement of arguments between registers to\nsatisfy calling convention. And the C2 perfectly inlined exactly the\nsame piece of code as shown above. At the same time, JNI performs a real\nnative call.\n\nBut what’s the point of writing inline asm snippets in Java? Usually\nthere is no reason to do so, the C2 is able to compile your code into\nsomething that works much faster. But there are several things that the\nC2 can’t do efficiently. The most important is that the C2 can’t rewrite\nyour algorithm using SIMD operations yet.\n\n## Go deeper to hidden places\n\nUsually our applications are not about A+B+C functions, but about some\nreal code. And our applications can contain, say, the function that\ncalculates checksums of buffers. A perfectly real task, that you can\nencounter in different kinds of software.\n\nLet’s imagine our application has a little function called checksum that\nmakes a sum of bytes in the buffer and gives us hash \\[0, 256) as a\nresult.\n\nHere’s the code:\n\n``` java\nprivate static int checksumPlainJava(ByteBuffer buffer, int size) {\n    int checksum = 0;\n    for (int i = 0; i \u003c size; ++i) {\n        checksum += buffer.get(i);\n    }\n    // make it unsigned first to avoid negative result\n    return (int) (Integer.toUnsignedLong(checksum) % 256);\n}\n```\n\nIn our application we operate big byte buffers and we have to calculate\nchecksums very often. We discovered that this checksum function is our\nbottleneck. And we need to optimize it. What options do we have?\n\n### JNI\n\nYou may see on the last line the ugly operation where we are trying to\nconvert our signed int to unsigned to get the proper result. Of course,\nit’s the bottleneck you might think, isn’t it? The cool C++ has unsigned\nvariables - let’s make a JNI call!\n\nOk, here we go, C++ code:\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    JNI checksum\n\n\u003c/div\u003e\n\n``` cpp\nJNIEXPORT jint JNICALL Java_me_serce_panex_ChecksumBenchmark_nativePlainChecksum\n    (JNIEnv * env, jclass clz, jlong addr, jint targetLength) {\n    char *target = reinterpret_cast\u003cchar *\u003e(addr);\n    unsigned int checksum = 0;\n    for (int i = 0; i \u003c targetLength; ++i) {\n        checksum += (unsigned int) target[i];\n    }\n    return checksum % 256;\n}\n```\n\nNow we have to check the performance. We may expect incredible results.\nFor performance measurement we will be using\n[JMH](http://openjdk.java.net/projects/code-tools/jmh/), the de-facto\nstandard in Java benchmarking. You can find a great deal of articles\nanswering the question \"why JMH?\" on the internet.\n\nThere is no way to get a native memory address for DirectByteBuffer, so\nwe are using reflection trick here to get the field that contains this\naddress. Now we’re able to access memory from C++ code directly. We’re\nchecking how fast the function is in case of *4*/*8096*/*129536* size\nbuffers.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Benchmark setup\n\n\u003c/div\u003e\n\n``` java\nprivate ByteBuffer buffer;\nprivate long address = 0;\n\n@Param({\"4\", \"8096\", \"129536\"})\nprivate int size = 4;\n\npublic static long getAddress(ByteBuffer buffy) throws Throwable {\n    Field address = Buffer.class.getDeclaredField(\"address\");\n    address.setAccessible(true);\n    return address.getLong(buffy);\n}\n\n@Setup\npublic void setup() throws Throwable {\n    buffer = ByteBuffer.allocateDirect(size).order(ByteOrder.nativeOrder());\n    ThreadLocalRandom random = ThreadLocalRandom.current();\n    for (int i = 0; i \u003c size / 4; i++) {\n        buffer.putInt(random.nextInt());\n    }\n    address = getAddress(buffer);\n}\n```\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    And the results\n\n\u003c/div\u003e\n```\nBenchmark                       (size)  Mode  Cnt   Score    Error  Units\nChecksumBenchmark.JNI_Checksum       4  avgt    3   0.009 ±  0.001  us/op\nChecksumBenchmark.JNI_Checksum    8096  avgt    3   3.085 ±  0.039  us/op\nChecksumBenchmark.JNI_Checksum  129536  avgt    3  48.879 ±  5.655  us/op\nChecksumBenchmark.plainJava          4  avgt    3   0.006 ±  0.001  us/op\nChecksumBenchmark.plainJava       8096  avgt    3   2.190 ±  0.834  us/op\nChecksumBenchmark.plainJava     129536  avgt    3  34.452 ±  3.341  us/op\n```\nAs you can see, the JNI loop is slower. But what happened? Could it mean\nthat JNI is really slow? As we saw earlier CodeSnippet is faster. So we\ncan try the same with code, but written using CodeSnippet!\n\nHowever, it may be hard to write code in pure machine codes, so we can\nmake it another way. We can write C++ code; then compile it, open it in\na hex editor and put the machine code into our method. Sounds creepy,\nbut it’s possible.\n\nSeveral things you should be careful about:\n\n- You shouldn’t have a ret instruction, JVM will take care of it.\n\n- You should look carefully through your assembly code to be sure that\nit doesn’t try to access outside memory using an outside method.\n\n- And, finally, you should be careful about calling convention\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Typical `ls` picture that you can see get after several experiments\n\n\u003c/div\u003e\n\n![](/images/wild-panama/crashes.png)\n\nHere’s the code and we’re ready to run benchmark again\n\n``` java\nstatic final MethodHandle codeSnippetChecksum = jdk.internal.panama.CodeSnippet.make(\n        \"checksum\", MethodType.methodType(int.class, long.class, int.class),\n        isX64(),\n        0x48, 0x85, 0xF6, 0x74, 0x1E, 0x48, 0x01, 0xFE, 0x31, 0xC0, 0x66, 0x0F, 0x1F, 0x44,\n        0x00, 0x00, 0x0F, 0xBE, 0x17, 0x48, 0x83, 0xC7, 0x01, 0x01, 0xD0, 0x48, 0x39, 0xF7,\n        0x75, 0xF2, 0x0F, 0xB6, 0xC0, 0xEB, 0x02, 0x31, 0xC0);\n\n@Benchmark\npublic int codeSnippetChecksum() throws Throwable {\n    return (int) plainC_O2.invoke(address, size);\n}\n```\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Result\n\n\u003c/div\u003e\n```\nBenchmark                              (size)  Mode  Cnt   Score    Error  Units\nChecksumBenchmark.JNI_Checksum              4  avgt    4   0.008 ±  0.001  us/op\nChecksumBenchmark.JNI_Checksum           8096  avgt    4   3.060 ±  0.056  us/op\nChecksumBenchmark.JNI_Checksum         129536  avgt    4  49.865 ±  2.135  us/op\nChecksumBenchmark.codeSnippetChecksum       4  avgt    4   0.005 ±  0.001  us/op\nChecksumBenchmark.codeSnippetChecksum    8096  avgt    4   2.806 ±  0.243  us/op\nChecksumBenchmark.codeSnippetChecksum  129536  avgt    4  48.911 ±  0.448  us/op\nChecksumBenchmark.plainJava                 4  avgt    4   0.006 ±  0.001  us/op\nChecksumBenchmark.plainJava              8096  avgt    4   2.163 ±  0.035  us/op\nChecksumBenchmark.plainJava            129536  avgt    4  34.414 ±  0.984  us/op\n```\nAnd finally, you can observe pretty much the same results. The only\nnoticeable difference is for buffers that have a very small size. And\neven the CodeSnippet version is slower than the code produced by JIT.\n\nThe key is I used -O2 GCC option, which doesn’t perform a lot of\ninteresting optimizations.\n\n    g++ -shared -fpic  -Wall -O2   -I/usr/include ... checksum.c -o libchecksum.so\n\nAnd as a result, GCC didn’t perform well, and we’ve got an almost\nliteral translation of that we wrote in C++ to assembly. At the same\ntime, JIT gave us a good unrolled loop.\n\n\u003ctable\u003e\n    \u003ccolgroup\u003e\n        \u003ccol style={{width: \"50%\"}}/\u003e\n        \u003ccol style={{width: \"50%\"}}/\u003e\n    \u003c/colgroup\u003e\n    \u003cthead\u003e\n    \u003ctr class=\"header\"\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003eJIT\u003c/th\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003eGCC O2\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n    \u003ctr class=\"odd\"\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"x86asm\"\u003e\n```x86asm\n....\nloop:\n movsx  r10d,BYTE PTR [rbp+0x7]\n movsx  r8d,BYTE PTR [rbp+0x6]\n movsx  r11d,BYTE PTR [rbp+0x5]\n movsx  ebx,BYTE PTR [rbp+0x4]\n movsx  ecx,BYTE PTR [rbp+0x3]\n movsx  edx,BYTE PTR [rbp+0x2]\n movsx  edi,BYTE PTR [rbp+0x1]\n movsx  ebp,BYTE PTR [rbp+0x0]\n add    eax,ebp\n add    eax,edi\n add    eax,edx\n add    eax,ecx\n add    eax,ebx\n add    eax,r11d\n add    eax,r8d\n add    eax,r10d\n add    r9d,0x8 ; i+= 8\n cmp    r9d,r13d\n jl     loop  ;*if_icmpge\n....\n```\n\u003c/pre\u003e\n        \u003c/td\u003e\n\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"x86asm\"\u003e\n```x86asm\n...\nloop:\n movsx  edi,BYTE PTR [rsi+rdx*1]\n add    rsi,0x1 ; i+= 1\n add    eax,edi\n cmp    ecx,esi ; if return\n jg     loop\n ...\n```\n\u003c/pre\u003e\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\nSo, we can use -O3 if we need more optimizations.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    With -03\n\n\u003c/div\u003e\n```\nBenchmark                                (size)  Mode  Cnt   Score    Error  Units\nChecksumBenchmark.JNI_Checksum                4  avgt    4   0.009 ±  0.001  us/op\nChecksumBenchmark.JNI_Checksum             8096  avgt    4   3.089 ±  0.066  us/op\nChecksumBenchmark.JNI_Checksum           129536  avgt    4  49.481 ±  2.071  us/op\nChecksumBenchmark.codeSnippetChecksum         4  avgt    4   0.005 ±  0.001  us/op\nChecksumBenchmark.codeSnippetChecksum      8096  avgt    4   2.784 ±  0.153  us/op\nChecksumBenchmark.codeSnippetChecksum    129536  avgt    4  49.350 ±  2.208  us/op\nChecksumBenchmark.codeSnippetChecksumO3       4  avgt    4   0.006 ±  0.001  us/op\nChecksumBenchmark.codeSnippetChecksumO3    8096  avgt    4   0.621 ±  0.022  us/op\nChecksumBenchmark.codeSnippetChecksumO3  129536  avgt    4   9.672 ±  0.201  us/op\nChecksumBenchmark.plainJava                   4  avgt    4   0.006 ±  0.001  us/op\nChecksumBenchmark.plainJava                8096  avgt    4   2.161 ±  0.089  us/op\nChecksumBenchmark.plainJava              129536  avgt    4  34.825 ±  1.178  us/op\n```\nThere is a simple explanation why GCC -03 version is faster than code\nemitted by JIT. Here GCC was able to vectorize our loop. So, it used\nSIMD instructions which gave our processor an ability to \"parallelize\"\nexecution.\n\n\u003ctable\u003e\n    \u003ccolgroup\u003e\n        \u003ccol style={{width: \"50%\"}}/\u003e\n        \u003ccol style={{width: \"50%\"}}/\u003e\n    \u003c/colgroup\u003e\n    \u003cthead\u003e\n    \u003ctr class=\"header\"\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003eJIT\u003c/th\u003e\n        \u003cth style={{textAlign: \"left\"}}\u003eGCC O3\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n    \u003ctr class=\"odd\"\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"x86asm\"\u003e\n```x86asm\n....\nloop:\n movsx  r10d,BYTE PTR [rbp+0x7]\n movsx  r8d,BYTE PTR [rbp+0x6]\n movsx  r11d,BYTE PTR [rbp+0x5]\n movsx  ebx,BYTE PTR [rbp+0x4]\n movsx  ecx,BYTE PTR [rbp+0x3]\n movsx  edx,BYTE PTR [rbp+0x2]\n movsx  edi,BYTE PTR [rbp+0x1]\n movsx  ebp,BYTE PTR [rbp+0x0]\n add    eax,ebp\n add    eax,edi\n add    eax,edx\n add    eax,ecx\n add    eax,ebx\n add    eax,r11d\n add    eax,r8d\n add    eax,r10d\n add    r9d,0x8 ; i+= 8\n cmp    r9d,r13d\n jl     loop  ;*if_icmpge\n....\n```\n\u003c/pre\u003e\n        \u003c/td\u003e\n        \u003ctd style={{textAlign: \"left\"}}\u003e\u003cpre class=\"x86asm\"\u003e\n```x86asm\n....\nloop:\n add          r11, 0x1\n add          r8, 0x20\n cmp          r10, r11\n vpmovsxbw    ymm2, xmm1\n vextracti128 xmm1, ymm1, 0x1\n vpmovsxwd    ymm3, xmm2\n vextracti128 xmm2, ymm2, 0x1\n vpmovsxbw    ymm1, xmm1\n vpaddd       ymm0, ymm3, ymm0\n vpmovsxwd    ymm2, xmm2\n vpaddd       ymm0, ymm2, ymm0\n vpmovsxwd    ymm2, xmm1\n vextracti128 xmm1, ymm1, 0x1\n vpaddd       ymm0, ymm2, ymm0\n vpmovsxwd    ymm1, xmm1\n vpaddd       ymm0, ymm1, ymm0\n ja           loop\n....\n```\n\u003c/pre\u003e\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\nBut what if we need more performance? Can we do it better than GCC?\n\n## SIMD\n\nIt is possible to write the same code, but using AVX2 (256 byte\nregisters) instructions. (Thanks,\n[@kellylittlepage](https://twitter.com/kellylittlepage), for an [awesome\narticle](https://www.klittlepage.com/2013/12/10/accelerated-fix-processing-via-avx2-vector-instructions/)\nwhere I’ve read how to do it).\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    C++ function that will be compiled and putted in CodeSnippet\n\n\u003c/div\u003e\n\n``` cpp\nint avxChecksumAVX2(const char *const target, size_t targetLength) {\n    const __m256i zeroVec = _mm256_setzero_si256();\n    short d[16] = {1, 1, 1, 1, 1, 1, 1, 1,\n                   1, 1, 1, 1, 1, 1, 1, 1};\n    const __m256i oneVec = *((__m256i *) d);\n    __m256i accum = _mm256_setzero_si256();\n    unsigned int checksum = 0;\n    size_t offset = 0;\n\n    if (targetLength \u003e= 32) {\n        for (; offset \u003c= targetLength - 32; offset += 32) {\n            __m256i vec = _mm256_loadu_si256(\n                    reinterpret_cast\u003cconst __m256i *\u003e(target + offset));\n            __m256i vl = _mm256_unpacklo_epi8(vec, zeroVec);\n            __m256i vh = _mm256_unpackhi_epi8(vec, zeroVec);\n\n            accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vl, oneVec));\n            accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vh, oneVec));\n        }\n    }\n\n    for (; offset \u003c targetLength; ++offset) {\n        checksum += (int) target[offset];\n    }\n\n    accum = _mm256_add_epi32(accum, _mm256_srli_si256(accum, 4));\n    accum = _mm256_add_epi32(accum, _mm256_srli_si256(accum, 8));\n    return (_mm256_extract_epi32(accum, 0) + _mm256_extract_epi32(accum, 4) +\n            checksum) % 256;\n}\n```\n\nThis is how a simple checksum function looks like after rewriting for\nvectorizing execution. Here, some GCC intrinsics like\n[_mm256_unpacklo_epi8](https://software.intel.com/en-us/node/524002)\nand\n[_mm256_add_epi32](https://software.intel.com/en-us/node/513929)\nare used. GCC has a special implementation for this functions that uses\nAVX2 instructions. Almost always it is just one instruction.\n\n[Here](https://software.intel.com/sites/landingpage/IntrinsicsGuide/)\nyou can find a full guide of Intel intrinsics\n\nThis functions isn’t so easy to understand, but how fast is it?\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Result\n\n\u003c/div\u003e\n```\nChecksumBenchmark.JNI_Checksum                4  avgt    4   0.008 ±  0.001  us/op\nChecksumBenchmark.JNI_Checksum             8096  avgt    4   3.128 ±  0.024  us/op\nChecksumBenchmark.JNI_Checksum           129536  avgt    4  49.629 ±  0.694  us/op\nChecksumBenchmark.avx2Impl                    4  avgt    4   0.014 ±  0.001  us/op\nChecksumBenchmark.avx2Impl                 8096  avgt    4   0.239 ±  0.018  us/op\nChecksumBenchmark.avx2Impl               129536  avgt    4   4.128 ±  0.052  us/op\nChecksumBenchmark.codeSnippetChecksum         4  avgt    4   0.005 ±  0.001  us/op\nChecksumBenchmark.codeSnippetChecksum      8096  avgt    4   2.795 ±  0.044  us/op\nChecksumBenchmark.codeSnippetChecksum    129536  avgt    4  49.656 ±  0.733  us/op\nChecksumBenchmark.codeSnippetChecksumO3       4  avgt    4   0.006 ±  0.001  us/op\nChecksumBenchmark.codeSnippetChecksumO3    8096  avgt    4   0.630 ±  0.004  us/op\nChecksumBenchmark.codeSnippetChecksumO3  129536  avgt    4   9.810 ±  0.100  us/op\nChecksumBenchmark.plainJava                   4  avgt    4   0.006 ±  0.001  us/op\nChecksumBenchmark.plainJava                8096  avgt    4   2.224 ±  0.122  us/op\nChecksumBenchmark.plainJava              129536  avgt    4  35.042 ±  0.252  us/op\n```\nAwesome it is 8x times faster than our original code.\n\n### Java way\n\nLet’s say, now we met our performance requirements, but can we make it\nmore readable than just an ugly blob of ASM code produced by GCC? It is\npossible to save the main loop inside Java and use Long4 vectors to pass\ndata.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Java version of that scary function\n\n\u003c/div\u003e\n\n``` java\npublic class VectorIntrinsics {\n    ...\n    private static final MethodHandle _mm256_loadu_si256 = jdk.internal.panama.CodeSnippet.make(\n            \"_mm256_loadu_si256\", MethodType.methodType(Long4.class, long.class),\n            true,\n            0xC5, 0xFE, 0x6F, 0x06 // vmovdqu ymm0, YMMWORD PTR [rdi]\n    );\n    public static Long4 _mm256_loadu_si256(long address) throws Throwable {\n        return (Long4) _mm256_loadu_si256.invoke(address);\n    }\n    ...\n}\n\nprivate static int JAVA_avxChecksumAVX2(ByteBuffer buffer, long target, int targetLength)\n    throws Throwable {\n        Long4 zeroVec = Long4.ZERO;\n        Long4 oneVec = ones;\n        Long4 accum = Long4.ZERO;\n        int checksum = 0;\n        int offset = 0;\n\n        if (targetLength \u003e= 32) {\n            for (; offset \u003c= targetLength - 32; offset += 32) {\n                Long4 vec = _mm256_loadu_si256(target + offset);\n                Long4 vl = _mm256_unpacklo_epi8(vec, zeroVec);\n                Long4 vh = _mm256_unpackhi_epi8(vec, zeroVec);\n\n                accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vl, oneVec));\n                accum = _mm256_add_epi32(accum, _mm256_madd_epi16(vh, oneVec));\n            }\n        }\n\n        for (; offset \u003c targetLength; ++offset) {\n            checksum += (int) buffer.get(offset);\n        }\n\n        accum = _mm256_add_epi32(accum, _mm256_srli_si256_4(accum));\n        accum = _mm256_add_epi32(accum, _mm256_srli_si256_8(accum));\n        long finalChecksum = _mm256_extract_epi32_0(accum) + _mm256_extract_epi32_4(accum)\n                        + checksum;\n        return (int) (Integer.toUnsignedLong((int) finalChecksum) % 256);\n    }\n```\n\nNow it is written in the right way. We wrote a lot of small methods;\nevery method represents one small AVX2 instruction. And the main loop is\nwritten in Java. This code is reusable; it is much easier to write and\nunderstand than trying to write one big ASM blob. But, a big surprise,\nit is much slower than the ugly ASM blob.\n\nAnd again, JMH will help us to find answer with gc profiler.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    That’s why\n\n\u003c/div\u003e\n```\nJAVA_avx2Impl                                129536  avgt    4      30.394 ±     6.813   us/op\nJAVA_avx2Impl:·gc.alloc.rate                 129536  avgt    4         NaN              MB/sec\nJAVA_avx2Impl:·gc.count                      129536  avgt    4      34.000              counts\nJAVA_avx2Impl:·gc.time                       129536  avgt    4      39.000                  ms\navx2Impl                                     129536  avgt    4       4.192 ±     0.246   us/op\navx2Impl:·gc.alloc.rate                      129536  avgt    4         NaN              MB/sec\navx2Impl:·gc.count                           129536  avgt    4         ≈ 0              counts\n```\n`JAVA_avxChecksumAVX2` produces high allocation rate. Despite the fact\nthat vector types work with escape analysis really well, this loop\nbreaks our hopes. Because Long4 is immutable, we have to save `accum` to\nthe same variable on every loop iteration. Escape analysis can’t\nunderstand this and we are getting a lot of allocations of boxed vector\nvalues.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Problematic code for Escape Analysis\n\n\u003c/div\u003e\n\n``` java\nLong accum = Long4.ZERO;\nfor (; offset \u003c= targetLength - 32; offset += 32) {\n    Long4 vec = _mm256_loadu_si256(target + offset);\n    accum = operation(accum, vec); // EA, you are drunk, go home\n}\n```\n\nThis problem is known issue. Very probably it will be fixed soon, but\nhow can it be solved now?\n\nAs a workaround, we may try to create a temporary buffer and use a pair\nof `_mm256_loadu_si256` and `_mm256_storeu_si256` instructions on every\niteration. That intrinsics use `vmovdqu` instruction to load/store\nregister value to the memory.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    GC free solution\n\n\u003c/div\u003e\n\n``` java\nstatic final ByteBuffer tmpBuf = ...\n...\nfor (; offset \u003c= targetLength - 32; offset += 32) {\n    Long4 vec = _mm256_loadu_si256(target + offset);\n    Long4 accum = _mm256_loadu_si256(tmpBuffAddr);\n    Long4 result = operation(accum, vec);\n    _mm256_storeu_si256(tmpBuffAddr, result);\n}\n```\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Results\n\n\u003c/div\u003e\n```\nBenchmark                                       (size)  Mode  Cnt   Score   Error   Units\nChecksumBenchmark.JAVA_avx2Impl                 129536  avgt    4  23.837 ± 0.064   us/op\nChecksumBenchmark.JAVA_avx2Impl:·gc.alloc.rate  129536  avgt    4     NaN          MB/sec\nChecksumBenchmark.JAVA_avx2Impl:·gc.count       129536  avgt    4     ≈ 0          counts\n```\nNow function is GC free; there is no garbage anymore and it is faster,\nbut actually it’s still quite slow. To understand why we should use a\nprofiler, but simple solutions like Yourkit or JProfiler won’t help us,\nwe must work on instruction level. Thank goodness, JMH has an excellent\nsupport of perf profiler, you need just to pass an option to it (don’t\nforget to install perf on your system before).\n\n``` x86asm\n 12.39%   26.58%    vmovdqu YMMWORD PTR [rsp+0x40],ymm0\n 12.88%    2.85%    movabs r10,0x6d61010e8\n           0.01%    vmovdqu ymm1,YMMWORD PTR [r10+0x10]\n  0.01%             vmovdqu ymm0,YMMWORD PTR [rsp+0x20]\n                    vpunpcklbw ymm0,ymm0,ymm1\n  4.42%    0.03%    movabs r10,0x6d61010b8\n  0.01%             vmovdqu ymm1,YMMWORD PTR [r10+0x10]\n  0.02%    0.01%    vpmaddwd ymm0,ymm0,ymm1\n  0.02%    0.01%    vpmaddwd ymm0,ymm0,ymm1\n           0.02%    vmovdqu ymm1,ymm0\n  4.20%    2.95%    vmovdqu ymm0,YMMWORD PTR [rsp+0x40]\n  8.45%   22.88%    vpaddd ymm0,ymm1,ymm0\n 12.91%    5.79%    vmovdqu YMMWORD PTR [rsp+0x40],ymm0\n```\n\nAs you can see, we are spending an enormous amount of time just to load\nout the temporary buffer and store it back just to avoid GC. So, we can\nrewrite algorithm a little bit instead. We’ll be saving a final result\nto `checksum` variable right in the loop instead of using it further in\nvector calculations.\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Here the code\n\n\u003c/div\u003e\n\n``` java\nfor (; offset \u003c= targetLength - 32; offset += 32) {\n    Long4 vec = _mm256_loadu_si256(target + offset);\n    Long4 lVec = _mm256_unpacklo_epi8(vec, zeroVec);\n    Long4 hVec = _mm256_unpackhi_epi8(vec, zeroVec);\n    Long4 sum = _mm256_add_epi16(lVec, hVec);\n    sum = _mm256_hadd_epi16(sum, sum);\n    sum = _mm256_hadd_epi16(sum, sum);\n    sum = _mm256_hadd_epi16(sum, sum);\n    checksum += _mm256_extract_epi16_0(sum) + _mm256_extract_epi16_15(sum);\n}\n```\n\n\u003cdiv class=\"formalpara-title\"\u003e\n\n    Benchmark results\n\n\u003c/div\u003e\n```\nBenchmark                        (size)  Mode  Cnt   Score    Error  Units\nChecksumBenchmark.JAVA_avx2Impl       4  avgt    4   0.005 ±  0.001  us/op\nChecksumBenchmark.JAVA_avx2Impl    8096  avgt    4   1.245 ±  0.028  us/op\nChecksumBenchmark.JAVA_avx2Impl  129536  avgt    4  20.095 ±  0.314  us/op\nChecksumBenchmark.avx2Impl            4  avgt    4   0.013 ±  0.001  us/op\nChecksumBenchmark.avx2Impl         8096  avgt    4   0.211 ±  0.004  us/op\nChecksumBenchmark.avx2Impl       129536  avgt    4   3.317 ±  0.077  us/op\nChecksumBenchmark.plainJava           4  avgt    4   0.005 ±  0.001  us/op\nChecksumBenchmark.plainJava        8096  avgt    4   2.109 ±  0.035  us/op\nChecksumBenchmark.plainJava      129536  avgt    4  33.503 ±  0.227  us/op\n```\nThis version of the code is even faster, but it can’t achieve the\nperformance of big ugly assembly blob yet because escape analysis is\nlike a big stone on our way. However this code can be maintained easily,\nand this API is under active development; there are a lot of experiments\nhappening right now. So, you will have fought this ugly blob when these\nfeatures are released.\n\nMoreover, all that machine snippets and direct Long\\* vector parameters\nare really low-level API. Prototypes of high-level API you can find\n[here](http://hg.openjdk.java.net/panama/panama/jdk/file/c5a104d33632/test/panama/vector-api-boxed-variant/src/test/java/com/oracle/vector/BytesLong2Test.java)\nand\n[here](http://hg.openjdk.java.net/panama/panama/jdk/file/c5a104d33632/test/panama/vector-api-patchable/src/test/java/SnippetTest.java).\n\nI think that’s a perfect point to end a journey through the jungle of\nPanama. We have seen enough crazy things. I’ll be glad to hear any\ncomments from you. You can find all the experiments\n[here](https://github.com/SerCeMan/panama-article) (don’t forget to\nbuild your own JDK before running the benchmarks). I’ll be glad to hear\nany comments from you.\n\n# Conclusions\n\n- Project Panama will bring us great features, but these are likely to\narrive much further down the line\n- Nothing is impossible, even running an inline assembler from Java\n- There are a lot of features that can be done in Java with Vector API\nand Machine Code Snippets already, although it is only the beginning\nof the story.\n- Compiler can optimize your code really well, most probably better than\nyou.\n- It is very important to measure performance while you are doing\noptimizations. Or else you can make it even worse.\n- Seeing how your code will work in the future will help you to better\nunderstand how it works now.\n\n## Thanks to\n\n- [@kellylittlepage](https://twitter.com/kellylittlepage) for an awesome article about AVX instruction\n- [@harrigan_shane](https://twitter.com/harrigan_shane) for comments about my writing style\n- [@iwan0www](https://twitter.com/iwan0www) for comments and suggestions regarding this post\n- You for reading it\n\n## Discuss on\n\n* [Twitter](https://twitter.com/SerCeMan/status/737889841132752896)\n\n","description":"Empty","slug":"01-06-2016-wild-panama","kind":"mdx"},{"title":"JNR-FUSE library for using FUSE from Java","date":"2015-06-22","content":"\nHi!\n\nIn this article, I’ll tell you how to implement userspace file system\nusing Java without a line of kernel space code. I’ll also show you how\nto connect Java and native code without writing C code and save maximum\nperformance.\n\nOriginally, I posted this article on\n[habrahabr](https://habrahabr.ru/post/260801/) (in Russian).\n\n![](/images/jnr-fuse/jackie.jpg)\n\n## FUSE\n\nFirst of all, it is important to understand what FUSE is. FUSE -\nFileSystem in Userspace - helps users without any privileges to create\ntheir file system without a necessity to write code in a kernel space.\n\nThis is possible because the filesystem code runs in userspace. And the\nFUSE module is just a bridge between the kernel API and your code. FUSE\nwas officially included in the Linux source tree in 2.6.14.\n\n![](/images/jnr-fuse/bridge.png)\n\nSo, you can easily create your own filesystem ([here is the simplest\nexample](https://github.com/libfuse/libfuse/blob/master/example/hello.c)).\nThere are a lot of areas where you can use it. For example, you can\nquickly write a FS where Github or DropBox would be the backend.\n\nOr, let’s imagine, you have a business application where all user files\nare stored in a database. But your client wants to have access to them\nfrom a filesystem on the server. Of course to duplicate files in the\nfilesystem and the database is the wrong decision. And here FUSE comes\nin; you just need a little FUSE program which handles all user requests\nto the directory and redirects them to the database.\n\n## Java and native code\n\nSo far so good. But implementation of FUSE starts from \"include header\n\u0026lt;fuse.h\u0026gt;\". But your business application is written in Java.\nObviously, it is necessary to communicate with the native code.\n\n### JNI\n\nThe standard tool for a native communication in Java is JNI. But it\nbrings a lot of complexity. Especially because for using FUSE we need a\nlot of callbacks from the native code to Java. And \"write once\" is\nsuffering in this case (However, in the case of FUSE it is not so\nimportant). If you try to find projects that implement a FUSE wrapper\nusing JNI you will find a few projects, but they have been obsolete for\na long time. And their API is very inconvenient.\n\n### JNA\n\nAnother option is [JNA\nlibrary](https://github.com/java-native-access/jna). JNA (Java Native\nAccess) gives you the possibility to get access to the native code\neasily without using JNI only by using Java code. It is very easy; you\njust need to declare an interface which matches the native code and get\nan implementation through \"Native.loadLibrary\". And that’s all. Another\nadvantage of JNA is very detailed documentation. The project is alive,\nand it is in active development.\n\nMoreover, a good project implementing FUSE using JNA already exists. But\nJNA has a lot of problems with performance. JNA is reflection based, so\njumping from native code to Java code with data conversion is very\nexpensive. It is not so important if native calls are rare. But a FS has\na lot of native calls. The only way to speed up fuse-jna is to read\nfiles using big chunks, but it doesn’t always work. For example, when\nyou have no access to a client’s code or when all files are small.\nObviously, we need a library that combines JNI performance and JNA\nconvenience.\n\n### JNR\n\nAnd here is where JNR (Java Native Runtime) comes in. JNR, like JNA, is\nbased on libffi library, but it uses a bytecode generation instead of\nreflection. And as a result, JNR achieves excellent performance. The\ninformation about JNR is very limited. The most detailed piece of\ninformation is [Charles Nutter’s talk on JVMLS\n2013](http://medianetwork.oracle.com/video/player/2630340184001). But\ndespite the lack of information, JNR is already a big ecosystem actively\nused by JRuby. A lot of jnr-based libraries such as unix-sockets and\nposix-api are actively used by different projects.\n\n![](/images/jnr-fuse/jnr.png)\n\nJNR is a library which became a basis for the development of [JEP 191 -\nForeign Function Interface](http://openjdk.java.net/jeps/191), which is\ntargeted for Java 10. In comparison to JNA, JNR has no proper\ndocumentation, and you need to look for answers in the source code. That\nis the main reason for this mini-guide.\n\n# Specialties of writing code using Java Native Runtime\n\n## Function binding\n\n*The simplest binding*\n```java\nimport jnr.ffi.*;\nimport jnr.ffi.types.pid_t;\n\n/**\n * Gets the process ID of the current process, and that of its parent.\n */\npublic class Getpid {\n    public interface LibC  {\n        public @pid_t long getpid();\n        public @pid_t long getppid();\n    }\n\n    public static void main(String[] args) {\n        LibC libc = LibraryLoader.create(LibC.class).load(\"c\");\n\n        System.out.println(\"pid=\" + libc.getpid() + \" parent pid=\" + libc.getppid());\n    }\n}\n```\n\nHere we are the loading java library which matches the native interface\nby name.\n\nIn the case of FUSE, we need an interface with method fuse\\_main\\_real\nwhere FuseOperations structure with all callbacks passes as an argument.\n\n```java\npublic interface LibFuse {\n    int fuse_main_real(int argc, String argv[], FuseOperations op, int op_size, Pointer user_data);\n}\n```\n\n### Implementation of structures\n\nOften you need to work with structure which is located at a certain\naddress, for example with fuse\\_bufvec structure:\n```c\nstruct fuse_bufvec {\n    size_t count;\n    size_t idx;\n    size_t off;\n    struct fuse_buf buf[1];\n};\n```\n\nFor its implementation, we need to make a successor of jnr.ffi.Struct.\n\n```java\nimport jnr.ffi.*;\n\npublic class FuseBufvec extends Struct {\n    public FuseBufvec(jnr.ffi.Runtime runtime) {\n        super(runtime);\n    }\n    public final size_t count = new size_t();\n    public final size_t idx = new size_t();\n    public final size_t off = new size_t();\n    public final FuseBuf buf = inner(new FuseBuf(getRuntime()));\n}\n```\n\nAfter that, you have to set proper callback implementation into the\ngetattr field.\n\n```java\nfuseOperations.getattr.set((path, stbuf) -\u003e 0);\n```\n\n### Enum\n\nEnum implementation is not so obvious as other parts of the library. You\nneed to inherit your enum from jnr.ffi.util.EnumMapper.IntegerEnum and\nimplement method intValue\n\n```java\nenum fuse_buf_flags {\n    FUSE_BUF_IS_FD    = (1 \u003c\u003c 1),\n    FUSE_BUF_FD_SEEK    = (1 \u003c\u003c 2),\n    FUSE_BUF_FD_RETRY    = (1 \u003c\u003c 3),\n};\n\npublic enum FuseBufFlags implements EnumMapper.IntegerEnum {\n    FUSE_BUF_IS_FD(1 \u003c\u003c 1),\n    FUSE_BUF_FD_SEEK(1 \u003c\u003c 2),\n    FUSE_BUF_FD_RETRY(1 \u003c\u003c 3);\n\n    private final int value;\n\n    FuseBufFlags(int value) {\n        this.value = value;\n    }\n\n    @Override\n    public int intValue() {\n        return value;\n    }\n}\n```\n\n### Work with memory\n\n* For working with direct memory wrapper jnr.ffi.Pointer exists.\n* You can allocate memory using jnr.ffi.Memory\n* The entry point of JNR API learning is jnr.ffi.Runtime\n\nThis knowledge is enough to implement a simple cross-platform wrapper\nfor some native library.\n\n## JNR-FUSE\n\nWhat I’ve implemented is a FUSE wrapper in my project jnr-fuse.\nPreviously I used fuse-jna, but it was a bottleneck in the FS\nimplementation. During the development, I tried to save compatibility\nwith the native interface (\u0026lt;fuse.h\u0026gt;).\n\nFor implementing your own file system, you need to extend\nru.serce.jnrfuse.FuseStubFS and implement several methods.\nFuse\\_operations contain [a lot of\nmethods](http://fuse.sourcearchive.com/documentation/2.8.4-1.4ubuntu1/structfuse__operations.html),\nbut for getting your FS up and running, you just need to implement\nseveral methods. It is very easy, [here are some\nexamples](https://github.com/SerCeMan/jnr-fuse/tree/master/src/main/java/ru/serce/jnrfuse/examples).\n\nCurrently, only Linux is supported (x86 and x64).\n\nLibrary exists in JCenter.\n\n*Gradle*\n```kotlin\nrepositories {\n    jcenter()\n}\n\ndependencies {\n    compile 'com.github.serceman:jnr-fuse:0.1'\n}\n```\n\n*Maven*\n```xml\n\u003crepositories\u003e\n    \u003crepository\u003e\n        \u003cid\u003ecentral\u003c/id\u003e\n        \u003cname\u003ebintray\u003c/name\u003e\n        \u003curl\u003ehttp://jcenter.bintray.com\u003c/url\u003e\n    \u003c/repository\u003e\n\u003c/repositories\u003e\n\n\u003cdependencies\u003e\n    \u003cdependency\u003e\n        \u003cgroupId\u003ecom.github.serceman\u003c/groupId\u003e\n        \u003cartifactId\u003ejnr-fuse\u003c/artifactId\u003e\n        \u003cversion\u003e0.1\u003c/version\u003e\n    \u003c/dependency\u003e\n\u003c/dependencies\u003e\n```\n### JNR-FUSE and FUSE-JNA performance comparison\n\nIn my case the FS was read-only, and I was interested in throughput. The\nperformance will mostly depend on your FS implementation, so if you\nalready use fuse-jna, you’ll be able to change it easily to jnr-fuse,\nwrite a test, and to see the performance difference on your workload.\n(It will be helpful anyway because we all love to achieve new levels of\nperformance, right?)\n\nIn order to show performance the difference, I moved MemoryFS\nimplementation from fuse-jna to jnr-fuse with minimal changes and ran a\nreading test. For the test, I used\n[fio](http://freecode.com/projects/fio) framework.\n\n\u003cdetails\u003e\n    \u003csummary\u003eTest configuration\u003c/summary\u003e\n    ```\n    [readtest]\n    blocksize=4k\n    directory=/tmp/mnt/\n    rw=randread\n    direct=1\n    buffered=0\n    ioengine=libaio\n    time_based=60\n    size=16M\n    runtime=60\n    ```\n\u003c/details\u003e\n\u003cdetails\u003e\n    \u003csummary\u003eThe result of of fuse-jna\u003c/summary\u003e\n    ```\n    serce@SerCe-FastLinux:~/git/jnr-fuse/bench$ fio read.ini\n    readtest: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1\n    fio-2.1.3\n    Starting 1 process\n    readtest: Laying out IO file(s) (1 file(s) / 16MB)\n    Jobs: 1 (f=1): [r] [100.0% done] [24492KB/0KB/0KB /s] [6123/0/0 iops] [eta 00m:00s]\n    readtest: (groupid=0, jobs=1): err= 0: pid=10442: Sun Jun 21 14:49:13 2015\n    read: io=1580.2MB, bw=26967KB/s, iops=6741, runt= 60000msec\n    slat (usec): min=46, max=29997, avg=146.55, stdev=327.68\n    clat (usec): min=0, max=69, avg= 0.47, stdev= 0.66\n    lat (usec): min=47, max=30002, avg=147.26, stdev=327.88\n    clat percentiles (usec):\n    | 1.00th=[ 0], 5.00th=[ 0], 10.00th=[ 0], 20.00th=[ 0],\n    | 30.00th=[ 0], 40.00th=[ 0], 50.00th=[ 0], 60.00th=[ 1],\n    | 70.00th=[ 1], 80.00th=[ 1], 90.00th=[ 1], 95.00th=[ 1],\n    | 99.00th=[ 2], 99.50th=[ 2], 99.90th=[ 3], 99.95th=[ 12],\n    | 99.99th=[ 14]\n    bw (KB /s): min=17680, max=32606, per=96.09%, avg=25913.26, stdev=3156.20\n    lat (usec): 2=97.95%, 4=1.96%, 10=0.02%, 20=0.06%, 50=0.01%\n    lat (usec): 100=0.01%\n    cpu: usr=1.98%, sys=5.94%, ctx=405302, majf=0, minf=28\n    IO depths: 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u003e=64=0.0%\n    submit: 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0%\n    complete: 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0%\n    issued: total=r=404511/w=0/d=0, short=r=0/w=0/d=0\n\n    Run status group 0 (all jobs):\n    READ: io=1580.2MB, aggrb=26967KB/s, minb=26967KB/s, maxb=26967KB/s, mint=60000msec, maxt=60000msec\n    ```\n\u003c/details\u003e\n\u003cdetails\u003e\n    \u003csummary\u003eThe result of jnr-fuse\u003c/summary\u003e\n    ```\n    serce@SerCe-FastLinux:~/git/jnr-fuse/bench$ fio read.ini\n    readtest: (g=0): rw=randread, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=1\n    fio-2.1.3\n    Starting 1 process\n    readtest: Laying out IO file(s) (1 file(s) / 16MB)\n    Jobs: 1 (f=1): [r] [100.0% done] [208.5MB/0KB/0KB /s] [53.4K/0/0 iops] [eta 00m:00s]\n    readtest: (groupid=0, jobs=1): err= 0: pid=10153: Sun Jun 21 14:45:17 2015\n    read: io=13826MB, bw=235955KB/s, iops=58988, runt= 60002msec\n    slat (usec): min=6, max=23671, avg=15.80, stdev=19.97\n    clat (usec): min=0, max=1028, avg= 0.37, stdev= 0.78\n    lat (usec): min=7, max=23688, avg=16.29, stdev=20.03\n    clat percentiles (usec):\n    | 1.00th=[ 0], 5.00th=[ 0], 10.00th=[ 0], 20.00th=[ 0],\n    | 30.00th=[ 0], 40.00th=[ 0], 50.00th=[ 0], 60.00th=[ 0],\n    | 70.00th=[ 1], 80.00th=[ 1], 90.00th=[ 1], 95.00th=[ 1],\n    | 99.00th=[ 1], 99.50th=[ 1], 99.90th=[ 2], 99.95th=[ 2],\n    | 99.99th=[ 10]\n    lat (usec): 2=99.88%, 4=0.10%, 10=0.01%, 20=0.01%, 50=0.01%\n    lat (usec): 100=0.01%, 250=0.01%\n    lat (msec): 2=0.01%\n    cpu: usr=9.33%, sys=34.01%, ctx=3543137, majf=0, minf=28\n    IO depths: 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u003e=64=0.0%\n    submit: 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0%\n    complete: 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u003e=64=0.0%\n    issued: total=r=3539449/w=0/d=0, short=r=0/w=0/d=0\n\n    Run status group 0 (all jobs):\n    READ: io=13826MB, aggrb=235955KB/s, minb=235955KB/s, maxb=235955KB/s, mint=60002msec, maxt=60002msec\n    ```\n\u003c/details\u003e\n![](/images/jnr-fuse/table.png)\n\nThe only information this test shows us is the difference in reading a\nfile using fuse-jna and jnr-fuse, but it gives us an understanding of\nthe level of performance difference of JNA and JNR. If you are\ninterested, you can write a more detailed test for native calls using\nthe [JMH](http://openjdk.java.net/projects/code-tools/jmh/) tool.\n\nThe performance differences in throughput and latency are about ~10\ntimes. Charles Nutter in his presentation gave us the same numbers.\n\n### References\n\n- [FUSE on GitHub](https://github.com/libfuse/libfuse)\n- [JNR on GitHub](https://github.com/jnr)\n- [Charles Nutter presentation about JNR](http://www.oracle.com/technetwork/java/jvmls2013nutter-2013526.pdf)\n- [JEP 191](http://openjdk.java.net/jeps/191)\n- [Java HelloFuse](https://github.com/SerCeMan/jnr-fuse/blob/master/src/main/java/ru/serce/jnrfuse/examples/HelloFuse.java)\n/[CHelloFuse](https://github.com/libfuse/libfuse/blob/master/example/hello.c)\n\nThe [jnr-fuse](https://github.com/SerCeMan/jnr-fuse) project is located\non GitHub. I’ll appreciate stars, pull-requests, and comments. I’ll be\nglad to answer any questions you have about JNR or jnr-fuse.\n","description":"Empty","slug":"22-06-2015-jnr-fuse","kind":"mdx"}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"Pcyft9E01JztMB997G3j_","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>